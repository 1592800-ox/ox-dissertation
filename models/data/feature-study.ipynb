{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigate the lha length requirement for editing efficiency\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from os.path import join as pjoin, dirname, abspath, basename\n",
    "\n",
    "# load up the shap analysis dataset for deepprime pe2 hek293t\n",
    "data = pd.read_csv('shap/shap-dp-hek293t-pe2.csv')\n",
    "data = data.sample(frac=0.05)\n",
    "f_size = 14\n",
    "\n",
    "# plot the mean editing efficiency for each lha length with error bars\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# plot all data on the same plot\n",
    "fig, ax = plt.subplots(3, 1, figsize=(10, 9))\n",
    "palette = iter(sns.color_palette('Spectral', 6))\n",
    "\n",
    "for ind, l in enumerate(['rha-length', 'lha-length', 'pbs-length']):\n",
    "    # remove grid\n",
    "    ax[ind].grid(False)\n",
    "    # no top and right axis\n",
    "    ax[ind].spines['top'].set_visible(False)\n",
    "    ax[ind].spines['right'].set_visible(False)\n",
    "    # plot as boxplot\n",
    "    c = next(palette)\n",
    "    sns.boxplot(x=l, y='editing-efficiency', data=data, ax=ax[ind], color=c, boxprops={'alpha': 0.4}, dodge=True, showfliers=False)\n",
    "    # get the y limits\n",
    "    y_min, y_max = ax[ind].get_ylim()\n",
    "    # show samples of the data as stripplot\n",
    "    \n",
    "    sns.stripplot(x=l, y='editing-efficiency', data=data, ax=ax[ind], color=c,  dodge=True, size=1, jitter=True)\n",
    "    # set the y limits\n",
    "    ax[ind].set_ylim(y_min, y_max+5)\n",
    "    # remove the - in the labels\n",
    "    ax[ind].set_xlabel(ax[ind].get_xlabel().replace('-', ' '))\n",
    "    ax[ind].set_ylabel('editing efficiency')\n",
    "\n",
    "    # limit the x axis to 20 if x ticks have more than 20 values\n",
    "    # if len(ax[ind].get_xticks()) > 20:\n",
    "    #     ax[ind].set_xlim(-0.5, 20.5)\n",
    "    if len(ax[ind].get_xticks()) > 20:\n",
    "        # limit the amount of x ticks\n",
    "        ax[ind].set_xticks(ax[ind].get_xticks()[::2])\n",
    "\n",
    "    # set the font size of the labels and ticks\n",
    "    ax[ind].tick_params(axis='both', which='major', labelsize=f_size)\n",
    "    ax[ind].set_xlabel(ax[ind].get_xlabel(), fontsize=f_size)\n",
    "    ax[ind].set_ylabel(ax[ind].get_ylabel(), fontsize=f_size)\n",
    "\n",
    "plt.tight_layout()\n",
    "# save the figure\n",
    "plt.savefig(pjoin('../../dissertation/figures', 'lha-rha-pbs-length-requirement.png'), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# convert pd data of different cell types\n",
    "# to std format, and then shap format\n",
    "# ==================================\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from utils.data_utils import convert_to_SHAP\n",
    "from os.path import join as pjoin\n",
    "import pandas as pd\n",
    "\n",
    "pridict_data = pjoin('std', 'pd-pe2.csv')\n",
    "pridict_data = pd.read_csv(pridict_data)\n",
    "# cell types\n",
    "cell_lines = pridict_data['cell-line'].unique()\n",
    "\n",
    "for cell_line in cell_lines:\n",
    "    cell_line_data = pridict_data[pridict_data['cell-line'] == cell_line]\n",
    "    cell_line_data.to_csv(pjoin('std',f'std-pd-pe2-{cell_line}.csv'), index=False)\n",
    "    convert_to_SHAP(pjoin('std', f'std-pd-pe2-{cell_line}.csv'), pjoin('shap', f'shap-pd-pe2-{cell_line}.csv'), 'GG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features to use for the study\n",
    "# top 24 features in shap-dp-hek293t-pe2\n",
    "import pandas as pd\n",
    "from os.path import join as pjoin\n",
    "\n",
    "shap_data = pd.read_csv(pjoin('shap', 'shap-dp-hek293t-pe2.csv'))\n",
    "features = shap_data.columns\n",
    "print(features[:24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert std data to conventional ml format\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from utils.data_utils import convert_to_conventional_ml\n",
    "from glob import glob\n",
    "from os.path import join as pjoin\n",
    "\n",
    "for std_data_source in glob(pjoin('std', '*.csv')):\n",
    "    print(std_data_source)\n",
    "    convert_to_conventional_ml(std_data_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the distribution of editing efficiency values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "f_size = 14\n",
    "\n",
    "def plot_histogram(dp_data, pridict_hek_data, pridict_k562_data, pridict_k562mlh1d_data, pridict_adv_data, bin_min: int = 0, bin_max: int = 100, bin_width: int = 10, legend: bool = True):\n",
    "    lbins = np.arange(bin_min, bin_max, bin_width)\n",
    "    # plot the editing-efficiency of the five datasets in a histogram\n",
    "    # prebin the editing efficiency\n",
    "    dp_data['editing-efficiency'] = pd.cut(dp_data['editing-efficiency'], bins=lbins)\n",
    "    pridict_hek_data['editing-efficiency'] = pd.cut(pridict_hek_data['editing-efficiency'], bins=lbins)\n",
    "    pridict_k562_data['editing-efficiency'] = pd.cut(pridict_k562_data['editing-efficiency'], bins=lbins)\n",
    "    pridict_k562mlh1d_data['editing-efficiency'] = pd.cut(pridict_k562mlh1d_data['editing-efficiency'], bins=lbins)\n",
    "    pridict_adv_data['editing-efficiency'] = pd.cut(pridict_adv_data['editing-efficiency'], bins=lbins)\n",
    "\n",
    "    binned_dp_data = dp_data.groupby('editing-efficiency', observed=False).size().reset_index(name='count')\n",
    "    binned_pridict_hek_data = pridict_hek_data.groupby('editing-efficiency', observed=False).size().reset_index(name='count')\n",
    "    binned_pridict_k562_data = pridict_k562_data.groupby('editing-efficiency', observed=False).size().reset_index(name='count')\n",
    "    binned_pridict_k562mlh1d_data = pridict_k562mlh1d_data.groupby('editing-efficiency', observed=False).size().reset_index(name='count')\n",
    "    binned_pridict_adv_data = pridict_adv_data.groupby('editing-efficiency', observed=False).size().reset_index(name='count')\n",
    "\n",
    "    interval = bin_width * 0.18\n",
    "    # get the left point of the pandas interval\n",
    "    binned_dp_data['editing-efficiency'] = binned_dp_data['editing-efficiency'].apply(lambda x: x.left)\n",
    "    binned_pridict_hek_data['editing-efficiency'] = binned_pridict_hek_data['editing-efficiency'].apply(lambda x: x.left + 1 * interval)\n",
    "    binned_pridict_k562_data['editing-efficiency'] = binned_pridict_k562_data['editing-efficiency'].apply(lambda x: x.left + 2 * interval)\n",
    "    binned_pridict_k562mlh1d_data['editing-efficiency'] = binned_pridict_k562mlh1d_data['editing-efficiency'].apply(lambda x: x.left + 3 * interval)\n",
    "    binned_pridict_adv_data['editing-efficiency'] = binned_pridict_adv_data['editing-efficiency'].apply(lambda x: x.left + 4 * interval)\n",
    "\n",
    "\n",
    "    # plot all data on the same plot, but different y axis for the deep prime data\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "    palette = iter(sns.color_palette('Spectral', 6))\n",
    "\n",
    "    # remove grid\n",
    "    ax.grid(False)\n",
    "    # no top and right axis\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    # two x axis\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.spines['top'].set_visible(False)\n",
    "\n",
    "    ax2.set_ylabel('PRIDICT editing efficiency')\n",
    "    ax.set_ylabel('DeepPrime editing efficiency')\n",
    "\n",
    "    # plot as barplot with mutliple categories\n",
    "    bar_width = interval\n",
    "    c = next(palette)\n",
    "    ax.bar(binned_dp_data['editing-efficiency'], binned_dp_data['count'], color=c, label='DP', width=bar_width)\n",
    "    c = next(palette)\n",
    "    ax2.bar(binned_pridict_hek_data['editing-efficiency'], binned_pridict_hek_data['count'], color=c, label='PD HEK293T', width=bar_width)\n",
    "    c = next(palette)\n",
    "    ax2.bar(binned_pridict_k562_data['editing-efficiency'], binned_pridict_k562_data['count'], color=c, label='PD K562', width=bar_width)\n",
    "    c = next(palette)\n",
    "    ax2.bar(binned_pridict_k562mlh1d_data['editing-efficiency'], binned_pridict_k562mlh1d_data['count'], color=c, label='PD K562MLH1D', width=bar_width)\n",
    "    c = next(palette)\n",
    "    ax2.bar(binned_pridict_adv_data['editing-efficiency'], binned_pridict_adv_data['count'], color=c, label='PD ADV', width=bar_width)\n",
    "\n",
    "    ax.tick_params(axis='both', which='major', labelsize=f_size)\n",
    "    ax2.tick_params(axis='both', which='major', labelsize=f_size)\n",
    "    ax.set_xlabel('Editing Efficiency', fontsize=f_size)\n",
    "    ax.set_ylabel('DeepPrime Count', fontsize=f_size)\n",
    "    ax2.set_ylabel('PRIDICT Count', fontsize=f_size)\n",
    "\n",
    "    # set the font size of the labels and ticks\n",
    "    ax.tick_params(axis='both', which='major', labelsize=f_size)\n",
    "    ax2.tick_params(axis='both', which='major', labelsize=f_size)\n",
    "    ax.set_xlabel('Editing Efficiency', fontsize=f_size)\n",
    "    ax.set_ylabel('DeepPrime Count', fontsize=f_size)\n",
    "    ax2.set_ylabel('PRIDICT Count', fontsize=f_size)\n",
    "    \n",
    "    if legend:\n",
    "        # merge the legends\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "        ax.legend(handles + handles2, labels + labels2, loc='upper right', fontsize=f_size)\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up the four main datasets\n",
    "import pandas as pd\n",
    "from os.path import join as pjoin\n",
    "\n",
    "\n",
    "# load the data\n",
    "dp_data = pd.read_csv(pjoin('std', 'std-dp-hek293t-pe2.csv'))\n",
    "pridict_hek_data = pd.read_csv(pjoin('std', 'std-pd-hek293t-pe2.csv'))\n",
    "pridict_k562_data = pd.read_csv(pjoin('std', 'std-pd-k562-pe2.csv'))\n",
    "pridict_k562mlh1d_data = pd.read_csv(pjoin('std', 'std-pd-k562mlh1d-pe2.csv'))\n",
    "pridict_adv_data = pd.read_csv(pjoin('std', 'std-pd-adv-pe2.csv'))\n",
    "\n",
    "# plot the editing-efficiency of the four datasets in a histogram\n",
    "import numpy as np\n",
    "\n",
    "plot_histogram(dp_data, pridict_hek_data, pridict_k562_data, pridict_k562mlh1d_data, pridict_adv_data, bin_min=0, bin_max=100, bin_width=10, legend=True)\n",
    "\n",
    "# figure title at the bottom\n",
    "# plt.figtext(0.5, 0.01, '(a) Original', ha='center', va='center', fontsize=f_size)\n",
    "\n",
    "# save the figure\n",
    "plt.savefig(pjoin('../../dissertation/figures', 'editing-efficiency-comparison.png'), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log adjusted\n",
    "import pandas as pd\n",
    "from os.path import join as pjoin\n",
    "import numpy as np\n",
    "\n",
    "# load the data\n",
    "dp_data = pd.read_csv(pjoin('std', 'std-dp-hek293t-pe2.csv'))\n",
    "pridict_hek_data = pd.read_csv(pjoin('std', 'std-pd-hek293t-pe2.csv'))\n",
    "pridict_k562_data = pd.read_csv(pjoin('std', 'std-pd-k562-pe2.csv'))\n",
    "pridict_k562mlh1d_data = pd.read_csv(pjoin('std', 'std-pd-k562mlh1d-pe2.csv'))\n",
    "pridict_adv_data = pd.read_csv(pjoin('std', 'std-pd-adv-pe2.csv'))\n",
    "\n",
    "# plot the editing-efficiency of the four datasets in a histogram\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# log adjust the editing efficiency\n",
    "dp_data['editing-efficiency'] = np.log1p(dp_data['editing-efficiency'])\n",
    "pridict_hek_data['editing-efficiency'] = np.log1p(pridict_hek_data['editing-efficiency'])\n",
    "pridict_k562_data['editing-efficiency'] = np.log1p(pridict_k562_data['editing-efficiency'])\n",
    "pridict_k562mlh1d_data['editing-efficiency'] = np.log1p(pridict_k562mlh1d_data['editing-efficiency'])\n",
    "pridict_adv_data['editing-efficiency'] = np.log1p(pridict_adv_data['editing-efficiency'])\n",
    "\n",
    "plot_histogram(dp_data, pridict_hek_data, pridict_k562_data, pridict_k562mlh1d_data, pridict_adv_data, bin_min=0, bin_max=5, bin_width=0.5, legend=False)\n",
    "\n",
    "# save the figure\n",
    "plt.savefig(pjoin('../../dissertation/figures', 'editing-efficiency-log-adjusted.png'), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# undersampling the data with low editing efficiency\n",
    "import pandas as pd\n",
    "from os.path import join as pjoin\n",
    "import numpy as np\n",
    "\n",
    "# load the data\n",
    "dp_data = pd.read_csv(pjoin('std', 'std-dp-hek293t-pe2.csv'))\n",
    "pridict_hek_data = pd.read_csv(pjoin('std', 'std-pd-hek293t-pe2.csv'))\n",
    "pridict_k562_data = pd.read_csv(pjoin('std', 'std-pd-k562-pe2.csv'))\n",
    "pridict_k562mlh1d_data = pd.read_csv(pjoin('std', 'std-pd-k562mlh1d-pe2.csv'))\n",
    "pridict_adv_data = pd.read_csv(pjoin('std', 'std-pd-adv-pe2.csv'))\n",
    "\n",
    "# undersample the low editing efficiency data\n",
    "def undersample(df: pd.DataFrame, frac: float, threshold: int) -> pd.DataFrame:\n",
    "    df_low = df[df['editing-efficiency'] < 10]\n",
    "    df_high = df[df['editing-efficiency'] >= 10]\n",
    "    df_low = df_low.sample(frac=frac)\n",
    "    return pd.concat([df_low, df_high])\n",
    "\n",
    "dp_data = undersample(dp_data, 0.1, 10)\n",
    "pridict_hek_data = undersample(pridict_hek_data, 0.1, 10)\n",
    "pridict_k562_data = undersample(pridict_k562_data, 0.1, 10)\n",
    "pridict_k562mlh1d_data = undersample(pridict_k562mlh1d_data, 0.1, 10)\n",
    "pridict_adv_data = undersample(pridict_adv_data, 0.1, 10)\n",
    "\n",
    "\n",
    "plot_histogram(dp_data, pridict_hek_data, pridict_k562_data, pridict_k562mlh1d_data, pridict_adv_data, bin_min=0, bin_max=100, bin_width=10, legend=False)\n",
    "\n",
    "# save the figure\n",
    "plt.savefig(pjoin('../../dissertation/figures', 'editing-efficiency-undersample.png'), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qunatile transform the data\n",
    "import pandas as pd\n",
    "from os.path import join as pjoin\n",
    "import numpy as np\n",
    "\n",
    "# load the data\n",
    "dp_data = pd.read_csv(pjoin('std', 'std-dp-hek293t-pe2.csv'))\n",
    "pridict_hek_data = pd.read_csv(pjoin('std', 'std-pd-hek293t-pe2.csv'))\n",
    "pridict_k562_data = pd.read_csv(pjoin('std', 'std-pd-k562-pe2.csv'))\n",
    "pridict_k562mlh1d_data = pd.read_csv(pjoin('std', 'std-pd-k562mlh1d-pe2.csv'))\n",
    "pridict_adv_data = pd.read_csv(pjoin('std', 'std-pd-adv-pe2.csv'))\n",
    "\n",
    "# quantile transform the editing efficiency\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "qt = QuantileTransformer(n_quantiles=1000, output_distribution='uniform')\n",
    "dp_data['editing-efficiency'] = qt.fit_transform(dp_data['editing-efficiency'].values.reshape(-1, 1))\n",
    "pridict_hek_data['editing-efficiency'] = qt.transform(pridict_hek_data['editing-efficiency'].values.reshape(-1, 1))\n",
    "pridict_k562_data['editing-efficiency'] = qt.transform(pridict_k562_data['editing-efficiency'].values.reshape(-1, 1))\n",
    "pridict_k562mlh1d_data['editing-efficiency'] = qt.transform(pridict_k562mlh1d_data['editing-efficiency'].values.reshape(-1, 1))\n",
    "pridict_adv_data['editing-efficiency'] = qt.transform(pridict_adv_data['editing-efficiency'].values.reshape(-1, 1))\n",
    "\n",
    "plot_histogram(dp_data, pridict_hek_data, pridict_k562_data, pridict_k562mlh1d_data, pridict_adv_data, bin_min=-0.3, bin_max=1.3, bin_width=0.16, legend=False)\n",
    "\n",
    "# save the figure\n",
    "plt.savefig(pjoin('../../dissertation/figures', 'editing-efficiency-quantile-transform.png'), dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing the performance of XGBoost model trained on the adjusted datasets\n",
    "# with 5 fold cross validation\n",
    "import scipy.stats\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from os.path import join as pjoin\n",
    "import torch\n",
    "\n",
    "performance = {}\n",
    "\n",
    "def undersample(df: pd.DataFrame, frac: float, threshold: int) -> pd.DataFrame:\n",
    "    df_low = df[df['editing-efficiency'] < 10]\n",
    "    df_high = df[df['editing-efficiency'] >= 10]\n",
    "    df_low = df_low.sample(frac=frac)\n",
    "    return pd.concat([df_low, df_high])\n",
    "\n",
    "adjustment_shorthands = {\n",
    "    'weighted-mse': 'WMSE',\n",
    "    'quantile-transform': 'QT',\n",
    "    'original': 'OG',\n",
    "    'log-adjusted': 'LA',\n",
    "    'undersample': 'US'\n",
    "}\n",
    "\n",
    "for adjustment in ['weighted-mse', 'quantile-transform','original', 'log-adjusted', 'undersample']:\n",
    "    print(adjustment)\n",
    "    correlation = defaultdict(list)\n",
    "    for data_file in ['ml-dp-hek293t-pe2.csv', 'ml-pd-hek293t-pe2.csv', 'ml-pd-k562-pe2.csv', 'ml-pd-k562mlh1d-pe2.csv', 'ml-pd-adv-pe2.csv']:\n",
    "        print(data_file)\n",
    "        data = pd.read_csv(pjoin('conventional-ml', data_file))\n",
    "        if adjustment == 'log-adjusted':\n",
    "            data['editing-efficiency'] = np.log1p(data['editing-efficiency'])\n",
    "        elif adjustment == 'undersample':\n",
    "            data = undersample(data, 0.1, 10)\n",
    "        elif adjustment == 'quantile-transform':\n",
    "            qt = QuantileTransformer(n_quantiles=1000, output_distribution='uniform')\n",
    "            data['editing-efficiency'] = qt.fit_transform(data['editing-efficiency'].values.reshape(-1, 1))\n",
    "\n",
    "        for fold in range(5):\n",
    "            print(f'Fold: {fold+1}')\n",
    "            train = data[data['fold'] == fold]\n",
    "            test = data[data['fold'] != fold]\n",
    "            \n",
    "            print(train.columns)\n",
    "            \n",
    "            train_features = train.iloc[:, :24].values\n",
    "            train_targets = train['editing-efficiency'].values\n",
    "            test_features = test.iloc[:, :24].values\n",
    "            test_targets = test['editing-efficiency'].values\n",
    "            \n",
    "            sample_weights = np.clip(np.exp(6 * (np.log(train_targets + 1) - 3) + 1), 0, 5)\n",
    "            \n",
    "            model = xgb.XGBRegressor(verbosity=2, max_depth=3, n_estimators=200)\n",
    "            if adjustment == 'weighted-mse':\n",
    "                model.fit(train_features, train_targets, sample_weight=np.clip(np.exp(6 * (np.log(train_targets + 1) - 3) + 1), 0, 5))\n",
    "            else:\n",
    "                model.fit(train_features, train_targets)\n",
    "            \n",
    "            # evaluate\n",
    "            results = model.predict(test_features)\n",
    "            # flatten the results\n",
    "            results = results.flatten()\n",
    "            \n",
    "            if adjustment == 'log-adjusted':\n",
    "                results = np.expm1(results)\n",
    "            elif adjustment == 'quantile-transform':\n",
    "                results = qt.inverse_transform(results.reshape(-1, 1))\n",
    "                results = results.flatten()\n",
    "            pearson = np.corrcoef(test_targets, results)[0, 1]\n",
    "            spearman = scipy.stats.spearmanr(test_targets, results)[0]\n",
    "            print(f'Pearson: {pearson}, Spearman: {spearman}')\n",
    "            correlation[data_file.split('.')[0]].append((pearson, spearman))\n",
    "            \n",
    "    performance[adjustment_shorthands[adjustment]] = correlation\n",
    "    \n",
    "# save the performance\n",
    "performance = pd.DataFrame(performance)\n",
    "print(performance)\n",
    "performance.to_csv(pjoin('performance', 'xgboost-performance.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot both the pearson and spearman correlation performance\n",
    "# each figure contains the performance of the model on the different datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import join as pjoin\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ast\n",
    "\n",
    "performance = pd.read_csv(pjoin('performance', 'xgboost-performance.csv'), index_col=0)\n",
    "f_size = 14\n",
    "palette = iter(sns.color_palette('Spectral', 10))\n",
    "\n",
    "for data in ['ml-dp-hek293t-pe2', 'ml-pd-hek293t-pe2', 'ml-pd-k562-pe2', 'ml-pd-k562mlh1d-pe2', 'ml-pd-adv-pe2']:\n",
    "    # plot the performance of the model on the different datasets\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    \n",
    "    \n",
    "    performance_data = performance.loc[data]\n",
    "    performance_data = performance_data.apply(lambda x: ast.literal_eval(x))\n",
    "    # split performance data into pearson and spearman\n",
    "    performance_pearson = performance_data.apply(lambda x: [i[0] for i in x])\n",
    "    performance_spearman = performance_data.apply(lambda x: [i[1] for i in x])\n",
    "    performance_data = [performance_pearson, performance_spearman]\n",
    "    \n",
    "\n",
    "    for ind, metric in enumerate(['pearson', 'spearman']):\n",
    "        # remove grid\n",
    "        ax[ind].grid(False)\n",
    "        # no top and right axis\n",
    "        ax[ind].spines['top'].set_visible(False)\n",
    "        ax[ind].spines['right'].set_visible(False)\n",
    "        # plot as boxplot\n",
    "        c = next(palette)\n",
    "        print(performance_data[ind].apply(lambda x: np.mean(x)))\n",
    "        sns.barplot(data=performance_data[ind].apply(lambda x: np.mean(x)), ax=ax[ind], color=c)\n",
    "        # get the y limits\n",
    "        y_min, y_max = ax[ind].get_ylim()\n",
    "        # show samples of the data as stripplot\n",
    "        sns.stripplot(data=performance_data[ind], ax=ax[ind], color=c,  dodge=True, size=1, jitter=True)\n",
    "        # set the y limits\n",
    "        ax[ind].set_ylim(y_min, y_max)\n",
    "        # remove the - in the labels\n",
    "        ax[ind].set_xlabel(ax[ind].get_xlabel().replace('-', ' '))\n",
    "        ax[ind].set_ylabel(f'{metric} correlation')\n",
    "\n",
    "        # set the font size of the labels and ticks\n",
    "        ax[ind].tick_params(axis='both', which='major', labelsize=f_size)\n",
    "        ax[ind].set_xlabel(ax[ind].get_xlabel(), fontsize=f_size)\n",
    "        ax[ind].set_ylabel(ax[ind].get_ylabel(), fontsize=f_size)\n",
    "        \n",
    "        # rotate the x labels\n",
    "        ax[ind].tick_params(axis='x', rotation=30)\n",
    "        ax[ind].set_xticklabels(ax[ind].get_xticklabels(), rotation=30)\n",
    "        \n",
    "    plt.tight_layout()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
