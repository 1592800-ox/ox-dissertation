\chapter{Method}

\minitoc

\section{Curating a Benchmarking Dataset}

\label{sec:datasets}

Before starting the development of novel models, to establish a common ground for evaluating the performance of machine learning models, a benchmarking dataset was curated using data from various literatures. The main data source is the 2024 study by Mathis et al. which has the most diverse editing types ranging from 1-to-5bp replacement to 1-to-15bp deletions and insertions\cite{mathisMachineLearningPrediction2024}. It is complimented by the DeepPrime dataset, as it has a much bigger HEK293T dataset ($\sim290,000$ vs $\sim22,000$ for DeepPrime and PRIDICT respectively) as well as a wider range of cell type and PE pairs that can be used for fine-tuning the models to achieve better generalizability and higher impact\cite{yuPredictionEfficienciesDiverse2023}. 

The dataset was parsed to a uniform format to preserve all essential information required by the models while also limiting the number of fields for better readability and portability. The standardized format also allows easier conversion between datasets, with each data source requiring only two parsing functions, one to convert the model dataset to standard format and one to convert from standard format to the format required by other models. 

The standard (std) format contains the following fields:

\begin{itemize}[itemsep=-0mm]
    \item \textcolor{blue}{cell-line}: the cell line used in the experiment
    \item \textcolor{blue}{group-id}: the id of the target loci, used for grouping the data from the same target loci together during cross validation
    \item \textcolor{blue}{mut-type}: the type of mutation introduced by the prime editor, 0 for replacements, 1 for insertions, 2 for deletions
    \item \textcolor{blue}{wt-sequence/mut-sequence}: the 100bp/74bp long wild-type/mutated target sequence starting from 10bp/4bp upstream of the protospacer(PRIDCT/DeepPrime dataset)
    \item \textcolor{blue}{protospacer-location}: the location of protospacer sequence complementary to ngRNA in the wild-type sequence 
    \item \textcolor{blue}{pbs-location}: the location of primer binding site in the wild-type sequence
    \item \textcolor{blue}{rtt-location-wt/rtt-location-mut}: the location of reverse transcription template in the wild-type/edited sequence
    \item \textcolor{blue}{lha-location}: the location of left homology arm in the wild-type sequence
    \item \textcolor{blue}{rha-location-wt/rha-location-mut}: the location of right homology arm in the wild-type/edited sequence
    \item \textcolor{blue}{spcas9-score}: precalculated SpCas9 score
    \item \textcolor{blue}{editing-result}: empirically observed editing efficiency
\end{itemize}

The locations are separated into two columns in the format of "start:end", where start is the 0-based index of the first base of the sequence, and end is for the location of the last base (non-inclusive). All sequences were read in the direction of protospacer to PAM for easier interpretation, thus the ngRNA was read in the $5'$ to $3'$ direction, while the extension was read from $3'$ to $5'$. This is often different from previous studies that read from $5'$ end to the $3'$ end of the pegRNA, regardless of its relative position to the target sequence. 

Additionally, differentiation was made between the wild-type and mutated sequences for the location of the RTT and RHA, as the editing process can change the length of those regions and shift their locations.

The datasets were named as ``\{data-source\}-\{cell-line\}-\{PE\}". `data-source' is the name of the study that the data was extracted from (pd for PRIDICT, dp for DeepPrime), while `cell-line' and `PE' are self explanatory. The datasets are split into 5 folds for cross validation based on the group id assigned to each target loci. Edits on the same target loci have the same group id, and are thus placed in the same fold to prevent training data leakage invalidating the testing result. 

Five main datasets of size greater than 20,000 were used frequently for training, tuning and evaluation in this study: DeepPrime HEK293T PE2, PRIDICT2.0 HEK293T, PRIDICT2.0 K562, PRIDICT2.0 K562MLH1d, and PRIDICT2.0 Adv. The HEK293T cells refer to the human embryonic kidney cells\cite{kavsanImmortalizedCellsOne2011}; `Adv' refers to prime editing performed in-vivo (in a living organism) in mouse liver cells; K562 cells were derived from a chronic myelogenous leukemia patient\cite{lozzioMultipotentialLeukemiaCell1981} (K562MLH1d refers to K562 cells with MMR inhibition using the MLH1dn proteins). Other smaller datasets were used for fine tuning models trained with the DeepPrime HEK293T PE2 dataset, such as the DeepPrime HEK293T PE2-Max dataset.

The editing efficiencies were calculated using the method suggested by Kim et al.\cite{kimPredictingEfficiencyPrime2021} in \autoref{eq:efficiency}, which was also used by PRIDICT:
\begin{equation}
    \label{eq:efficiency}
    \begin{split}
        \text{Editing Efficiency} =& \frac{\text{Edited Count}}{\text{Total Count}} \times 100\% \\
    \end{split}
\end{equation}
Where:
\begin{equation}
    \begin{split}
        \text{Edited Count} =& \text{Read Counts With Intended Edit at Target Site} - \\
        &( \text{Total Read Counts} \times \text{Background Editing  Rate} ) \\
        \text{Total Count} =& \text{Total Read Counts} - (\text{Total Read Counts} \times \\ &\text{Background Editing  Rate})
    \end{split}
\end{equation}
        
Background editing rate refers to the percentage of target loci that were edited without being transfected with the prime editor. Subtracting the background edits from the total read counts gives the estimated true number of edits that were introduced by the prime editor.

\section{Data Engineering}
\label{sec:data-engineering}

A conspicuous issue with the dataset is the imbalance in target values. Shown in Figure \ref{fig:imbalanced-original}, the distribution of the editing efficiency is heavily skewed towards the lower end, with a large number of pegRNAs having an an efficiency of around 0, especially in the DeepPrime HEK293T and PRIDICT Adv datasets. This can cause the model to be less accurate in predicting the higher efficiency pegRNAs, as the model would be more inclined to predict lower efficiency to minimize the loss. 

\begin{figure}
    \centering
    \subfigure[Original]{
        \label{fig:imbalanced-original}
        \includegraphics[width=0.45\textwidth]{editing-efficiency-comparison.png}
    }
    \subfigure[Log Adjusted]{
        \label{fig:imbalanced-log}
        \includegraphics[width=0.45\textwidth]{editing-efficiency-log-adjusted.png}
    }
    \subfigure[Undersampling]{
        \label{fig:imbalanced-undersampling}
        \includegraphics[width=0.45\textwidth]{editing-efficiency-undersample.png}
    }
    \subfigure[Quantile transformation]{
        \label{fig:imbalanced-quantile}
        \includegraphics[width=0.45\textwidth]{editing-efficiency-quantile-transform.png}
    }
    \caption[Target Distribution Imbalance]{The distribution of the editing efficiency in the DeepPrime HEK293T PE2 (DP) dataset, the PRIDICT2.0 HEK293T (PD HEK293T) dataset, the PRIDICT2.0 K562 (PD K562) as well as K562 MMR deficient (PD K562MLH1dn) dataset, and the PRIDICT2.0 Adv (PD ADV) dataset \textbf{(a)}, followed by the distribution after several adjustments \textbf{(b-d)}. Due to the significant difference in dataset sizes, DeepPrime datasets and PRIDICT datasets used different y-axis scale (left and right for DeepPrime and PRIDICT respectively): 
    \textbf{(a)} the distribution of original editing efficiency in the datasets. editing efficiency on the x axis is limited to [0, 100], with bin size of 10; \textbf{(b)} the distribution of the log adjusted editing efficiency; \textbf{(c)} the distribution of the editing efficiency after undersampling the data with editing efficiency $<10$ with a ratio of 10:1 (10\% of the original data were preserved); \textbf{(d)}, the distribution of the editing efficiency after uniformly quantile transformed.}
    \label{fig:imbalanced}
\end{figure}

Although the research for imbalanced regression task is relatively limited compared to classification, a number of methods have been proposed\cite{krawczykLearningImbalancedData2016}. The simplest method is to adjust the target values so that better balance can be achieved in the projected space. Log transformation is a suitable method for the editing efficiency dataset, as it increases the distance between the lower values while keeping the higher values close (Figure \ref{fig:imbalanced-log}). The numpy \verb|log1p| function was used to prevent the transformation from being undefined when the target value is 0. During inference, the predicted values were transformed back to the original scale using the \verb|expm1| function. Undersampling is also a useful technique, by removing the majority of the lower efficiency pegRNAs, the model can be trained to better predict the higher efficiency pegRNAs\cite{torgoResamplingStrategiesRegression2015}. 
The undersampling ratio was set to 10:1, with the majority of the pegRNAs with efficiency $<10$ removed (Figure \ref{fig:imbalanced-undersampling}). The undersampling ratio and threshold for determining the part of the data to undersample was tuned in the list of [20:1, 15:1, 10:1, 5:1] and [5, 10, 15] respectively using a XGBoost model trained on the first fold of DeepPrime HEK293T PE2 dataset. Quantile transformation was also considered, as it can transform the target values to a uniform distribution, which can help the model to better predict the target values. The quantile transformation was performed using the \verb|QuantileTransformer| class from the scikit-learn library (Figure \ref{fig:imbalanced-quantile}), which first projected the continuous target values to a set of discretized values, then transformed the discretized values to a uniform distribution. SMOTE (Synthetic Minority Over-sampling Technique) was also considered, but the generation of synthetic data is very difficult for the prime editing prediction task, as the pegRNA sequence is highly structured and the synthetic data generated may not be biologically feasible. 

In addition to adjusting the target values, the loss function can also be weighted to assign different importance to different target values. Sample weights can be used to penalize the model more for mispredicting the higher efficiency pegRNAs. 

DeepPrime applied a weighted loss function of the editing efficiency tuned for their dataset:
\begin{equation}
    \text{weight} = \text{min}(\exp(6(\log(x+1)-3)+1),5)
\end{equation}
where x is the measured editing efficiency. The weight is then used in the loss function to penalize the model more for mispredicting the higher efficiency pegRNAs.

At the mean time, PRIDICT did not report any special treatment for the imbalanced dataset, possibly because the imbalance is less severe in their dataset (Figure \ref{fig:imbalanced-original}, the PRIDICT HEK293T and K562 datasets have a more balanced distribution with large amount of examples with high editing efficiency). 

To verify if the adjustments had made any significant difference on the model's performance, a simple two layer MLP with 128 hidden units in each layer was trained on the DeepPrime HEK293T PE2 dataset using the original, log adjusted, undersampled and quantile transformed datasets. An additional model using the DeepPrime adjusted MSE was also tested. The models were trained using the Adam optimizer with a learning rate of 0.005, and the learning rate was adjusted using the CosineAnnealingLRWarmRestart scheduler, with a T\_0 (epoch of the first restart) of 15 and T\_mult (the factor to extend the restart intervals) of 1 to escape local minima. 

\begin{figure}
    \centering
    \vspace{-3mm} % Reduce vertical space between subfigures
    \subfigure[DP HEK293T]{
        \centering
        \includegraphics[width=0.8\textwidth]{adjustment-dp-hek293t-performance.png}
        \label{fig:dp-hek293t-adjustment-performance}
    }
    \vspace{-3mm}
    \subfigure[PD HEK293T]{
        \centering
        \includegraphics[width=0.8\textwidth]{adjustment-pd-hek293t-performance.png}
        \label{fig:pd-hek293t-adjustment-performance}
    }
    \vspace{-3mm}
    \subfigure[PD K562]{
        \centering
        \includegraphics[width=0.8\textwidth]{adjustment-pd-k562-performance.png}
        \label{fig:pd-k562-adjustment-performance}
    }
    \vspace{-3mm}
    \subfigure[PD K562MLH1dn]{
        \centering
        \includegraphics[width=0.8\textwidth]{adjustment-pd-k562mlh1d-performance.png}
        \label{fig:pd-k562mlh1d-adjustment-performance}
    }
    \vspace{-3mm}
    \subfigure[PD Adv]{
        \centering
        \includegraphics[width=0.8\textwidth]{adjustment-pd-adv-performance.png}
        \label{fig:pd-adv-adjustment-performance}
    }
    \caption[MLP model performance comparison after adjustments]{Performance comparison of the MLP model trained on all five folds of the five main datasets. The asterisks indicating significance are only shown when an adjustment performed significantly \textbf{better} than training on original data. The bar plot shows the mean performance of the MLP model trained on DeepPrime and PRIDICT HEK293T \textbf{(a, b)}, PRIDICT2 K562 (PD K562\textbf{(c)}), PRIDICT K562MLH1dn \textbf{(d)}, and PRIDICT Adv \textbf{(e)} datasets using the log adjusted (LA), undersampled (US), quantile transformed (QT), and DeepPrime weighted MSE (WMSE) adjustments, in addition to the original dataset (OG) across the five folds. The performance of each fold is shown as a point in the strip plot on top of the matching bar.}
    \label{fig:adjustment-performance}
\end{figure}

The performance of MLP model trained on the five datasets using the five adjustments are shown in \autoref{fig:adjustment-performance}. Significant improvements were only observed in three occasions ($p<0.05$, paired t-test between performance in each fold). The log adjustment improved both the Pearson ($p<0.001$) and Spearman ($p<0.01$) correlation of the model on the DeepPrime HEK293T PE2 dataset, as well as the Pearson correlation on the PRIDICT K562 dataset ($p<0.001$), while undersampling noticeably improved both correlation coefficients on the PRIDICT Adv dataset ($p<0.001$). However, the improvement is not universal, as significantly lower performance for undersampling was observed on all datasets other than PRIDICT Adv ($p<0.001$). At the same time, log adjustment resulted in severely reduced performance PRIDICT HEK293T and K562MLH1dn datasets ($p<0.001$). 

This narrows down the options to log adjustment and undersampling. numpy provides a very efficient implementation of the log1p and expm1 functions, making the log adjustment a very attractive option. Meanwhile, the undersampling method is even simpler to implement, and it has the added benefit of reducing the training time, as the model would have to process less data. However, on more complex models, the undersampling method significantly reduce the training size, which may lead to overfitting and reduced validation performance.

% reduce subfigure vertical space
\begin{figure}
    \centering
    \vspace{-3mm} % Reduce vertical space between subfigures
    \subfigure[DP HEK293T]{
        \includegraphics[width=0.8\textwidth]{adjustment-deepprime-dp-hek293t-performance.png}
        \label{fig:deepprime-dp-hek293t-adjustment-performance}
    }
    \vspace{-3mm} % Reduce vertical space between subfigures
    \subfigure[PD HEK293T]{
        \centering
        \includegraphics[width=0.8\textwidth]{adjustment-deepprime-pd-hek293t-performance.png}
        \label{fig:deepprime-pd-hek293t-adjustment-performance}
    }
    \vspace{-3mm} 
    \subfigure[PD K562]{
        \centering
        \includegraphics[width=0.8\textwidth]{adjustment-deepprime-pd-k562-performance.png}
        \label{fig:deepprime-pd-k562-adjustment-performance}
    }
    \vspace{-3mm} 
    \subfigure[PD K562MLH1dn]{
        \centering
        \includegraphics[width=0.8\textwidth]{adjustment-deepprime-pd-k562mlh1d-performance.png}
        \label{fig:deepprime-pd-k562mlh1d-adjustment-performance}
    }
    \vspace{-3mm} 
    \subfigure[PD Adv]{
        \centering
        \includegraphics[width=0.8\textwidth]{adjustment-deepprime-pd-adv-performance.png}
        \label{fig:deepprime-pd-adv-adjustment-performance}
    }
    \caption[DeepPrime model performance comparison after adjustments]{
        Similar to \autoref{fig:adjustment-performance}, the performance comparison of the DeepPrime model trained on  DeepPrime HEK293T PE2 \textbf{(a)}, PRIDICT HEK293T  \textbf{(b)},  PRIDICT K562  \textbf{(c)},  PRIDICT K562MLH1d  \textbf{(d)}, and  PRIDICT Adv dataset \textbf{(e)} using the log adjusted (LA), undersampled (US), and the original dataset (OG). 
    }
    \label{fig:deepprime-adjustment-performance}
\end{figure}

To further verify the necessity of the adjustments, the DeepPrime model with log adjusted and undersampled target were trained on the five datasets using the optimized hyperparameters reported by Yu et al in their study (detailed training process to be discussed in \autoref{sec:training-deepprime-pridict})\cite{yuPredictionEfficienciesDiverse2023}. The performance of the models were then evaluated using Pearson and Spearman correlation and compared to the result trained on the original model, shown in \autoref{fig:deepprime-adjustment-performance}.

As expected, the relative performance of the undersampling adjustment took a nosedive when training the DeepPrime model with far more parameters than the simple MLP model. Significantly lower performance was observed on all datasets ($p<0.01$) trained with undersampled training data. The log adjustment, on the other hand, showed a significant improvement in terms of Spearman's $\rho$ on the DeepPrime HEK293T dataset ($p<0.001$), the PRIDICT K562 dataset ($p<0.001$), and the PRIDICT Adv dataset ($p<0.01$). However, at the same time, significant decrease in performance was observed in terms of Pearson's R for the DeepPrime and PRIDICT HEK293T datasets ($p<0.05$). As a result, I decided to follow PRIDICT's protocol and not adjust the target values. 

Another problem with the dataset is the lack of variety in the PBS length and location in the PRIDICT dataset. For some reason, all PBS sequences start at 5bp downstream of the protospacer start location and have a length of 13bp, ending before the nick site at 3bp upstream of PAM. This can significantly impact the model's performance during inference on data with arbitrary PBS length and location, especially if PBS related features were directly used by the models. 

To help the models trained on PRIDICT datasets work on unseen data supplied by the users, the suggested PBS length is fixed at 13bp when inferencing using models trained on PRIDICT dataset, with PBS always starting at 5bp downstream of the protospacer start location.



\section{Determinants of Prime Editing Outcome}
\label{sec:determinants}

\begin{figure}
    \subfigure[DeepPrime]{
        \label{fig:shap-dp-pe2-hek}
        \includegraphics[width=0.98\textwidth]{shap-dp-hek293t-pe2.png}}
    \subfigure[Deletion]{
        \includegraphics[width=0.33\textwidth]{shap-pd-hek293t-pe2-delete.png}
    }%
    \subfigure[Insertion]{
        \includegraphics[width=0.33\textwidth]{shap-pd-hek293t-pe2-insert.png}
    }%
    \subfigure[Replacement]{
        \includegraphics[width=0.33\textwidth]{shap-pd-hek293t-pe2-replace.png}
    }%
    \caption[SHAP analysis for DeepPrime and PRIDICT datasets]{SHAP analysis for DeepPrime and PRIDICT datasets on HEK293T cell line: \textbf{(a)} Top 15 determinants from SHAP analysis using all of DeepPrime HEK293T Datasets. \textbf{(b-d)} Top 10 determinants from SHAP analysis using PRIDICT HEK293T data for each individual editing types. The color of the individual data point shows their normalized values from high (red) to low (blue). A number of shorthands have been used to improve the clarity of the illustrations. gcc for GC Content (different from GC count); et for editing type; tm for melting temperature; mfe for minimum free energy; n ap \# for base n at protospacer position \#; max cns for the length of maximum consecutive sequence of base n.} 
    \label{fig:shap}
\end{figure}

As discussed in \autoref{sec:motivation}, most of the recent deep learning models surveyed in the literature (DeepPrime\cite{yuPredictionEfficienciesDiverse2023} and PRIDICT\cite{mathisPredictingPrimeEditing2023,mathisMachineLearningPrediction2024}) follow a similar two model structure. The first model is a deep neural network taking in raw sequence data, and the second model is a smaller MLP/regression model that takes as input the features extracted from the PBS, RTT and ngRNA. 

As a result, it is crucial to find the most prominent determinants of prime editing outcomes and to extract the most informative features for describing the pegRNA. A large number of features were selected from various sources as different studies focus on different aspects of pegRNA. Shapley Additive Explanations (SHAP) with XGBoost regressor was performed to rank the features based on their importance. The full list of features investigated can be found in \autoref{appendix:features}. 

A number of Python libraries were utilized when extracting the biological features. Melting temperature of the sequences were calculated using the 'biopython' library\cite{cockBiopythonFreelyAvailable2009}, while the minimum free energy was calculated with the ViennaRNA package and adjusted using sequence length to ensure it's mostly influenced by sequence's nucleotide order and composition\cite{lorenzViennaRNAPackage2011,trottaNormalizationMinimumFree2014}, and finally, the SpCas9 scores were provided by the `DeepSpCas9' model\cite{kimSpCas9ActivityPrediction2019}. Other features such as GC content were extracted using simple Python string processing functions.

SHAP analysis was first conducted on the largest DeepPrime PE2 HEK293T dataset to provide a robust identification of the most informative features (Figure \ref{fig:shap-dp-pe2-hek}). The features were then sorted based on their importance (absolute mean SHAP value), and the top 24 features were selected for the model, matching the design optimized by DeepPrime and PRIDICT (both use 24 features)\cite{yuPredictionEfficienciesDiverse2023,mathisMachineLearningPrediction2024}. Additionally, to identify the influence of the features on different editing types, SHAP analysis was also conducted on the PRIDICT2.0 HEK293T dataset with higher variety of edit lengths for each individual mutation type (\autoref{fig:shap}(b-d)).

The major determinants are as follows:
\begin{itemize}[itemsep=-0mm]
    \item \textcolor{red}{Editing type and length}: editing type had a significant impact on the editing efficiency, with the replacement type having higher efficiency than the insertion and deletion, as shown by the `et replacement' feature in Figure \ref{fig:shap-dp-pe2-hek}. The length of the edit also had great influence on editing result, and its important was more pronounced for insertions and deletions than for replacements. As expected, longer insertions and deletions had lower efficiency due to the increased difficulty in the annealing and repair process.
    \item \textcolor{red}{Melting temperature}: melting temperature (Tm) refers to the temperature at which half of the RNA strand become unfolded or denatured. It is strongly correlated with the editing efficiency, possibly due to its influence on RNA structural stability. Its actual effect, however, was highly mixed in the DeepPrime dataset. A low Tm in RHA seemed to positively affect the editing result, while the opposite was true for PBS (`tm rha' and `tm pbs' in Figure \ref{fig:shap-dp-pe2-hek}). For the PRIDICT dataset, Tm had a more consistent positive effect on the editing efficiency, with a higher Tm in the RHA and spacer region leading to higher efficiency (`tm rha' and `tm spacer' in \autoref{fig:shap}(b-d)).
    \item \textcolor{red}{Minimum free energy}: minimum free energy (MFE) describes the lowest possible energy required for an RNA sequence to stay in a particular form\cite{lorenzViennaRNAPackage2011}. Thus, it should be inversely correlated with the stability of RNA structure and Tm. This was indeed true in the DeepPrime dataset, with a lower MFE in the extension sequence leading to higher editing efficiency (`mfe' extension in Figure \ref{fig:shap-dp-pe2-hek}). 
    \item \textcolor{red}{GC content/count}: GC count in the PBS sequence was shown as the most important feature for the DeepPrime dataset (`gc\#' in Figure \ref{fig:shap-dp-pe2-hek}), with prominent positive effect on editing efficiency. This is consistent with the observation made by Liu et al in their 2019 study introducing prime editors. They expected lower editing efficiency with low GC count in the PBS, due to the energetic requirements of hybridization of the nicked DNA strand to the PBS\cite{liudavidr.SearchandreplaceGenomeEditing2019}. At the same time, GC content also correlates to the minimum free energy and melting temperature, as GC base pairs have stronger hydrogen bond than AU base pairs, leading to a more stable RNA structure\cite{kimPredictingEfficiencyPrime2021}. As a result, the GC count and content in the RHA sequences had a similar positive effect on the editing efficiency in both DeepPrime and PRIDICT dataset (`gc\# rha' and `gcc rha'). 
    \item \textcolor{red}{Poly-T sequences}: Shown as `max cts' in \autoref{fig:shap}, the length of the longest consecutive T (poly-T) sequences in the cDNA of the pegRNA sequence had a clear negative impact on the editing efficiency. Poly-T sequences are known to cause premature termination of the RNA polymerase III (Pol III) during transcription of the pegRNA plasmids\cite{nielsenMechanismEukaryoticRNA2013}. This can significantly hinder the effectiveness of pegRNA transfection, ending the editing process before the components are even produced.
    \item \textcolor{red}{SpCas9 score}: DeepSpCas9 is a deep learning model that estimates the activity of the SpCas9 protein on a target loci (higher is better), and has been shown to be a good indicator of prime editing efficiency\cite{kimPredictingEfficiencyPrime2021}. This is likely due the SpCas9's role in the prime editing process, as it is responsible for the initial scission of the the target loci. 
    \item \textcolor{red}{PAM disruption}: it was shown that prime editors can sometimes rebind to the edited sequences and induce unintended edits, lowering the editing efficiency\cite{liudavidr.SearchandreplaceGenomeEditing2019}. Thus, the disruption of the PAM sequence is beneficial for the editing efficiency as it prevents the reannealing of the pegRNA to the edited sequence. 
    \item \textcolor{red}{LHA/RHA/PBS length}: the length of rha (right homology arm, RTT overhang) was a significant determinant for all editing types (`rha length'). A short rha length can severely hinder prime editing efficiency, while the positive effect of a long rha is not as pronounced. This suggests the existence of a minimum threshold for rha length below which the edited strand cannot efficiently anneal to the unedited strand, consistent with the findings of Yu et al in 2023, recommending a rha length of at least 7nt\cite{yuPredictionEfficienciesDiverse2023}. At the same time, reported as edit position in some literatures, lha length corresponding to the distance between the PAM sequence and the edit location is also a significant feature for all editing types, especially for insertion and deletion (`lha length' in \autoref{fig:shap}(b, c)). A longer lha length adversely affects the editing efficiency, suggesting that although prime editors have less stringent requirements for PAM locations, editing should still be done close to the protospacer whenever possible. As for PBS, both a very short and a very long PBS have a negative impact on the editing efficiency (`pbs length' in Figure \ref{fig:shap-dp-pe2-hek}), indicating the existence of an optimal range for the length PBS.
    \item \textcolor{red}{Protospacer nucleotide composition}: in Figure \ref{fig:shap-dp-pe2-hek}, the nucleotide compositions from protospacer location 13 to 17 (nicking position) were all shown to have a significant impact. In particular, a guanine (G) at position 16 and a cytosine (C) at position 17 (`g ap 16', `c ap 17')
     were shown to have a positive effect on the editing efficiency, while a adenine (A) at position 13 and 14 (`a ap 13', `a ap 14') had a negative effect.
    This is consistent with Mathis et al's finding that the nucleotide composition from protospacer position 10 to 20 shows high integrated gradient scores, suggesting that the nucleotide composition in this region is highly influential in determining the editing efficiency\cite{mathisPredictingPrimeEditing2023}.
\end{itemize}

On top of the quantitatively invested sequence based features, higher level features have also been shown to impact prime editing efficiency:
\begin{itemize}[itemsep=-0mm]
    \item \textcolor{red}{pegRNA secondary structure}: the secondary structure of the pegRNA can impact the efficiency by inducing degradation of the 3' extension. The defunct pegRNA can still combine with target loci using its functioning 5' ngRNA, preventing functional prime editors from hybridization and thus lowering the editing efficiency\cite{nelsonEngineeredPegRNAsImprove2022}.
    \item \textcolor{red}{MMR Behaviour}: as briefly mentioned at the beginning of \autoref{sec:prime-editing-process}, the mismatch repair(MMR) system adversely affects the editing efficiency during step (e) of the prime editing process. The MMR system may reject the annealing of the editing strand to the target loci or  excise the edited sequence, preferring the original sequence\cite{chenEnhancedPrimeEditing2021}. 
\end{itemize}

These features are encapsulated in the editing cell type and PE version, and since we are developing one model for each cell line - PE pair instead of a multi-task model, they were not explicitly included in the feature list of this project. For example, epegRNAs(engineered pegRNA) are improved version of pegRNA that are designed for protection against 3' degradation with a more robust secondary structure than the original versions\cite{nelsonEngineeredPegRNAsImprove2022}. At the same time, PE4 and PE5 are newer versions of prime editors with MLH1dn proteins that can suppress the MMR system\cite{chenEnhancedPrimeEditing2021}, and the HEK293T cell line has weaker MMR activity than other cell lines such as HAP1(derived from a chronic myelogenous leukemia patient)\cite{mathisPredictingPrimeEditing2023}. 

\section{Conventional Machine Learning Models}
\label{sec:conventional-ml}

Using the top 24 extracted features, a number of conventional machine learning models were trained to serve as a baseline for the more complex models to be developed in this project. The models include Lasso and Ridge regression, Random Forest Regressor, Gradient Boosted Trees from the XGBoost library, and a Multi-Layer Perceptron.

Their hyperparameters were optimized on one fold of the DeepPrime HEK293T PE2 dataset using sklearn's GridSearchCV function, consistent with the protocol used by mathis et al during the development of PRIDICT. The MLP model was wrapped using the Skorch library to allow for the use of sklearn functions on Pytorch models. 

Most of the training of Pytorch models were completed on a Nvidia RTX 3080 laptop GPU, with some of fine tuning performed on the Apple M3 Pro chip using MPS (Metals Performance Shaders) to accelerate the training process.

The optimized parameters for the models are as follows:

\begin{itemize}[itemsep=-0mm]
    \item \textcolor{blue}{Lasso}: \verb|alpha=0.006158482110660266| (coefficient for penalty term for L1 regularization)
    \item \textcolor{blue}{Ridge}: \verb|alpha=494.17133613238286| (coefficient for penalty term for L2 regularization)
    \item \textcolor{blue}{Random Forest}: \verb|n_estimators=200| (number of trees to build), \verb|max_depth=10| (maximum depth of the trees)
    \item \textcolor{blue}{XGBoost}: \verb|n_estimators=200| (number of boosting rounds, similar to number of trees), \verb|max_depth=5| 
    \item \textcolor{blue}{MLP}: \verb|hidden_layer_sizes=(64, 64)| (two hidden layer with 64 units each), \verb|activation='relu'| (rectified linear unit activation), \verb|solver='adam'| (adaptive moment estimation solver), \verb|lr=0.005| (learning rate)
\end{itemize}

When training the models, MLP made a further validation split in the training data to reduce overfitting and improve the model's generalization. At the same time, since sklearn does not offer a monitor for validation loss, other models were fitted using the entire training data and trained until convergence.

The models' performances were evaluated with Pearson and Spearman correlation between predicted and measured efficiency, as per the protocol used by DeepPrime and PRIDICT, and the result of the conventional models on the five datasets are shown in \autoref{fig:conventional_ml_models_performance}. 

The Lasso and Ridge regression were the worst performing models, with similar performance between the two. This is to be expected, as they are both linear models and the relationship between the features and the target is likely non-linear. The Random Forest model performed significantly better than the linear models, but with a much longer training time. Another tree based method, XGBoost, had far superior efficiency ($\sim$300 times faster) compared to the Random Forest model, thanks to its GPU support and optimized implementation of the gradient boosting algorithm. XGBoost also achieved better or similar performance to Random Forest in terms of both Pearson and Spearman correlation The MLP model trailed closely behind the XGBoost model in terms of Pearson's $r$, but significantly outperformed the XGBoost model on multiple datasets in terms of Spearman's $\rho$ ($p<0.01$, paired t-test across five folds), which is a more important metric for this task as we are more interested in the ranking of the pegRNAs. The great performance of the MLP model validated the choice of PRIDICT and DeepPrime to use a MLP as both a feature encoder and the model head processing the concatenated output, and the same design was thus adopted in the Transformer model to be developed in this project.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{conventional_ml_models_performance.png}
    \caption[Conventional ML model performance comparison]{Performance comparison in terms of Pearson's $r$ (\textbf{left}) and Spearman's $\rho$ (\textbf{right}) for a number of conventional machine learning models on the DeepPrime HEK293T PE2 dataset and the PRIDICT datasets for four different cell lines. The models are Multi-Layer Perceptron(MLP), Random Forest Regressor(RF), Lasso, Ridge, and XGBoost. The performance of the models were evaluated using 5-fold cross validation, with the mean performance shown in the heatmap.}
    \label{fig:conventional_ml_models_performance}
\end{figure}


\section{Improving Performance with Ensemble Learning}
\label{sec:ensemble}

Although good results have been achieved with the conventional machine learning models as well as the deep learning solutions of DeepPrime and PRIDICT, better performance may be possible by fusing the predictions of multiple models using ensemble learning.
The idea behind ensemble learning is to combine the results of multiple weaker models through various voting methods to achieve better prediction than any individual model. It is a common practice in machine learning, and is frequently seen in the top performing models in competitions hosted by Kaggle\cite{dongSurveyEnsembleLearning2020}. 

In fact, most of the models discussed in this study so far are already using ensemble learning in some way. Random forrest trains multiple decision trees using a subset of features and samples to alleviates the problem of overfitting, while the XGBoost model is another ensemble of decision trees using optimized gradient boosting \cite{chenXGBoostScalableTree2016}. At the same time, DeepPrime and PRIDICT are effectively an ensemble of a deep learning model and a MLP, using another MLP as a meta learning to combine the representation produced by the two base models. 

To create a functioning ensemble of these models, a number of common supervised ensemble learning methods were evaluated, including weighted averaging, bagging, and adaptive boosting (AdaBoost). Due to the large quantity of models to be used in the ensembles, the initial training, tuning and testing were conducted on the PRIDICT2.0 HEK293T dataset for a balance between the size of the dataset and training time. 

% TODO: error distribution of the models

\subsection{Training DeepPrime and PRIDICT}
\label{sec:training-deepprime-pridict}

To begin with, in order to utilize the DeepPrime and PRIDICT models in the ensemble, they need to be retrained on all datasets available in this study. A training script was written for both models with the help of Skorch wrapper, similar to the one used by the MLP model. The scripts load the dataset, perform data conversions, and train the model using the hyperparameters optimized by the original authors. The hyperparameters selected by DeepPrime were listed in their study, while the parameters for PRIDICT were acquired by loading the pickled trained model's configuration. 

To verify the correctness of my implementation, the models were trained and evaluated on their respective datasets. The DeepPrime model achieved slightly lower performance than the one reported by Yu et al, with a mean Pearson's correlation of 0.75 on the DeepPrime HEK293T PE2 dataset, compared to the 0.77 reported in their study. PRIDICT also achieved slightly lower Spearman's correlation on the PRIDICT HEK293T dataset, with a mean of 0.81 compared to the 0.82 reported by Mathis et al. These differences could be a result of different seed used for train val split used during training on each fold, or other specific details of the training process, such as the parameters used by learning rate scheduler or the optimizer. Although their original performance for each fold was not reported and thus no statistical test was performed, the discrepancy of 0.02 in correlation is marginal, and the models were considered correctly implemented.

After verifying the correctness of the model implementation, to remove the added variance of feature selection, all models were reconfigured to use the top 24 features extracted from the SHAP analysis in this study. For the same reason, the MLP models encoding the features and outputting the final prediction were also reconfigured to use the same design utilized by DeepPrime so that the only variation between the deep learning models is the sequence encoding model. 

On top of the difference in architecture, another distinction between DeepPrime and PRIDICT is the handling of the input sequence data. Although both yu et al and mathis et al reported the usage of one-hot encoding in their publication, the actual implementations are very different. DeepPrime directly inputted the one-hot encoded sequence into the convolutional network, while PRIDICT used an embedding layer to convert the one-hot encoded sequence into a dense representation. The dense representation is considered very beneficial in the NLP task as it significantly reduces the dimensionality of the input data, and can capture the semantic meaning of the the relationship between the words\cite{goldbergPrimerNeuralNetwork2015}. This can be helpful in other bioinformatic tasks where a segment of genome sequence is considered as a single word, resulting in a large vocabulary\cite{cegliaIdentificationTranscriptionalPrograms2023}. However, since we are working on the nucleotide level of representation, the vocabulary size is very small (4), and the semantic meanings of the nucleotides are not as pronounced. For these reasons, the embedding layer may not be necessary for the task at hand.

To determine the necessity of the PRIDCT embedding, the DeepPrime model was reconfigured to use the same input processing pipeline as the PRIDICT model, with the one-hot encoded sequence first inputted into an embedding layer before being fed into the convolutional network, using an embedding size of 4 to match the original input size. The model was then trained and evaluated on the DeepPrime HEK293T PE2 dataset, with no significant differences across five folds (0.74 vs 0.75 for using and not using embedding, respectively, p > 0.1, paired t-test). This suggests that, as expected, the embedding layer is not necessary for the prediction of prime editing efficiency on the DeepPrime dataset.

Additionally, PRIDICT's input data has boolean functional annotations denoting the each nucleotide's corresponding section in the pegRNA (PBS, RTT, protospacer), and are used to guide the local attention mechanism to focus on the relevant parts of the sequence. However, quantitatively testing the annotation would result in significant rewrite of DeepPrime and PRIDICT model's architecture. Limited by the time and resources, I decided to perform this comparison on the transformer model to be developed in this study. The transformer model would have a more flexible structure that allows for easy modification of different sections of the architecture. 

\subsection{Error Analysis}

To get an understanding of the suitability of different ensembling methods and establish an expectation of how helpful ensemble learning would be with the given set of models, the error distribution of the individual models were analyzed using the DeepPrime HEK293T dataset. The prediction of the five folds were combined to form the final prediction of the full dataset, and the standardized error of the individual models on each example was calculated.

\begin{figure}
    \centering
    \subfigure[][Error Distribution]{
        \centering
        \includegraphics[width=\textwidth]{error_comparison_dp-hek293t-pe2.png}
        % \caption{Error Distribution of the Individual Models}
        \label{fig:error-distribution}
    }
    \subfigure[][Pearson Correlation]{
        \centering
        \includegraphics[width=0.49\textwidth]{error_correlation_dp-hek293t-pe2_pearson.png}
        % \caption{Pearson Correlation of the Error of the Individual Models}
        \label{fig:pearson-correlation}
    }%
    \subfigure[][Spearman Correlation]{
        \centering
        \includegraphics[width=0.49\textwidth]{error_correlation_dp-hek293t-pe2_spearman.png}
        % \caption{Spearman Correlation of the Error of the Individual Models}
        \label{fig:spearman-correlation}
    }
    \caption[Error Analysis of the Individual Models]{Error Analysis of the Individual Models on the DeepPrime HEK293T dataset: \textbf{(a)} The distribution of the error of the individual models at each example, smoothened using a moving window of size 1000, and downsampled with a ratio of 100:1 for better visibility; \textbf{(b)} The Pearson correlation of the error of the individual models. \textbf{(c)} The Spearman correlation of the error of the individual models.}
    \label{fig:error-analysis}
\end{figure}

Shown in \autoref{fig:error-analysis}, high correlations in terms of both Pearson's $r$ and Spearman's $\rho$ of the errors were observed between all of the individual models ($r>0.5$ and $\rho>0.5$ for all model pairs). This suggests that the models are making similar mistakes, and an ensemble of these models may not be able to significantly outperform the individual models, especially for algorithms like AdaBoost that relies heavily on the diversity of the base learners to improve performance.

A number of models had especially strong correlation ($r>0.9$). The ridge and lasso regression models had 100\% pearson and spearman correlation, as they are essentially the same model with different penalty terms. At the same time, despite having significant dissimilarity in the underlying algorithms, the XGBoost and MLP model seemed to be making similar mistakes on the examples in the DeepPrime dataset ($r=0.95, R=0.83$).

However, very different results were observed when conducting further analysis on the four PRIDICT datasets (\autoref{appendix:error-analysis-figures}). Very high correlation among MLP, Lasso, Ridge and Random Forest models were observed in PRIDICT Adv and K562 datasets, while only the two tree based models XGBoost and Random Forest were highly correlated in the PRIDICT HEK293T and K562MLH1dn datasets.  

The only consistent observation was the high correlation between the two variants of linear regression models, Lasso and Ridge, always achieving 100\% Spearman and Pearson correlation. This ruled out the necessity of using both models in the ensemble, and the ridge regression was preserved due to its superior stability, as lasso regression's L1 regularization can result in zeros in coefficients and cause abrupt changes in the prediction. All other models were preserved in the ensemble for better generalizability across different datasets.

To further reduce the cost of time and computational resources, the ensembles methods were first tested without the deep learning models to verify the validity of the methods. Algorithms that achieved significantly higher performance than the individual models (base learners) were then trained using the full ensemble and on the entire benchmarking dataset.

\subsection{Ensemble Using Weighted Averaging}

The simplest method of ensembling in a regression task is to average the predictions of the individual models, and the averaging can be weighted to give more importance to the more accurate models. A number of methods were common in the literature for determining the weights of the models, including weighting based on model's performance on the training/validation set\cite{fathiImprovingPrecipitationForecasts2019}, using a grid search to find the optimal weights \cite{anandWeightedAverageEnsemble2023}, or direct optimization of the weights using a MLP. Due to the similarity of the grid search and direct optimization models, only the direct optimization and performance weighted ensembles were tested in this study. For each fold, the ensemble learner takes the the prediction of the individual models on the training set as input and the measured efficiency as target. 

For direct optimization, the ensemble learner was implemented as a one layer MLP with bias set to 0 and was trained with an additional validation split of the training set to reduce overfitting. At the same time, performance based weighting was implemented by recording the normalized Spearman's correlation of the individual models on the training in a numpy array, and using the normalized correlation as the weight of the model.

Additionally, to provide context to the MLP ensemble learner, direct optimization was also tested with extended input data. In addition to the predictions of the individual models, the corresponding 24 extracted features for each example were also inputted into the MLP. 

\begin{figure}
    \subfigure[Pearson]{
        \centering
        \includegraphics[width=0.45\textwidth]{ensemble_pearson.pdf}
        \label{fig:ensemble-weighted-mean-pearson}
    }
    \subfigure[Spearman]{
        \centering
        \includegraphics[width=0.45\textwidth]{ensemble_spearman.pdf}
        \label{fig:ensemble-weighted-mean-spearman}
    }
    \caption[Ensemble Model Performance]{Pearson \textbf{(a)} and Spearman \textbf{(b)} performance of the ensemble models using weighted averaging with different input data on the PRIDICT HEK293T dataset. The base learners used the uniform color of gray, while each ensemble model is represented by a different color. Three ensemble techniques were tested: direct optimization using only the predictions from base models (opt), direct optimization supplemented by the extracted features (opt-f), as well as performance based weighting (pwm, pearson weighted mean). The performance of the individual models were tested on all five folds of the PRIDICT HEK293T dataset, and the mean performance was plotted as the bar plot, while the individual runs' performances were shown as strip plots. Additionally, a dotted line of matching color was drawn at each ensemble model's mean performance for easier comparison. }
    \label{fig:ensemble-weighted-mean}
\end{figure}

This resulted in three different ensemble models: direct optimization with only the prediction of the individual models, direct optimization with the addition of 24 extracted features, and weighted averaging using only performance (Spearman's $\rho$) on the training set. All methods were tested on the PRIDICT HEK293T dataset, and their performances compared to the individual base leaner are shown in \autoref{fig:ensemble-weighted-mean}.

Interestingly, the simplest performance based weighting method achieved the best performance in terms of both Pearson and Spearman correlation, while the direct optimization methods could not outperform the individual models on the test set, possibly due to overfitting on the training set (despite the additional validation split). The performance based weighting method was on par with the best individual models XGBoost and random forest in terms of Pearson's $r$, while significantly outperforming both of them when evaluated using Spearman's correlation ($p<0.05$, paired t-test). 

As a result, the performance weighted mean was selected as one of the ensemble methods for further testing using the full ensemble of models.

\subsection{Ensemble with Bagging}

Bootstrap aggregating, or bagging, is a method that trains multiple models on different subsets of the training data and averages the predictions to produce the final prediction. Bagging works on the expectation that the models trained on different subsets of the data will have different biases and errors, and the averaging of the predictions will reduce the variance of the final prediction\cite{dongSurveyEnsembleLearning2020}.

Although sklearn provides implementation of bagging, their method does not work with regressors of different types. The deep learning models and the conventional ml models also use different input data, further exacerbating the difficulty of using preestablished libraries. Thus, a custom implementation of Bagging was written to handle the specific requirements of this task.

The bagging model has two important parameters, the number of rounds of training to perform and the percentage of the training test to sample for each round. In each round, all base learners are trained on a different subset of the training data, thus the total number of base learners is the number of rounds multiplied by the number of base leaners. To save time, the optimal number of rounds and sample percentage were tuned separately on the PRIDICT HEK293T dataset instead of being tuned with a grid search. 

\begin{figure}
    \centering
    \subfigure[Percentage Tuning Pearson]{
        \includegraphics[width=0.45\textwidth]{ensemble_pearson_bagging_percentage.pdf}
        \label{fig:ensemble-bagging-pearson-percentage}
    }%
    \subfigure[Percentage Tuning Spearman]{
        \includegraphics[width=0.45\textwidth]{ensemble_spearman_bagging_percentage.pdf}
        \label{fig:ensemble-bagging-spearman-percentage}
    }
    \subfigure[Round Tuning Pearson]{
        \includegraphics[width=0.45\textwidth]{ensemble_bagging_pearson_round.png}
        \label{fig:ensemble-bagging-pearson-round}
    }%
    \subfigure[Round Tuning Spearman]{
        \includegraphics[width=0.45\textwidth]{ensemble_bagging_spearman_round.png}
        \label{fig:ensemble-bagging-spearman-round}
    }
    \caption[Ensemble Model Performance]{Pearson \textbf{(a, c)} and Spearman \textbf{(b, d)} performance of the ensemble models using bagging with different hyperparameters on one fold of the PRIDICT HEK293T dataset. The percentage of samples varied from 0.3 to 0.9 with an increment of 0.2 (bag-0.3 to bag-0.9), while the number of rounds varied from 1 to 15 with increasing increment (bag-1 to bag-15). Similar to \autoref{fig:ensemble-weighted-mean}, the performance of the individual models were tested for five separate runs, with the mean and individual runs' performance plotted as bar and strip plots, respectively.}
    \label{fig:ensemble-bagging-tuning}
\end{figure}

Starting with the percentage of the sample, by fixing the number of rounds to 3, the percentage of the sample was varied from 0.3 to 0.9 with an increment of 0.2. Shown in Figure \ref{fig:ensemble-bagging-pearson-percentage} and Figure \ref{fig:ensemble-bagging-spearman-percentage}, the performance of the ensemble increases with the percentage of the sample and plateaued at 0.7. 

Meanwhile, fixing the sample percentage at 0.5, the number of rounds was increased from 1 to 15 with a varying increment mimicking the exponential increase. Similar to the effect of increasing the percentage of the sample, the performance of the ensemble increased with the number of rounds and plateaued at 3. Further increase in the number of rounds did not result in significant improvement in the performance of the ensemble.

Thus, the optimal hyperparameters for the bagging model were determined to be 3 rounds of training with 70\% of the training data sampled for each round. And since bagging also achieved significantly better performance than all of the base learners in terms of Spearman correlation ($p<0.05$, paired t-test), it was added to the list of ensemble methods for further testing.

\subsection{Ensemble with AdaBoost}
\label{sec:ensemble-adaboost}

Unlike weighted averaging and bagging, AdaBoost is a boosting method that trains multiple models sequentially. It trains one base learner on the training data, calculates the error of the model, and then trains another base learner on the same data with the error of the previous model being used to adjust the weight of the samples. This process is repeated until the desired number of regressors are fitted, and the final prediction is the weighted sum of the predictions of the individual models. The idea is similar to the weighted loss adjustment used by DeepPrime when training on their dataset (\autoref{sec:data-engineering}), but applicable to all dataset without the need to carefully tweaking the weight adjustment functions for each dataset.

Having faced similar problem with bagging, AdaBoost was also implemented from scratch. The implementation in this study was based on the the AdaBoost.RT algorithm, which is a variant of AdaBoost specifically designed for regression tasks \cite{shresthaExperimentsAdaBoostRT2006,solomatineAdaBoostRTBoosting2004}. However, the original algorithm had to be tweaked in a number of places to accommodate the different types of models used in this study. 

Starting with a set of input data $X=\{x_1, x_2, ..., x_n\}$ and the corresponding target values $Y=\{y_1, y_2, ..., y_n\}$
, the ensemble method has two hyperparameters to initialize: An integer specifying the number of rounds $T$, defined similarly to the bagging model, as well as a threshold $0<\epsilon < 1$ to determine if an individual example is correctly predicted by the model.
The error rate threshold has a very significant impact on the performance of the model, and needs to be selected carefully. A threshold too low would result in insufficient amount of correctly predicted examples, while a threshold too high would result in overfitting to the outliers with very high relative error, neither of which are desirable\cite{shresthaExperimentsAdaBoostRT2006}. 

Each iteration $t\leq T$ trains a base learner using a set of sample weights $D_t(i)$, and calculates the associated model weight $\alpha_t$ and error rate $\epsilon_t$ (percentage of mispredicted examples).
In the original model, the sample weight distribution is initialized as a uniform distribution summing up to one: $D_1(i) = 1/n$. However, in a dataset with large sample size, the uniform distribution could result in a very small weight for each sample, which does not work well with the XGBoost model and would result in NaN values in its prediction. Thus, the distribution was modified to be a scaled uniform distribution summing up to the number of samples in the dataset: $D_1(i) = 1$. Additionally, AdaBoost.RT has no separate definition of model weight, as the models are weighted using the log inverse of error rate ($log(1/\eta_t)$). However, since Spearman's correlation is the desirable metric in this task, the model weight was set to be the Spearman's $\rho$ of the model's prediction on the training set, while error rate was used in adjusting the sample weights.

After each iteration, the absolute relative error for each training example is calculated as $e_t(i) = |y_i - \hat{y}_t(i)/y_i|$, where $\hat{y}_t(i)$ is the prediction of the model at iteration $t$. The error is then used to calculate the error rate, indicating the percentage of examples predicted with higher relative error than the threshold $\phi$: $\epsilon_t = \sum_{i=1: e_t(i) > \phi} D_t(i) / n$. This projects the regression problem into a classification problem that the AdaBoost algorithm was originally designed for.

The sample weights are then updated using the calculated error rate's power and relative error associated with each sample as 
$$
\beta_t = \epsilon_t ^ n
$$
$$
D_{t+1}(i) = D_t(i) \times \begin{cases}
    \beta_t, & \text{if } e_t(i) < \phi \\
    1, & \text{otherwise}
\end{cases}
$$
Where n is another tunable hyperparameter that can be 1 (linear), 2 (quadratic), 3 (cubic) or any other positive integer. Higher value of n would result in the model focusing more on the misclassified examples, which can help the model better fit the data, but would also increase the risk of overfitting.

To prevent the sample weights from becoming too small for the XGBoost model during training, the sample weights are clipped to a minimum of 1e-6. After clipping, the distribution is then normalized to sum up to 1, ensuring that the sample weights are still a probability distribution. Then, the distribution is scaled by a factor of $n$ for compatibility with the XGBoost model.

The final output is then calculated as the weighted sum of the predictions of the individual models using the normalized model weights $\alpha$, similar to the performance weighted mean and bagging models.

The tuning process for AdaBoost is very time consuming, as three hyperparameters need to be optimized: the error rate threshold $\phi$, the power $n$ and the number of rounds $T$.
Ideally, the tuning process should be repeated for each dataset. However, considering the time and resource limit of the project, the configuration tuned for the PRIDICT HEK293T PE2 dataset was used for all datasets. 

\begin{figure}
    \centering
    \subfigure[$n = 1$]{
        \centering
        \includegraphics[width=0.9\textwidth]{ensemble-adaboost-threshold-power-fixed-at-1.png}
        \label{fig:ensemble-adaboost-threshold-power-1}
    }
    \subfigure[$n = 2$]{
        \centering
        \includegraphics[width=0.9\textwidth]{ensemble-adaboost-threshold-power-fixed-at-2.png}
        \label{fig:ensemble-adaboost-threshold-power-2}
    }
    \subfigure[$n = 3$]{
        \centering
        \includegraphics[width=0.9\textwidth]{ensemble-adaboost-threshold-power-fixed-at-3.png}
        \label{fig:ensemble-adaboost-threshold-power-3}
    }
    \caption[AdaBoost Tuning]{
        Pearson (\textbf{left}) and spearman (\textbf{right}) performance of the AdaBoost ensemble model on one fold of the PRIDICT HEK293T dataset with different error rate threshold $\phi$, power $n$ and number of rounds. 3 repeated runs were conducted on the same fold for each configuration. The mean performance was shown in the bar plot, while the performance of each individual run was shown in the strip plot. The dotted line represents the highest pearson/spearman correlation among all configurations for easier comparison across different power values. 
    }
    \label{fig:ensemble-adaboost-tuning}
\end{figure}

The result of the tuning process is shown in \autoref{fig:ensemble-adaboost-tuning}. As expected, the performance of the AdaBoost model was very sensitive to the error rate threshold. However, the power of the error rate and the number of base estimators (number of rounds) did not have a significant impact on the performance of the model. Boosting for multiple rounds did slightly improved the performance when the threshold is low (0.05), but did not help when the threshold is set at higher values. As for power, the overall performance was slightly improved with higher value of $n$, but the difference was not significant.

As the threshold increased, the performance plummeted, as the model focused more on the misclassified examples and overfitted to the training data. Yet, this effect was only observed when the model had more than 1 round of boosting. When the number of rounds was set to 1, the model's performance actually slightly increased with higher threshold. 

There were multiple configurations with very similar performance, but the best pearson's $r$ was achieved with a threshold of 0.7 and a power of 3 after one round of boosting, while the best Spearman's $\rho$ was recorded with a threshold of 0.5 and a power of 3, also after one round of boosting. Following the principle that the Spearman's correlation is a closer representation of the objective of this task, the configuration with a threshold of 0.5 and a power of 3 was selected as the final AdaBoost model for further testing on the DeepPrime dataset.

\begin{figure}
    \centering
    \subfigure[Pearson]{
        \centering
        \includegraphics[width=0.45\textwidth]{ensemble_adaboost_comparison_Pearson.png}
        \label{fig:ensemble-adaboost-pearson}
    }%
    \subfigure[Spearman]{
        \centering
        \includegraphics[width=0.45\textwidth]{ensemble_adaboost_comparison_Spearman.png}
        \label{fig:ensemble-adaboost-spearman}
    }%
    \caption[AdaBoost Ensemble Model Performance]{Pearson \textbf{(a)} and Spearman \textbf{(b)} performance of the AdaBoost ensemble model alongside bagging, power weighted mean and base learners. The horizontal line represents the mean performance of the AdaBoost model across all five folds of the PRIDICT HEK293T dataset.}
    \label{fig:ensemble-adaboost}
\end{figure}

\autoref{fig:ensemble-adaboost} compares the performance of AdaBoost using the optimized parameters with the previously developed ensemble models - bagging and performance weighted mean, as well as the base learners. AdaBoost matched the performance of the bagging ensemble, with slightly higher Pearson's correlation and similar Spearman's correlation compared to performance weighted mean. As a result, it was selected as the final ensemble model for further testing on the full benchmarking dataset.

\section{Adoption of Transformer Model}

The transformer architecture was a ground breaking innovation in the field of NLP, relying solely on attention mechanism to create a representation of the input that captures the long range dependencies in the sequences\cite{vaswaniAttentionAllYou2017}. This ability makes it a suitable candidate for processing the wide target sequences, as the objective of RNN and CNN used by PRIDICT and DeepPrime was to capture the influence of the surrounding nucleotides on the editing efficiency.

It was adopted by the Schwank lab (designers of PRIDICT) in the task of base editing efficiency prediction (BE-DICT), and achieved superior performance in multiple datasets when compared to its RNN and CNN counterparts\cite{marquartPredictingBaseEditing2021}. In their task, the transformer model was used to predict the bystander editing efficiency of the base editors given a target loci. The bystander editing refers to the phenomenon where the base editor induces unintended edits in the protospacer sequence by modifying other targetable nucleotides in the vicinity of the activity window. A number of possible edits result and its corresponding efficiency were inputted into the model during training, and the model can accurately output the efficiency of all possible outcome during inference, even at unseen target loci. Although having slightly lower on-target performance when compared to other models focusing on predicting the intended edits, the transformer model was able to predict the bystander editing efficiency with far superior accuracy.

The success of BE-DICT in base editing suggests that the transformer model may be capable of capturing the complex relationship between the wild type and edited sequences. This prompted me to utilize this architecture in the task of prime editing efficiency prediction, which involves a similar targeting mechanism as base editing.

\subsection{Model Architecture}

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{transformer-architecture.png}
    \caption[Transformer Model Architecture]{Transformer prime editing efficiency prediction model architecture. The sequence data is one hot encoded and fed into the transformer model, whose cross attention layer is used to capture the relationship between the wild type and edited sequences and outputs an encoding of both sequences. The encoding is then fed into the MLP header alongside the encoded biological features to produce the final prediction.}
    \label{fig:transformer-model}
\end{figure}

Shown in \autoref{fig:transformer-model}, the transformer architecture was adopted from Sasha Rush's implementation of the transformer model in Pytorch\cite{AnnotatedTransformer}, while the MLP encoder and decoder was developed to match the design of DeepPrime for focusing the comparison on the sequence encoding portion of the model\cite{yuPredictionEfficienciesDiverse2023}.

The transformer model is used as an encoder for the wild type and mutated sequences. To begin with, the input sequences are embedded with one-hot encoding, with the padding values set to a vector of zeros.
The embedded sequences are then added with the \verb|sine| and \verb|cosine| waves for even and odd dimensions to assign positional information to the input sequences, as attention mechanism does not have any inherent understanding of the order of the input data. 
To help the transformer model better perceive the relationship between wild type and edited sequences, the two sequences are aligned so that sections of the sequence not involved in the edit has matching positional encoding. The missing regions in the edited sequence for deletion and in the wild type sequence for insertion are filled with padding values, and the positional encodings are set to 0 for these regions.

The wild type sequence is then inputted into the transformer encoder, composed of a self-attention layer and a positional feed forward layer (MLP layers processing the input in the embedding dimension instead of sequence length dimension). The self-attention layer is used to capture the relationship between the nucleotides within the sequence, while the positional feed forward layer further transforms the representation and introduces non-linearity with the activation function. 

At the same time, the mutated sequence is inputted into the transformer decoder, starting with the same self attention layer and positional feed forward layer. The output of the encoder and feedforward layer are then joined by the cross attention layer, using the output of the encoder as the query and the output of the MLPs as the key and value. After the cross attention layer, the output is processed by another positional feed forward layer, which completes one layer of the decoder. 

The encoder and decoder could be stacked multiple times for the attention mechanism to capture higher level features of the input data, and residual connections as well as layer normalization are used after each multi-head attention and positional feed forward layer to stabilize the training process.

The final output of the transformer model is then pooled by a feature embedding attention layer, which assigns different weights to the output of the transformer model based on the importance of the each sequence position, and produces a flattened representation with length equal to the number of embedding dimension.

At the same time, the extracted biological features are processed by a separate MLP (MLP encoder), which produces a dense representation of the input and is concatenated with the output of the transformer model. 

Finally, the concatenated representation is fed into the MLP header (MLP decoder), which produces a real value as the predicted editing efficiency of the input sequences given the pegRNA.

\subsection{Architecture and Hyperparameter Tuning}

A number of architectural and hyperparameter choices were tested during the development of the transformer model. As mentioned in \autoref{sec:ensemble-adaboost}, the hyperparameter tuning of the model should ideally be repeated for each dataset. Limited by available resources, however, tuning was only performed on one fold of the PRIDICT HEK293T dataset, similar to the ensemble models.

% Similar to PRIDICT, the size of the MLP layers were controlled using three parameters. The first is the mlp\_embed\_factor, which controls the size of the hidden layers in the MLP encoder and decoder. This is followed by the embed dimension of the MLP, which is the size of the output of the MLP embedder processing the feature vector. The last parameter is the number of encoder units inside of the MLP, where a encoder unit is a combination of two linear layers converting the data from embed dimension to embed dimension * mlp\_embed\_factor, followed by dropout and layer normalization.

To avoid the computational cost of testing many parameters at the same times and forming a large parameter grid, the tuning was conducted in two separate stages, with the first stage of tuning focusing on the a number of design decisions of the model architecture. 

To begin with, a number of alternative attention mechanisms were tested, using the scalar dot product attention as a baseline. In order to alleviate the quadratic computational cost of the scalar dot product attention model, flash attention was included as a more efficient alternative to the self-attention mechanism. Half precision was required by the flash attention to train and test the model, which can significantly reduce memory usage and training time, achieving up to 8 times speed up when compared to single precision (float32)\cite{micikeviciusMixedPrecisionTraining2018}. However, certain small-magnitude gradients may fall out of range of half precision and become zero. This results in NaN values in the model's weights and losses, causing training to fail. As a result, the hugging face accelerator module was adopted to prevent the training from failing\cite{wolfHuggingFaceTransformersStateoftheart2020}. 

Additionally, global attention is not always necessary for lower level of the transformer model\cite{raeTransformersNeedDeep2020}, thus another 
possible improvement to the model is to use local attention for self-attention layers. The local attention only allows the model to attend to a fixed number of tokens within a certain window, reducing the computational cost of the attention mechanism and maybe improving the model's performance by focusing on the most relevant tokens.

However, during testing, neither flash attention nor local attention showed any significant improvement in the model's performance.

Even with the hugging face's accelerator module, the mixed precision training still causes around 1 out of 20 training runs to fail due to NaN values in the model's weights and losses, especially in model configurations with poor performances. Additionally, no significant improvement in neither training time nor performance was observed when using flash attention when training on the network with 1 encoder and decoder layer using 2 attention heads (one head works on CG representation, the other works on AT representation, performance comparison with pearson's $r$ of 0.79 vs 0.79 across 5 repeated runs, $p>0.1$, paired t-test) compared to scalar product attention. On the same configuration of 1 encoder and 1 decoder, the local attention mechanism also did not show any significant improvement in the model's performance (pearson's $r$ of 0.78 vs 0.79 for with and without local attention respectively, $p>0.1$, paired t-test).

Additionally, although having previously established that the usage of embedding does not significantly improve the DeepPrime and PRIDICT models' performance (\autoref{sec:training-deepprime-pridict}), the influence of using PRIDICT's functional annotation still needs to be investigated. Using the same configuration of 1 encoder and decoder with scalar dot product attention, marginal improvement was observed in the model's performance (3 heads used for annotated representation, with 2 heads processing ATGC representation as before, and 1 extra head processing the PBS and RTT representation, pearson's $r$ of 0.79 vs 0.81 for without and with functional annotation, $p>0.05$, paired t-test). This indicates that the functional annotation may provide important contextual information for the transformer model to differential between different edits on the same target loci. Although the improvement was not statistically significant, consistent enhancement could be observed over each repeated run, and thus the functional annotation was included in the final model.


\begin{figure}
    \subfigure[1 e/d unit]{
        \includegraphics[width=0.42\textwidth]{transformer-tune-pdropout-num_encoder_units-fixed-at-1-pearson.png}
        \label{fig:transformer-tune-pdropout-num_encoder_units-fixed-at-1-pearson}
    }%
    \subfigure[3 e/d units]{
        \includegraphics[width=0.27\textwidth]{transformer-tune-pdropout-num_encoder_units-fixed-at-3-pearson.png}
        \label{fig:transformer-tune-pdropout-num_encoder_units-fixed-at-3-pearson}
    }%
    \subfigure[5 e/d units]{
        \includegraphics[width=0.27\textwidth]{transformer-tune-pdropout-num_encoder_units-fixed-at-5-pearson.png}
        \label{fig:transformer-tune-pdropout-num_encoder_units-fixed-at-5-pearson}
    }
    \subfigure[1 e/d units]{
        \includegraphics[width=0.42\textwidth]{transformer-tune-pdropout-num_encoder_units-fixed-at-1-spearman.png}
        \label{fig:transformer-tune-pdropout-num_encoder_units-fixed-at-1-spearman}
    }%
    \subfigure[3 e/d units]{
        \includegraphics[width=0.27\textwidth]{transformer-tune-pdropout-num_encoder_units-fixed-at-3-spearman.png}
        \label{fig:transformer-tune-pdropout-num_encoder_units-fixed-at-3-spearman}
    }%
    \subfigure[5 e/d units]{
        \includegraphics[width=0.27\textwidth]{transformer-tune-pdropout-num_encoder_units-fixed-at-5-spearman.png}
        \label{fig:transformer-tune-pdropout-num_encoder_units-fixed-at-5-spearman}
    }
    \caption[Transformer Model Hyperparameter Tuning]{Pearson \textbf{(a-c)} and spearman \textbf{(d-f)} performance of the transformer model with different dropout rates (pdropout), positional feedforward embedding size (mlp\_embed\_dim) and number of encoder/decoder units (e/d unit) on all five folds of the PRIDICT2.0 HEK293T dataset. The highest mean pearson and spearman value among all configuration was shown as a horizontal red dotted line for easier comparison.}
    \label{fig:transformer-tune}
\end{figure}

The next stage focuses on the parameter size of the model. Since the MLP modules were already optimized by Yu at al in their DeepPrime study, the size of the MLP encoding and decoding layers were kept constant. The size of the transformer is mostly controlled by two parameters: the number of encoder and decoder layers, and the mlp embedding size of the positional feedforward layers. Additionally, the dropout rate was also optimized at this stage as it determines the amount of information that is retained during training. A grid search was conducted on the dataset, using the following configuration for each parameter:

\begin{itemize}[itemsep=-0mm]
    \item \textcolor{blue}{Number of Encoder/Decoder Units}: 1, 2, 3
    \item \textcolor{blue}{Positional Feedforward Hidden Unit Size}: 50, 100, 150
    \item \textcolor{blue}{Dropout Rate}: 0.05, 0.1, 0.2, 0.3, 0.5
\end{itemize}

Each configuration was repeated for 3 runs to get a better representation of the model's performance, with the result shown in \autoref{fig:transformer-tune}. 
The effects of dropout rate varied with the number of encoder and decoder units as well as the positional feedforward embedding size. With 100 hidden units, performance consistently increased with dropout rate util 0.2 or 0.3, and then started to plummet. With 50 and 150 units, similar trend could be seen when 1 or 3 encoder/decoder units were used, but with an initial drop in performance when the dropout rate was increased from 0.05 to 0.1. However, when a stack of 5 encoder/decoder units were used, the 150 hidden unit configurations still follows the first increase then decrease trend, while the 50 hidden unit configuration showed a steady decrease in both pearson and spearman correlation as the dropout rate increased.

Overall, the configuration with dropout rate of 0.2, 1 encoder and decoder unit, and positional feedforward hidden unit size of 100 achieved the highest performance in terms of both Pearson and Spearman correlation, and the 3 encoder and decoder units configuration with 0.3 dropout rate trails closely behind. The performance difference was not significant between the best and the second best configurations (pearson's $r$ of 0.82 vs 0.82, spearman's rank correlation of 0.84 vs 0.84, $p>0.1$, paired t-test). However, the 1 encoder and decoder unit configuration consumes significantly less computational resources and was noticeably faster to train. Taking the practical considerations into account, the 1 encoder and decoder unit configuration was selected as the final model.

To sum up, the best performing transformer model used scalar dot product attention mechanism with functional annotation from PRIDICT, and was configured with 1 encoder and decoder unit, positional feedforward hidden unit size of 100, 3 attention heads, and a dropout rate of 0.2. 

\section{Development of pegRNA Design Web Tool}

To facilitate the usage of models developed in this study, a web tool was created for users to input their target edit and receive the best pegRNA design as per the model's prediction. The web application was implemented using the Django framework for easier interaction with Python scripts. 

To begin with, an algorithm for suggesting candidate pegRNAs given a desirable edit was developed. As discussed in \autoref{sec:prime-editing-process}, the pegRNA is composed of three components: the spacer, PBS and RTT, and the RTT is further divided into LHA and RHA. The protospacer corresponding to the spacer sequence can be easily located by finding the required PAM sequences in appropriate locations.
Supposing the nick is done at 3bp upstream of the PAM, for the edit to be possible, the PAM sequence must be at least 3bp downstream of the edit start location (in which case LHA length is 0). Locating the protospacer also fixes the LHA length, as it always starts at the nick site 3bp upstream of the PAM and ends at the edit location. PBS and RHA, on the other hand, are much more flexible, as they can be freely extended to their 3' and 5' ends respectively.

However, as discussed in \autoref{sec:determinants}, the length of LHA, RHA and PBS all have significant impact on the editing efficiency. To come up with a recommended range for these parameters, a more detailed analysis of the relationship between the lengths and the editing efficiency was conducted.

\begin{figure}
    \includegraphics[width=\textwidth]{lha-rha-pbs-length-requirement.png}
    \caption[Relationship between LHA, RHA and PBS length and editing efficiency]{Relationship between PBS \textbf{(top)}, RHA \textbf{(middle)} and LHA \textbf{(bottom)} length and editing efficiency. Data from all of the DeepPrime and PRIDICT datasets were used in the analysis for better generalizability. Random samples of 5\% of the examples' efficiencies were plotted as a strip plot on top of the box plot to show individual data point while keeping the figure from cluttering. Y axis is limited to a max value of 5 + 1.5 * IQR to further improve readability. }
    % The values associated with rha length over 30bp were omitted due to the small number of data points and very low efficiency.
    \label{fig:lha-rha-pbs-length}
\end{figure}


Shown in \autoref{fig:lha-rha-pbs-length}, the relationship between the lengths of LHA, RHA and PBS and the editing efficiency was analyzed using the entire benchmarking dataset. Consistent with Yu et al, 2023's finding, editing editing efficiency plateaued with RHA length of 7-12bp, and was steadily high with a number of peaks until 20bp long, after which the efficiency started to decrease as the RHA grows. However, to limit the number of pegRNAs generated by the algorithm, the recommended range for RHA length was set to 7-12bp, which covers the majority of the high efficiency region.
 As for LHA, the efficiency was shown to decrease with longer LHA length, with an elbow point at around 13bp, where the decrease in performance with regard to the lha length speeded up. Thus, the recommended distance between the nick and the edit location is set to 0-13bp, and is only extended if no suitable PAM is found within the range. Last but not least, matching the observation in \autoref{sec:determinants}, PBS length's impact on the editing efficiency was roughly normally distributed, with the highest efficiency at around 12bp. Decent performance could observed from 8bp all the way up to the highest recorded value of 16bp, which covers the entire range of the protospacer upstream of the nick site. Thus, although the SHAP analysis determined that long PBS can be detrimental to the editing efficiency, the range of PBS length was still set to 8-16bp.

\begin{table}[ht]
    \centering
    \begin{tabular}{c|c|c}
        % \hline
        \textbf{Component} & \textbf{Min Length} & \textbf{Max Length} \\
        \hline
        LHA & 0 & 13 \\
        RHA & 7 & 12 \\
        PBS & 8 & 16 \\
        % \hline
    \end{tabular}
    \caption{Recommended range for LHA, RHA and PBS length}
    \label{tab:recommended-range}
\end{table}

In summary, unless specified otherwise by the users, the algorithm would only propose pegRNAs with component length in the range specified in \autoref{tab:recommended-range}. The recommending range would result in a maximum of 756 pegRNAs (true number is usually much smaller as LHA location is heavily constrained), which would then be processed and evaluated by the model to produce the estimated editing efficiency.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{webapp-interface.png}
    \caption[pegRNA Design Web Tool Interface]{Interface of the pegRNA Design Web Tool. A dropdown menu is provided for the user to select the PE version and cell line, and the user can input their target edit in the PRIDICT format in the input box.}
    \label{fig:webtool}
\end{figure}



\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{webapp-table.png}
    \caption[pegRNA Design Web Tool Output]{Output of the pegRNA Design Web Tool. The output contains the position of the protospacer, RTT and PBS, as well as the predicted editing efficiency. For each pegRNA, the user can click on the `Visualize Sequence' button to see the composition of the pegRNA with regard to the wild type sequence.}
    \label{fig:webtool-output}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{webapp-visualization.png}
    \caption[pegRNA Design Web Tool Visualization]{Visualization of the pegRNA. The composition of the pegRNA with regard to the wild type sequence is shown, with the brackets indicating the position of the protospacer, RTT and PBS. To highlight the edit position, the bases involved in the edit are colored in red.}
    \label{fig:webtool-visualize}
\end{figure}


Plugging in the pegRNA design algorithm, the full application logic can now be defined.
The user can supply their target edit in the PRIDICT format of \{at-lesat-100bp\}-(before-edit/after-edit)-\{at-lesat-100bp\} in the input box, and can also use the example by clicking the `Use Test Example' button. The PE and Cell Line should also be selected from the dropdown menu above the text box before running the prediction, shown in \autoref{fig:webtool}. Any missing input would result in an error message delivered to the user with browser notification, while format error in the input sequence would likely result in prediction failure, which is also communicated to the user.

The user response is then handled by the prediction API endpoint, which parses the input into original and mutated sequence, as well as edit location, type and length. After receiving the parsed input, the pegRNA design algorithm is run to generate the candidate pegRNAs in standard (std) data format
, which is then converted to the suitable formats for the models. Subsequently, the best performing model would be used to predict the editing efficiency of the candidate pegRNAs. Trained models corresponding to all five folds of the matching datasets are used during inference, and the results are averaged to produce the final prediction. 

The result is returned as a JSON response and is parsed by the front end as a table of pegRNA candidates with their corresponding editing efficiency (\autoref{fig:webtool-output}). The output contains the position of the protospacer, RTT and PBS, as well as the predicted editing efficiency. For each pegRNA, the user can click on the `Visualize Sequence' button to see the composition of the pegRNA with regard to the wild type sequence (\autoref{fig:webtool-visualize}).

Note that although the corresponding 20bp protospacer sequence does not necessarily start with a guanine (G), the spacer sequence must begin with a G so that the U6 motor used for the transcription of the pegRNA can recognize the sequence and start the expression\cite{hsieh-fengEfficientExpressionMultiple2020}. 

Limited by the scale and funding of this project, the web application is not deployed to a public server at the current stage. However, it can still be tested locally using Python with Django, Pytorch and Tensorflow installed (all requirements are listed in \verb|requirements.txt|) by executing:

\verb|python webtool/manage.py runserver|

suppose that the user is in the root directory of the project. The web tool can then be accessed by visiting \url{http://127.0.0.1:8000/}. 

When testing on personal devices with limited computational resources, HEK293T - PE2 combination is not recommended, as the model needs to first load the large DeepPrime dataset into memory to make predictions, which could take up to 10 minutes.