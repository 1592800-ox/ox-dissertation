\chapter{Discussion}

The cross attention mechanism is possibly more suitable for the case of base editing efficiency prediction, since the editing window is relatively static with regard to the protospacer location. While for prime editors, even by following the more stringent constraint on LHA length than practical standard, the edit position can vary in a window as wide as 20bp. This resulted in a far bigger possible range of possible outcomes

A number of alternative structures could be experimented with 

The performance of the ensemble can also be improved further by performing hyperparameter tuning on each dataset instead of using the parameters from PRIDICT HEK293T PE2 dataset as representative setting. 