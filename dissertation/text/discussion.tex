\chapter{Discussion}

Overall, the ensemble models have shown promising results in predicting the efficiency of prime editing in a number of configurations of prime editors and cell lines. They were able to achieve on par performance with the current state of the art model, PRIDICT and DeepPrime on most datasets other than the DeepPrime HEK293T PE2 dataset. 

Additionally, as discussed in the benchmarking section, the ensemble models may be able to achieve better performance than the current state of the art models on more datasets if they were tuned to the specific dataset instead of using one set of hyperparameters for all datasets. 

However, the transformer model underperformed in this task, unable to achieve the same level of performance as the ensemble models on the five main datasets. This is possibly due to the inherent differences between the tasks of prime editing and base editing efficiency prediction. 

For base editors, the editing window is relatively static with regard to the protospacer location. At the same time, each base editor can only introduce a single type of base change (C to T or A to G) at a single location in the protospacer. This limits the number of possible outcomes for each base editor to a small number of possibilities, and the relationship between the wild type and mutated sequence is relatively predictable.

Meanwhile, the prime editing efficiency prediction task is much more complex. The prime editor can introduce a variety of changes to the target sequence, and the editing window is also dynamic, as the prime editor can introduce changes at different locations after the nick site of the protospacer. This results in a much larger number of possible outcomes, and the attention mechanism might have a much harder time capturing enough features specific to each edit.

A number of possible improvements could be made to the transformer model to help it establish the connection between wild type and mutated sequences. 

On the data preprocessing level, we had established that the functional annotation denoting the PBS and RTT sequences used by PRIDICT was important for the performance of the transformer model. However, the number of possible edits given the locations of the PBS and RTT sequences is still very large. It may be beneficial to incorporate the actual bases of the PBS and RTT sequences into the wild type sequence to help the model establish the connection between the wild type and mutated sequences. 

On top of that, I avoided modifying the architecture of the transformer model too much as the development of a new architecture would require a significant amount of time and resources beyond the limit of this project. However, a number of alternative architectures can be explored that may improve upon the performance of the transformer model.

For example, the transformer model could be modified to include a convolutional neural network (CNN) layer at the input to help the model capture local features in the sequence. At the same time, a number of different generator could be used to generate the output of the transformer model other than the feature embedding attention layer used in this study. A number of possibilities include another CNN layer, a gated recurrent unit used by DeepPrime, or a simple feedforward neural network. 

Finally, as briefly mentioned in \autoref{sec:determinants}, the current methods all follow the same general approach of training one model for each prime editor and cell line combination. However, recent research has shown that multi-task learning can be beneficial in a number of different tasks, including the prediction of the efficiency of base editors\cite{mollaysaAttentionbasedMultitaskLearning2023a}

Obviously, this would introduce many problems during implementation. The most prominent issue would be the assembling of the dataset. As discussed in \autoref{sec:determinants}, many cellular level factors can affect the efficiency of prime editing. These factors were ignored in this study since they were encapsulated in the cell line and prime editor. However, if a single model were to be trained for all prime editors and cell lines, these factors would have to be included in the dataset. This may require substantial biological knowledge and could involve scrapping multiple sequencing datasets to obtain the necessary information. 
More imbalance in the dataset would also arise, for example, cells with MMR deficiency may become underrepresented and cause the model to perform poorly on these cell lines. 
These problems would require significantly more resources (both in time and data) to solve, and should be an interesting topic for future research.