# concatenate the predictions for each fold into a single array
prediction_pd_list = np.concatenate([prediction_pd[fold].flatten() for fold in range(5)])
prediction_dp_list = np.concatenate([prediction_dp[fold].flatten() for fold in range(5)])
prediction_rf_list = np.concatenate([prediction_rf[fold] for fold in range(5)])
prediction_xgb_list = np.concatenate([prediction_xgb[fold] for fold in range(5)])
prediction_ridge_list = np.concatenate([prediction_ridge[fold] for fold in range(5)])
prediction_lasso_list = np.concatenate([prediction_lasso[fold] for fold in range(5)])
prediction_mlp_list = np.concatenate([prediction_mlp[fold].flatten() for fold in range(5)])

print('Concatenated')

# calculate the squared error from the predictions
y_test = np.concatenate([data[data['fold'] == i].iloc[:, -2].values for i in range(5)])
mse_dp = (y_test - prediction_dp_list)**2
mse_pd = (y_test - prediction_pd_list)**2
mse_rf = (y_test - prediction_rf_list)**2
mse_xgb = (y_test - prediction_xgb_list)**2
mse_ridge = (y_test - prediction_ridge_list)**2
mse_lasso = (y_test - prediction_lasso_list)**2
mse_mlp = (y_test - prediction_mlp_list)**2

# print the mse of each model
print('DeepPrime:', np.sum(mse_dp) / len(mse_dp))
print('Pridict:', np.sum(mse_pd) / len(mse_pd))
print('Random Forest:', np.sum(mse_rf) / len(mse_rf))
print('XGBoost:', np.sum(mse_xgb) / len(mse_xgb))
print('Ridge:', np.sum(mse_ridge) / len(mse_ridge))
print('Lasso:', np.sum(mse_lasso) / len(mse_lasso))
print('MLP:', np.sum(mse_mlp) / len(mse_mlp))

# print the pearson correlation coefficient between model predictions and true values
from scipy.stats import pearsonr
print('DeepPrime:', pearsonr(y_test, prediction_dp_list)[0])
print('Pridict:', pearsonr(y_test, prediction_pd_list)[0])
print('Random Forest:', pearsonr(y_test, prediction_rf_list)[0])
print('XGBoost:', pearsonr(y_test, prediction_xgb_list)[0])
print('Ridge:', pearsonr(y_test, prediction_ridge_list)[0])
print('Lasso:', pearsonr(y_test, prediction_lasso_list)[0])
print('MLP:', pearsonr(y_test, prediction_mlp_list)[0])

# normalize the squared error
mse_dp = mse_dp / np.sum(mse_dp)
mse_pd = mse_pd / np.sum(mse_pd)
mse_rf = mse_rf / np.sum(mse_rf)
mse_xgb = mse_xgb / np.sum(mse_xgb)
mse_ridge = mse_ridge / np.sum(mse_ridge)
mse_lasso = mse_lasso / np.sum(mse_lasso)
mse_mlp = mse_mlp / np.sum(mse_mlp)


# plot the smoothened error using a moving average
import matplotlib.pyplot as plt
import seaborn as sns

def moving_average(x, w):
    return np.convolve(x, np.ones(w), 'valid') / w


fig, axes = plt.subplots(1, 1, figsize=(8, 4.5))
window = 1000
axes.plot(moving_average(mse_dp, window), label='DeepPrime')
axes.plot(moving_average(mse_pd, window), label='Pridict')
axes.plot(moving_average(mse_rf, window), label='Random Forest')
axes.plot(moving_average(mse_xgb, window), label='XGBoost')
axes.plot(moving_average(mse_ridge, window), label='Ridge')
axes.plot(moving_average(mse_lasso, window), label='Lasso')
axes.plot(moving_average(mse_mlp, window), label='MLP')
# legend of two rows
axes.legend(loc='upper center', bbox_to_anchor=(0.5, -0.2), ncol=4)

# set x and y labels
axes.set_xlabel('Data Index')
axes.set_ylabel('Normalized Squared Error')


# reduce thickness and alpha of the lines
for line in plt.gca().lines:
    line.set_linewidth(0.5)
    line.set_alpha(0.5)

# remove the top and right spines
axes.spines['top'].set_visible(False)
axes.spines['right'].set_visible(False)

plt.tight_layout()
plt.savefig(os.path.join('dissertation', 'figures', f'error_comparison_{dataf}.png'), dpi=300)

# calculate the pearson correlation coefficient between each error and plot in heatmap
from scipy.stats import pearsonr, spearmanr

mse_dict = {
    'DeepPrime': mse_dp,
    'Pridict': mse_pd,
    'Random Forest': mse_rf,
    'XGBoost': mse_xgb,
    'Ridge': mse_ridge,
    'Lasso': mse_lasso,
    'MLP': mse_mlp,
}

fig, axes = plt.subplots(1, 1, figsize=(5, 5.5))

correlation_matrix = np.zeros((len(mse_dict), len(mse_dict)))
for i, mse in enumerate(mse_dict.values()):
    for j, mse2 in enumerate(mse_dict.values()):
        correlation_matrix[i, j] = pearsonr(mse, mse2)[0]

# heat map with cbar in range 0 to 1, horizontal and on top of the plot
sns.heatmap(correlation_matrix, ax=axes, cbar_kws={'location': 'top', 'pad': 0.05}, vmin=0, vmax=1, cmap='coolwarm', annot=True)

# set cbar limit
# cbar = axes.collections[0].colorbar
# cbar.set_ticks([0, 0.5, 1])
plt.xticks(np.arange(len(mse_dict)) + 0.5, mse_dict.keys(), rotation=45)
plt.yticks(np.arange(len(mse_dict)) + 0.5, mse_dict.keys(), rotation=0)

plt.tight_layout()
plt.savefig(os.path.join('dissertation', 'figures', 'error_correlation_pearson.png'), dpi=300)

fig, axes = plt.subplots(1, 1, figsize=(5, 5.5))

correlation_matrix = np.zeros((len(mse_dict), len(mse_dict)))
for i, mse in enumerate(mse_dict.values()):
    for j, mse2 in enumerate(mse_dict.values()):
        # spearman correlation
        correlation_matrix[i, j] = spearmanr(mse, mse2)[0]

# heat map with cbar in range 0 to 1, horizontal and on top of the plot
sns.heatmap(correlation_matrix, ax=axes, cbar_kws={'location': 'top', 'pad': 0.05}, vmin=0, vmax=1, cmap='coolwarm', annot=True)

plt.xticks(np.arange(len(mse_dict)) + 0.5, mse_dict.keys(), rotation=45)
plt.yticks(np.arange(len(mse_dict)) + 0.5, mse_dict.keys(), rotation=0)

# save the plots
plt.tight_layout()
plt.savefig(os.path.join('dissertation', 'figures', 'error_correlation_spearman.png'), dpi=300)