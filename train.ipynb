{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 of 5\n",
      "Training Lasso\n",
      "Training Ridge\n",
      "Training XGBoost\n",
      "Training Random Forest\n",
      "Training MLP\n",
      "  epoch    train_loss    valid_loss    cp      lr     dur\n",
      "-------  ------------  ------------  ----  ------  ------\n",
      "      1      \u001b[36m355.0077\u001b[0m      \u001b[32m398.9306\u001b[0m     +  0.0050  0.2238\n",
      "      2      \u001b[36m297.0485\u001b[0m      401.7555        0.0050  0.1005\n",
      "      3      \u001b[36m264.6860\u001b[0m      419.2020        0.0049  0.2115\n",
      "      4      \u001b[36m248.6370\u001b[0m      \u001b[32m375.9338\u001b[0m     +  0.0047  0.0974\n",
      "      5      \u001b[36m244.8991\u001b[0m      \u001b[32m360.5987\u001b[0m     +  0.0045  0.1015\n",
      "      6      \u001b[36m237.1639\u001b[0m      373.6597        0.0043  0.1002\n",
      "      7      \u001b[36m227.6807\u001b[0m      365.5049        0.0040  0.1018\n",
      "      8      \u001b[36m222.9722\u001b[0m      \u001b[32m353.7408\u001b[0m     +  0.0036  0.0999\n",
      "      9      \u001b[36m217.3117\u001b[0m      \u001b[32m350.1201\u001b[0m     +  0.0033  0.0998\n",
      "     10      \u001b[36m211.7745\u001b[0m      \u001b[32m344.9673\u001b[0m     +  0.0029  0.1035\n",
      "     11      \u001b[36m208.1597\u001b[0m      \u001b[32m340.6567\u001b[0m     +  0.0025  0.0981\n",
      "     12      \u001b[36m204.9070\u001b[0m      \u001b[32m338.3592\u001b[0m     +  0.0021  0.1011\n",
      "     13      \u001b[36m202.0720\u001b[0m      \u001b[32m335.6905\u001b[0m     +  0.0017  0.1042\n",
      "     14      \u001b[36m200.0158\u001b[0m      \u001b[32m333.0607\u001b[0m     +  0.0014  0.1010\n",
      "     15      \u001b[36m198.4513\u001b[0m      \u001b[32m331.2626\u001b[0m     +  0.0010  0.1005\n",
      "     16      \u001b[36m197.2302\u001b[0m      \u001b[32m330.1693\u001b[0m     +  0.0007  0.1027\n",
      "     17      \u001b[36m196.3390\u001b[0m      \u001b[32m329.4973\u001b[0m     +  0.0005  0.2081\n",
      "     18      \u001b[36m195.7374\u001b[0m      \u001b[32m329.1053\u001b[0m     +  0.0003  0.0983\n",
      "     19      \u001b[36m195.3702\u001b[0m      \u001b[32m328.9193\u001b[0m     +  0.0001  0.1011\n",
      "     20      \u001b[36m195.1836\u001b[0m      \u001b[32m328.8696\u001b[0m     +  0.0000  0.0997\n",
      "     21      202.6666      333.6032        0.0050  0.1056\n",
      "     22      200.0559      \u001b[32m324.4641\u001b[0m     +  0.0050  0.1047\n",
      "     23      \u001b[36m194.6978\u001b[0m      \u001b[32m322.7162\u001b[0m     +  0.0049  0.1105\n",
      "     24      \u001b[36m193.3861\u001b[0m      \u001b[32m316.9138\u001b[0m     +  0.0047  0.1046\n",
      "     25      \u001b[36m188.7207\u001b[0m      \u001b[32m314.1403\u001b[0m     +  0.0045  0.1104\n",
      "     26      \u001b[36m187.9428\u001b[0m      \u001b[32m310.9533\u001b[0m     +  0.0043  0.1008\n",
      "     27      \u001b[36m184.0332\u001b[0m      \u001b[32m308.0953\u001b[0m     +  0.0040  0.1041\n",
      "     28      \u001b[36m183.2747\u001b[0m      \u001b[32m305.4619\u001b[0m     +  0.0036  0.1021\n",
      "     29      \u001b[36m180.2381\u001b[0m      \u001b[32m302.7385\u001b[0m     +  0.0033  0.1008\n",
      "     30      \u001b[36m179.1719\u001b[0m      \u001b[32m300.6378\u001b[0m     +  0.0029  0.1124\n",
      "     31      \u001b[36m177.1331\u001b[0m      \u001b[32m298.8681\u001b[0m     +  0.0025  0.1105\n",
      "     32      \u001b[36m175.7792\u001b[0m      \u001b[32m297.0882\u001b[0m     +  0.0021  0.1058\n",
      "     33      \u001b[36m174.4386\u001b[0m      \u001b[32m295.9849\u001b[0m     +  0.0017  0.2136\n",
      "     34      \u001b[36m173.2224\u001b[0m      \u001b[32m295.0045\u001b[0m     +  0.0014  0.1091\n",
      "     35      \u001b[36m172.2617\u001b[0m      \u001b[32m294.2126\u001b[0m     +  0.0010  0.1048\n",
      "     36      \u001b[36m171.4694\u001b[0m      \u001b[32m293.6754\u001b[0m     +  0.0007  0.0993\n",
      "     37      \u001b[36m170.8402\u001b[0m      \u001b[32m293.3508\u001b[0m     +  0.0005  0.0986\n",
      "     38      \u001b[36m170.3837\u001b[0m      \u001b[32m293.1604\u001b[0m     +  0.0003  0.1010\n",
      "     39      \u001b[36m170.0853\u001b[0m      \u001b[32m293.0732\u001b[0m     +  0.0001  0.1036\n",
      "     40      \u001b[36m169.9207\u001b[0m      \u001b[32m293.0510\u001b[0m     +  0.0000  0.1022\n",
      "     41      178.0007      299.7987        0.0050  0.1024\n",
      "     42      186.6074      294.0952        0.0050  0.1001\n",
      "     43      173.1088      \u001b[32m290.0414\u001b[0m     +  0.0049  0.1003\n",
      "     44      179.4210      291.1469        0.0047  0.1017\n",
      "     45      170.9654      \u001b[32m282.1521\u001b[0m     +  0.0045  0.1094\n",
      "     46      174.8609      288.3333        0.0043  0.1012\n",
      "     47      \u001b[36m168.2814\u001b[0m      \u001b[32m280.8946\u001b[0m     +  0.0040  0.0994\n",
      "     48      171.1771      281.2101        0.0036  0.1000\n",
      "     49      \u001b[36m164.8196\u001b[0m      \u001b[32m279.3897\u001b[0m     +  0.0033  0.2130\n",
      "     50      168.2466      \u001b[32m277.9737\u001b[0m     +  0.0029  0.0998\n",
      "     51      \u001b[36m161.7698\u001b[0m      \u001b[32m276.7666\u001b[0m     +  0.0025  0.1048\n",
      "     52      165.2028      277.9732        0.0021  0.1015\n",
      "     53      \u001b[36m159.8733\u001b[0m      \u001b[32m275.7941\u001b[0m     +  0.0017  0.1023\n",
      "     54      161.0585      276.4290        0.0014  0.1064\n",
      "     55      \u001b[36m159.5650\u001b[0m      275.9201        0.0010  0.1036\n",
      "     56      \u001b[36m158.4793\u001b[0m      \u001b[32m275.5019\u001b[0m     +  0.0007  0.1008\n",
      "     57      \u001b[36m158.1380\u001b[0m      275.6515        0.0005  0.1019\n",
      "     58      \u001b[36m157.7736\u001b[0m      275.8075        0.0003  0.1077\n",
      "     59      \u001b[36m157.4758\u001b[0m      275.8602        0.0001  0.0996\n",
      "     60      \u001b[36m157.3045\u001b[0m      275.8701        0.0000  0.0997\n",
      "     61      166.1833      280.7166        0.0050  0.1018\n",
      "     62      184.7365      284.0417        0.0050  0.0972\n",
      "     63      169.9829      \u001b[32m273.4903\u001b[0m     +  0.0049  0.1014\n",
      "     64      166.6232      277.5000        0.0047  0.2116\n",
      "     65      164.0679      \u001b[32m269.6465\u001b[0m     +  0.0045  0.1035\n",
      "     66      163.6273      272.7228        0.0043  0.1014\n",
      "     67      162.1444      270.0412        0.0040  0.1007\n",
      "     68      160.0755      \u001b[32m267.8865\u001b[0m     +  0.0036  0.0989\n",
      "     69      159.0105      268.8288        0.0033  0.1065\n",
      "     70      157.9086      \u001b[32m267.3560\u001b[0m     +  0.0029  0.0991\n",
      "     71      \u001b[36m156.4623\u001b[0m      \u001b[32m266.7563\u001b[0m     +  0.0025  0.1033\n",
      "     72      \u001b[36m155.7502\u001b[0m      267.2633        0.0021  0.1137\n",
      "     73      \u001b[36m154.4296\u001b[0m      266.9509        0.0017  0.1051\n",
      "     74      \u001b[36m153.9096\u001b[0m      267.2557        0.0014  0.1151\n",
      "     75      \u001b[36m153.0639\u001b[0m      267.3667        0.0010  0.1079\n",
      "     76      \u001b[36m152.4944\u001b[0m      267.6411        0.0007  0.1071\n",
      "     77      \u001b[36m152.0822\u001b[0m      267.9379        0.0005  0.1116\n",
      "     78      \u001b[36m151.7449\u001b[0m      268.1223        0.0003  0.1101\n",
      "     79      \u001b[36m151.5054\u001b[0m      268.2143        0.0001  0.1156\n",
      "     80      \u001b[36m151.3652\u001b[0m      268.2426        0.0000  0.2160\n",
      "     81      159.1697      267.9424        0.0050  0.1135\n",
      "     82      174.3080      267.1724        0.0050  0.1069\n",
      "     83      163.6966      \u001b[32m262.6400\u001b[0m     +  0.0049  0.1052\n",
      "     84      158.9659      265.6986        0.0047  0.1095\n",
      "     85      160.2500      \u001b[32m260.3336\u001b[0m     +  0.0045  0.1033\n",
      "     86      156.0810      261.7441        0.0043  0.1039\n",
      "     87      157.6297      \u001b[32m257.4670\u001b[0m     +  0.0040  0.1077\n",
      "     88      154.2933      260.0708        0.0036  0.1047\n",
      "     89      155.0728      260.5909        0.0033  0.1003\n",
      "     90      152.0439      258.9060        0.0029  0.1027\n",
      "     91      153.4611      259.5417        0.0025  0.1016\n",
      "     92      \u001b[36m149.6945\u001b[0m      259.3907        0.0021  0.1061\n",
      "     93      151.9425      262.4685        0.0017  0.1020\n",
      "     94      \u001b[36m148.2890\u001b[0m      259.2046        0.0014  0.1078\n",
      "     95      149.2082      262.0868        0.0010  0.1029\n",
      "     96      148.4464      262.4248        0.0007  0.2090\n",
      "     97      \u001b[36m147.7610\u001b[0m      261.8230        0.0005  0.1061\n",
      "     98      \u001b[36m147.5284\u001b[0m      261.8625        0.0003  0.1036\n",
      "     99      \u001b[36m147.3650\u001b[0m      262.0495        0.0001  0.1027\n",
      "    100      \u001b[36m147.2445\u001b[0m      262.1173        0.0000  0.1053\n",
      "    101      155.1078      258.5399        0.0050  0.1035\n",
      "    102      167.3973      \u001b[32m254.7909\u001b[0m     +  0.0050  0.1034\n",
      "    103      158.5213      255.9708        0.0049  0.1060\n",
      "    104      154.6908      258.8517        0.0047  0.1037\n",
      "    105      156.9766      \u001b[32m252.7846\u001b[0m     +  0.0045  0.1062\n",
      "    106      151.9496      253.4840        0.0043  0.1014\n",
      "    107      153.0267      253.7187        0.0040  0.1087\n",
      "    108      150.7117      255.8818        0.0036  0.1048\n",
      "    109      150.5225      \u001b[32m251.9415\u001b[0m     +  0.0033  0.1036\n",
      "    110      148.1601      253.9894        0.0029  0.1052\n",
      "    111      148.9821      254.3403        0.0025  0.1085\n",
      "    112      \u001b[36m145.8732\u001b[0m      253.7474        0.0021  0.2208\n",
      "    113      147.8053      257.0750        0.0017  0.1106\n",
      "    114      \u001b[36m144.2453\u001b[0m      253.3784        0.0014  0.1122\n",
      "    115      145.4385      257.3263        0.0010  0.1091\n",
      "    116      144.3269      256.6069        0.0007  0.1177\n",
      "    117      \u001b[36m143.7632\u001b[0m      255.8640        0.0005  0.1098\n",
      "    118      \u001b[36m143.6316\u001b[0m      256.1620        0.0003  0.1117\n",
      "    119      \u001b[36m143.4739\u001b[0m      256.4683        0.0001  0.1128\n",
      "    120      \u001b[36m143.3543\u001b[0m      256.5563        0.0000  0.1107\n",
      "    121      150.6287      \u001b[32m250.8329\u001b[0m     +  0.0050  0.1115\n",
      "    122      163.3047      \u001b[32m247.4733\u001b[0m     +  0.0050  0.1135\n",
      "    123      156.7345      248.2141        0.0049  0.1101\n",
      "    124      148.2071      252.3887        0.0047  0.1118\n",
      "    125      154.8423      \u001b[32m243.4430\u001b[0m     +  0.0045  0.1249\n",
      "    126      151.0702      247.4236        0.0043  0.1124\n",
      "    127      146.5013      249.2777        0.0040  0.2340\n",
      "    128      147.7666      248.7244        0.0036  0.1157\n",
      "    129      145.4141      244.9727        0.0033  0.1130\n",
      "    130      144.6186      247.3030        0.0029  0.1122\n",
      "    131      \u001b[36m143.3420\u001b[0m      247.4698        0.0025  0.1106\n",
      "    132      \u001b[36m142.8244\u001b[0m      247.1809        0.0021  0.1106\n",
      "    133      \u001b[36m141.4188\u001b[0m      247.1742        0.0017  0.1115\n",
      "    134      141.5181      248.8986        0.0014  0.1093\n",
      "    135      \u001b[36m140.2326\u001b[0m      247.5985        0.0010  0.1106\n",
      "    136      \u001b[36m140.2038\u001b[0m      248.9966        0.0007  0.1091\n",
      "    137      \u001b[36m139.8068\u001b[0m      249.5565        0.0005  0.1145\n",
      "    138      \u001b[36m139.4805\u001b[0m      249.5111        0.0003  0.1112\n",
      "    139      \u001b[36m139.3089\u001b[0m      249.5179        0.0001  0.1110\n",
      "    140      \u001b[36m139.2145\u001b[0m      249.5424        0.0000  0.1097\n",
      "    141      145.1419      \u001b[32m243.2151\u001b[0m     +  0.0050  0.1109\n",
      "    142      158.0527      \u001b[32m242.8500\u001b[0m     +  0.0050  0.2206\n",
      "    143      156.2603      \u001b[32m237.0868\u001b[0m     +  0.0049  0.1097\n",
      "    144      143.5793      251.0525        0.0047  0.1108\n",
      "    145      149.9156      241.6248        0.0045  0.1113\n",
      "    146      154.0155      239.3813        0.0043  0.1086\n",
      "    147      142.9904      254.3116        0.0040  0.1129\n",
      "    148      146.8024      249.1573        0.0036  0.1097\n",
      "    149      145.7962      247.7223        0.0033  0.1093\n",
      "    150      141.0504      239.7741        0.0029  0.1122\n",
      "    151      140.1598      241.8562        0.0025  0.1120\n",
      "    152      140.0278      243.7921        0.0021  0.1120\n",
      "    153      \u001b[36m137.9568\u001b[0m      244.4021        0.0017  0.1125\n",
      "    154      139.2804      247.7475        0.0014  0.1136\n",
      "    155      \u001b[36m136.8322\u001b[0m      242.5794        0.0010  0.1119\n",
      "    156      137.4314      245.6310        0.0007  0.1197\n",
      "    157      136.9568      246.8189        0.0005  0.1168\n",
      "    158      \u001b[36m136.5704\u001b[0m      246.4537        0.0003  0.2213\n",
      "    159      \u001b[36m136.4268\u001b[0m      246.2959        0.0001  0.1438\n",
      "    160      \u001b[36m136.3591\u001b[0m      246.2951        0.0000  0.1080\n",
      "    161      141.3445      239.2794        0.0050  0.1101\n",
      "    162      151.4075      237.4422        0.0050  0.1094\n",
      "    163      151.4532      \u001b[32m233.7043\u001b[0m     +  0.0049  0.1143\n",
      "    164      140.5315      247.6054        0.0047  0.1195\n",
      "    165      145.0811      237.9183        0.0045  0.1202\n",
      "    166      151.4913      235.3890        0.0043  0.1221\n",
      "    167      142.1770      252.2220        0.0040  0.1124\n",
      "    168      145.0456      245.1213        0.0036  0.1162\n",
      "    169      145.8587      243.8468        0.0033  0.1091\n",
      "    170      137.7335      238.5219        0.0029  0.1101\n",
      "    171      137.8338      239.5694        0.0025  0.1106\n",
      "    172      136.5241      240.4560        0.0021  0.1119\n",
      "    173      \u001b[36m135.9187\u001b[0m      242.6072        0.0017  0.1115\n",
      "    174      \u001b[36m135.4642\u001b[0m      242.8988        0.0014  0.2191\n",
      "    175      \u001b[36m135.0425\u001b[0m      242.9500        0.0010  0.1112\n",
      "    176      \u001b[36m134.7287\u001b[0m      243.4553        0.0007  0.1119\n",
      "    177      \u001b[36m134.4663\u001b[0m      243.9232        0.0005  0.1098\n",
      "    178      \u001b[36m134.2722\u001b[0m      244.1972        0.0003  0.1135\n",
      "    179      \u001b[36m134.1412\u001b[0m      244.3301        0.0001  0.1123\n",
      "    180      \u001b[36m134.0649\u001b[0m      244.3707        0.0000  0.1123\n",
      "    181      138.6958      237.1163        0.0050  0.1145\n",
      "    182      147.8082      235.1530        0.0050  0.1134\n",
      "    183      148.3481      \u001b[32m230.5644\u001b[0m     +  0.0049  0.1092\n",
      "    184      138.6881      245.2393        0.0047  0.1120\n",
      "    185      141.4655      234.9294        0.0045  0.1110\n",
      "    186      149.3334      232.1781        0.0043  0.1124\n",
      "    187      142.3594      248.7472        0.0040  0.1208\n",
      "    188      143.2704      243.9957        0.0036  0.1211\n",
      "    189      145.2754      242.4506        0.0033  0.2367\n",
      "    190      135.7233      238.9908        0.0029  0.1236\n",
      "    191      136.9984      237.9539        0.0025  0.1303\n",
      "    192      134.3963      238.1018        0.0021  0.1126\n",
      "    193      135.8831      243.1020        0.0017  0.1132\n",
      "    194      \u001b[36m132.9077\u001b[0m      238.4167        0.0014  0.1115\n",
      "    195      134.2495      245.9613        0.0010  0.1119\n",
      "    196      \u001b[36m132.5542\u001b[0m      241.9100        0.0007  0.1085\n",
      "    197      \u001b[36m132.4188\u001b[0m      241.3897        0.0005  0.1091\n",
      "    198      \u001b[36m132.4148\u001b[0m      242.5567        0.0003  0.1136\n",
      "    199      \u001b[36m132.2469\u001b[0m      243.1302        0.0001  0.1112\n",
      "    200      \u001b[36m132.1492\u001b[0m      243.2401        0.0000  0.1118\n",
      "Fold 2 of 5\n",
      "Training Lasso\n",
      "Training Ridge\n",
      "Training XGBoost\n",
      "Training Random Forest\n",
      "Training MLP\n",
      "  epoch    train_loss    valid_loss    cp      lr     dur\n",
      "-------  ------------  ------------  ----  ------  ------\n",
      "      1      \u001b[36m353.2171\u001b[0m      \u001b[32m409.7557\u001b[0m     +  0.0050  0.1147\n",
      "      2      \u001b[36m287.7734\u001b[0m      \u001b[32m387.3132\u001b[0m     +  0.0050  0.1173\n",
      "      3      \u001b[36m266.2944\u001b[0m      406.9485        0.0049  0.2254\n",
      "      4      \u001b[36m248.4983\u001b[0m      \u001b[32m375.4525\u001b[0m     +  0.0047  0.1135\n",
      "      5      \u001b[36m243.5434\u001b[0m      \u001b[32m358.6630\u001b[0m     +  0.0045  0.1228\n",
      "      6      \u001b[36m236.3541\u001b[0m      370.2446        0.0043  0.1092\n",
      "      7      \u001b[36m226.2107\u001b[0m      361.8465        0.0040  0.1077\n",
      "      8      \u001b[36m219.7847\u001b[0m      \u001b[32m348.3631\u001b[0m     +  0.0036  0.1111\n",
      "      9      \u001b[36m212.6481\u001b[0m      \u001b[32m343.1807\u001b[0m     +  0.0033  0.1113\n",
      "     10      \u001b[36m206.9770\u001b[0m      \u001b[32m337.8844\u001b[0m     +  0.0029  0.1113\n",
      "     11      \u001b[36m202.8311\u001b[0m      \u001b[32m334.9228\u001b[0m     +  0.0025  0.1186\n",
      "     12      \u001b[36m199.2502\u001b[0m      \u001b[32m331.6188\u001b[0m     +  0.0021  0.1171\n",
      "     13      \u001b[36m196.6445\u001b[0m      \u001b[32m328.3263\u001b[0m     +  0.0017  0.1137\n",
      "     14      \u001b[36m194.5862\u001b[0m      \u001b[32m326.0498\u001b[0m     +  0.0014  0.1112\n",
      "     15      \u001b[36m192.9248\u001b[0m      \u001b[32m324.4749\u001b[0m     +  0.0010  0.1095\n",
      "     16      \u001b[36m191.7178\u001b[0m      \u001b[32m323.3615\u001b[0m     +  0.0007  0.1105\n",
      "     17      \u001b[36m190.8548\u001b[0m      \u001b[32m322.6469\u001b[0m     +  0.0005  0.1114\n",
      "     18      \u001b[36m190.2585\u001b[0m      \u001b[32m322.2432\u001b[0m     +  0.0003  0.2401\n",
      "     19      \u001b[36m189.8830\u001b[0m      \u001b[32m322.0577\u001b[0m     +  0.0001  0.1150\n",
      "     20      \u001b[36m189.6866\u001b[0m      \u001b[32m322.0089\u001b[0m     +  0.0000  0.1169\n",
      "     21      198.2688      325.6224        0.0050  0.1137\n",
      "     22      199.9922      \u001b[32m319.0343\u001b[0m     +  0.0050  0.1135\n",
      "     23      \u001b[36m188.8995\u001b[0m      \u001b[32m313.3298\u001b[0m     +  0.0049  0.1111\n",
      "     24      192.9209      313.7531        0.0047  0.1168\n",
      "     25      \u001b[36m184.0090\u001b[0m      \u001b[32m306.5037\u001b[0m     +  0.0045  0.1135\n",
      "     26      186.9868      \u001b[32m304.8493\u001b[0m     +  0.0043  0.1099\n",
      "     27      \u001b[36m179.4256\u001b[0m      \u001b[32m301.6094\u001b[0m     +  0.0040  0.1109\n",
      "     28      182.2774      \u001b[32m299.6940\u001b[0m     +  0.0036  0.1113\n",
      "     29      \u001b[36m175.5505\u001b[0m      \u001b[32m297.6090\u001b[0m     +  0.0033  0.1089\n",
      "     30      177.9911      \u001b[32m296.8996\u001b[0m     +  0.0029  0.1113\n",
      "     31      \u001b[36m172.8816\u001b[0m      \u001b[32m294.5688\u001b[0m     +  0.0025  0.1076\n",
      "     32      173.5230      \u001b[32m292.6701\u001b[0m     +  0.0021  0.1102\n",
      "     33      \u001b[36m171.2664\u001b[0m      292.7489        0.0017  0.1108\n",
      "     34      \u001b[36m170.1320\u001b[0m      \u001b[32m292.0660\u001b[0m     +  0.0014  0.2184\n",
      "     35      \u001b[36m169.5401\u001b[0m      \u001b[32m291.4481\u001b[0m     +  0.0010  0.1123\n",
      "     36      \u001b[36m168.6940\u001b[0m      \u001b[32m291.0648\u001b[0m     +  0.0007  0.1218\n",
      "     37      \u001b[36m168.0227\u001b[0m      \u001b[32m290.7507\u001b[0m     +  0.0005  0.1110\n",
      "     38      \u001b[36m167.5871\u001b[0m      \u001b[32m290.5496\u001b[0m     +  0.0003  0.1128\n",
      "     39      \u001b[36m167.3004\u001b[0m      \u001b[32m290.4617\u001b[0m     +  0.0001  0.1104\n",
      "     40      \u001b[36m167.1366\u001b[0m      \u001b[32m290.4411\u001b[0m     +  0.0000  0.1414\n",
      "     41      175.8829      293.7303        0.0050  0.1268\n",
      "     42      189.5750      290.7814        0.0050  0.1192\n",
      "     43      172.2943      \u001b[32m286.1901\u001b[0m     +  0.0049  0.1148\n",
      "     44      177.8749      292.0485        0.0047  0.1184\n",
      "     45      170.3351      \u001b[32m279.7194\u001b[0m     +  0.0045  0.1119\n",
      "     46      172.8557      287.3614        0.0043  0.1174\n",
      "     47      168.1513      \u001b[32m274.3796\u001b[0m     +  0.0040  0.1117\n",
      "     48      169.2654      278.2458        0.0036  0.1164\n",
      "     49      \u001b[36m164.5544\u001b[0m      277.9613        0.0033  0.2434\n",
      "     50      166.4754      274.7295        0.0029  0.1103\n",
      "     51      \u001b[36m161.7430\u001b[0m      274.9409        0.0025  0.1126\n",
      "     52      163.7489      275.8462        0.0021  0.1129\n",
      "     53      \u001b[36m160.0006\u001b[0m      \u001b[32m274.0472\u001b[0m     +  0.0017  0.1089\n",
      "     54      160.4079      274.5654        0.0014  0.1082\n",
      "     55      \u001b[36m159.3790\u001b[0m      274.6527        0.0010  0.1103\n",
      "     56      \u001b[36m158.4274\u001b[0m      274.3898        0.0007  0.1485\n",
      "     57      \u001b[36m158.0057\u001b[0m      274.3582        0.0005  0.1085\n",
      "     58      \u001b[36m157.6609\u001b[0m      274.4063        0.0003  0.1117\n",
      "     59      \u001b[36m157.3899\u001b[0m      274.4380        0.0001  0.1101\n",
      "     60      \u001b[36m157.2276\u001b[0m      274.4474        0.0000  0.1116\n",
      "     61      165.5694      280.0102        0.0050  0.1092\n",
      "     62      183.1051      278.2333        0.0050  0.1103\n",
      "     63      168.1686      \u001b[32m271.7123\u001b[0m     +  0.0049  0.1126\n",
      "     64      166.7097      276.5649        0.0047  0.2185\n",
      "     65      164.9125      \u001b[32m266.0831\u001b[0m     +  0.0045  0.1114\n",
      "     66      162.8349      272.9335        0.0043  0.1152\n",
      "     67      163.5398      \u001b[32m260.8904\u001b[0m     +  0.0040  0.1147\n",
      "     68      159.9346      266.1139        0.0036  0.1148\n",
      "     69      160.5377      265.5199        0.0033  0.1128\n",
      "     70      157.3301      263.3110        0.0029  0.1129\n",
      "     71      158.0671      265.3421        0.0025  0.1139\n",
      "     72      \u001b[36m155.2894\u001b[0m      263.5597        0.0021  0.1151\n",
      "     73      155.7982      263.8775        0.0017  0.1126\n",
      "     74      \u001b[36m153.8402\u001b[0m      263.4797        0.0014  0.1133\n",
      "     75      \u001b[36m153.5987\u001b[0m      263.9731        0.0010  0.1078\n",
      "     76      \u001b[36m153.0453\u001b[0m      264.0671        0.0007  0.1152\n",
      "     77      \u001b[36m152.4685\u001b[0m      263.9596        0.0005  0.1190\n",
      "     78      \u001b[36m152.1120\u001b[0m      263.9829        0.0003  0.1130\n",
      "     79      \u001b[36m151.8814\u001b[0m      264.0472        0.0001  0.1141\n",
      "     80      \u001b[36m151.7398\u001b[0m      264.0734        0.0000  0.2293\n",
      "     81      159.6061      266.7856        0.0050  0.1149\n",
      "     82      176.6091      266.3300        0.0050  0.1134\n",
      "     83      164.5986      \u001b[32m259.6180\u001b[0m     +  0.0049  0.1205\n",
      "     84      159.5854      263.6562        0.0047  0.1162\n",
      "     85      161.2017      \u001b[32m254.1891\u001b[0m     +  0.0045  0.1154\n",
      "     86      156.8428      260.8154        0.0043  0.1164\n",
      "     87      159.5079      \u001b[32m248.8654\u001b[0m     +  0.0040  0.1136\n",
      "     88      154.9204      259.7939        0.0036  0.1138\n",
      "     89      157.1086      252.3599        0.0033  0.1112\n",
      "     90      152.1514      253.0300        0.0029  0.1060\n",
      "     91      154.9566      255.4626        0.0025  0.1146\n",
      "     92      \u001b[36m149.7937\u001b[0m      251.5468        0.0021  0.1119\n",
      "     93      152.6771      255.7468        0.0017  0.1141\n",
      "     94      \u001b[36m148.3358\u001b[0m      252.9729        0.0014  0.1132\n",
      "     95      149.3766      254.6004        0.0010  0.2180\n",
      "     96      148.5781      254.6890        0.0007  0.1095\n",
      "     97      \u001b[36m147.7860\u001b[0m      254.1262        0.0005  0.1107\n",
      "     98      \u001b[36m147.5105\u001b[0m      254.1622        0.0003  0.1094\n",
      "     99      \u001b[36m147.3305\u001b[0m      254.3161        0.0001  0.1091\n",
      "    100      \u001b[36m147.1987\u001b[0m      254.3715        0.0000  0.1140\n",
      "    101      155.0662      255.0519        0.0050  0.1123\n",
      "    102      171.0024      254.8998        0.0050  0.1134\n",
      "    103      161.2307      \u001b[32m248.1485\u001b[0m     +  0.0049  0.1086\n",
      "    104      153.3196      252.6523        0.0047  0.1105\n",
      "    105      157.7856      \u001b[32m241.9129\u001b[0m     +  0.0045  0.1106\n",
      "    106      152.8094      247.1802        0.0043  0.1064\n",
      "    107      152.9271      \u001b[32m237.5331\u001b[0m     +  0.0040  0.1084\n",
      "    108      151.1460      248.9307        0.0036  0.1084\n",
      "    109      149.7952      \u001b[32m236.4918\u001b[0m     +  0.0033  0.1108\n",
      "    110      148.5260      240.9747        0.0029  0.1102\n",
      "    111      147.4773      239.0057        0.0025  0.2216\n",
      "    112      \u001b[36m145.5541\u001b[0m      238.3669        0.0021  0.1129\n",
      "    113      \u001b[36m145.2672\u001b[0m      239.9932        0.0017  0.1095\n",
      "    114      \u001b[36m143.7924\u001b[0m      239.0622        0.0014  0.1113\n",
      "    115      \u001b[36m143.5182\u001b[0m      239.7728        0.0010  0.1134\n",
      "    116      \u001b[36m142.6728\u001b[0m      239.6716        0.0007  0.1131\n",
      "    117      \u001b[36m142.2557\u001b[0m      239.8711        0.0005  0.1090\n",
      "    118      \u001b[36m141.9813\u001b[0m      240.1459        0.0003  0.1135\n",
      "    119      \u001b[36m141.7598\u001b[0m      240.2768        0.0001  0.1148\n",
      "    120      \u001b[36m141.6275\u001b[0m      240.3103        0.0000  0.1119\n",
      "    121      148.3800      239.1806        0.0050  0.1115\n",
      "    122      165.3203      244.4382        0.0050  0.1208\n",
      "    123      159.7655      \u001b[32m231.3974\u001b[0m     +  0.0049  0.1139\n",
      "    124      145.7914      237.9288        0.0047  0.1157\n",
      "    125      153.6591      232.1419        0.0045  0.1128\n",
      "    126      152.9925      231.9252        0.0043  0.1119\n",
      "    127      143.7211      \u001b[32m229.2463\u001b[0m     +  0.0040  0.2221\n",
      "    128      146.8820      235.1297        0.0036  0.1144\n",
      "    129      144.9231      \u001b[32m228.3310\u001b[0m     +  0.0033  0.1117\n",
      "    130      142.4307      231.7380        0.0029  0.1100\n",
      "    131      142.6396      \u001b[32m226.7653\u001b[0m     +  0.0025  0.1138\n",
      "    132      \u001b[36m140.2413\u001b[0m      229.0600        0.0021  0.1209\n",
      "    133      \u001b[36m140.2024\u001b[0m      230.6408        0.0017  0.1091\n",
      "    134      \u001b[36m138.7481\u001b[0m      228.2138        0.0014  0.1187\n",
      "    135      \u001b[36m138.6115\u001b[0m      229.4775        0.0010  0.1150\n",
      "    136      \u001b[36m137.9626\u001b[0m      230.1217        0.0007  0.1202\n",
      "    137      \u001b[36m137.5991\u001b[0m      230.3886        0.0005  0.1133\n",
      "    138      \u001b[36m137.3701\u001b[0m      230.5703        0.0003  0.1120\n",
      "    139      \u001b[36m137.1883\u001b[0m      230.6610        0.0001  0.1083\n",
      "    140      \u001b[36m137.0786\u001b[0m      230.6873        0.0000  0.1213\n",
      "    141      142.8667      229.2126        0.0050  0.1205\n",
      "    142      157.4555      234.0395        0.0050  0.1191\n",
      "    143      156.2471      \u001b[32m219.9402\u001b[0m     +  0.0049  0.2250\n",
      "    144      141.9627      228.7216        0.0047  0.1158\n",
      "    145      145.8345      228.7959        0.0045  0.1315\n",
      "    146      153.7338      \u001b[32m218.6671\u001b[0m     +  0.0043  0.1395\n",
      "    147      142.2621      228.8577        0.0040  0.1284\n",
      "    148      144.6114      232.7751        0.0036  0.1200\n",
      "    149      144.2232      225.0625        0.0033  0.1177\n",
      "    150      137.4850      227.1414        0.0029  0.1149\n",
      "    151      139.2115      220.7087        0.0025  0.1126\n",
      "    152      \u001b[36m136.0625\u001b[0m      223.3182        0.0021  0.1202\n",
      "    153      137.0268      225.8920        0.0017  0.1187\n",
      "    154      \u001b[36m134.6362\u001b[0m      222.0132        0.0014  0.1102\n",
      "    155      135.5040      225.3701        0.0010  0.1139\n",
      "    156      \u001b[36m134.1846\u001b[0m      224.5852        0.0007  0.1124\n",
      "    157      \u001b[36m134.0235\u001b[0m      224.7413        0.0005  0.1142\n",
      "    158      \u001b[36m133.9203\u001b[0m      225.2609        0.0003  0.2328\n",
      "    159      \u001b[36m133.7347\u001b[0m      225.4768        0.0001  0.1501\n",
      "    160      \u001b[36m133.6281\u001b[0m      225.5140        0.0000  0.1814\n",
      "    161      138.4498      224.0851        0.0050  0.1484\n",
      "    162      150.5541      228.9474        0.0050  0.1762\n",
      "    163      152.1238      \u001b[32m215.8845\u001b[0m     +  0.0049  0.1615\n",
      "    164      139.0983      225.2880        0.0047  0.1372\n",
      "    165      140.1934      227.9331        0.0045  0.1250\n",
      "    166      152.8329      \u001b[32m214.9579\u001b[0m     +  0.0043  0.1320\n",
      "    167      143.9671      229.4776        0.0040  0.1182\n",
      "    168      144.9529      233.4243        0.0036  0.1188\n",
      "    169      142.2181      222.6975        0.0033  0.1312\n",
      "    170      134.9769      224.2767        0.0029  0.1278\n",
      "    171      136.3388      218.3843        0.0025  0.1218\n",
      "    172      \u001b[36m133.2777\u001b[0m      219.9863        0.0021  0.1221\n",
      "    173      133.5056      223.2913        0.0017  0.2871\n",
      "    174      \u001b[36m132.1586\u001b[0m      220.9143        0.0014  0.1280\n",
      "    175      132.3326      222.2582        0.0010  0.1206\n",
      "    176      \u001b[36m131.6032\u001b[0m      222.2768        0.0007  0.1181\n",
      "    177      \u001b[36m131.3885\u001b[0m      222.6029        0.0005  0.1193\n",
      "    178      \u001b[36m131.2504\u001b[0m      222.9711        0.0003  0.1199\n",
      "    179      \u001b[36m131.1043\u001b[0m      223.1170        0.0001  0.1202\n",
      "    180      \u001b[36m131.0188\u001b[0m      223.1491        0.0000  0.1164\n",
      "    181      135.2450      221.2263        0.0050  0.1210\n",
      "    182      144.8019      222.9692        0.0050  0.1257\n",
      "    183      146.0638      \u001b[32m213.8506\u001b[0m     +  0.0049  0.1180\n",
      "    184      135.6948      224.5406        0.0047  0.1184\n",
      "    185      136.3061      222.0827        0.0045  0.1170\n",
      "    186      146.6732      214.5142        0.0043  0.1177\n",
      "    187      139.5028      227.8484        0.0040  0.1225\n",
      "    188      138.4887      230.4219        0.0036  0.1286\n",
      "    189      140.3633      218.4810        0.0033  0.2266\n",
      "    190      132.3777      221.0527        0.0029  0.1189\n",
      "    191      135.1131      216.7231        0.0025  0.1202\n",
      "    192      131.6836      218.0005        0.0021  0.1176\n",
      "    193      135.1137      224.8678        0.0017  0.1164\n",
      "    194      \u001b[36m130.3865\u001b[0m      218.1490        0.0014  0.1207\n",
      "    195      132.3903      226.6842        0.0010  0.1228\n",
      "    196      \u001b[36m129.1811\u001b[0m      219.9521        0.0007  0.1167\n",
      "    197      129.2656      219.7794        0.0005  0.1281\n",
      "    198      129.3447      221.2193        0.0003  0.1242\n",
      "    199      \u001b[36m129.0598\u001b[0m      221.8548        0.0001  0.1198\n",
      "    200      \u001b[36m128.9335\u001b[0m      221.9471        0.0000  0.1368\n",
      "Fold 3 of 5\n",
      "Training Lasso\n",
      "Training Ridge\n",
      "Training XGBoost\n",
      "Training Random Forest\n",
      "Training MLP\n",
      "  epoch    train_loss    valid_loss    cp      lr     dur\n",
      "-------  ------------  ------------  ----  ------  ------\n",
      "      1      \u001b[36m307.6800\u001b[0m      \u001b[32m396.8727\u001b[0m     +  0.0050  0.1137\n",
      "      2      \u001b[36m277.0192\u001b[0m      413.1036        0.0050  0.1090\n",
      "      3      \u001b[36m248.8512\u001b[0m      417.3035        0.0049  0.1163\n",
      "      4      \u001b[36m233.4613\u001b[0m      \u001b[32m364.5749\u001b[0m     +  0.0047  0.2654\n",
      "      5      \u001b[36m231.6445\u001b[0m      373.8106        0.0045  0.1191\n",
      "      6      \u001b[36m217.5598\u001b[0m      373.0271        0.0043  0.1170\n",
      "      7      \u001b[36m210.8901\u001b[0m      \u001b[32m352.6006\u001b[0m     +  0.0040  0.1203\n",
      "      8      \u001b[36m203.6969\u001b[0m      \u001b[32m348.8979\u001b[0m     +  0.0036  0.1221\n",
      "      9      \u001b[36m197.1508\u001b[0m      \u001b[32m337.8809\u001b[0m     +  0.0033  0.1211\n",
      "     10      \u001b[36m193.1500\u001b[0m      \u001b[32m334.1361\u001b[0m     +  0.0029  0.1177\n",
      "     11      \u001b[36m188.7949\u001b[0m      \u001b[32m329.5937\u001b[0m     +  0.0025  0.1212\n",
      "     12      \u001b[36m186.1718\u001b[0m      \u001b[32m325.2739\u001b[0m     +  0.0021  0.1195\n",
      "     13      \u001b[36m183.6150\u001b[0m      \u001b[32m322.7378\u001b[0m     +  0.0017  0.1164\n",
      "     14      \u001b[36m181.6116\u001b[0m      \u001b[32m320.6563\u001b[0m     +  0.0014  0.1146\n",
      "     15      \u001b[36m180.1799\u001b[0m      \u001b[32m319.1874\u001b[0m     +  0.0010  0.1215\n",
      "     16      \u001b[36m179.0306\u001b[0m      \u001b[32m318.2057\u001b[0m     +  0.0007  0.1152\n",
      "     17      \u001b[36m178.1640\u001b[0m      \u001b[32m317.5112\u001b[0m     +  0.0005  0.1155\n",
      "     18      \u001b[36m177.5748\u001b[0m      \u001b[32m317.0739\u001b[0m     +  0.0003  0.1120\n",
      "     19      \u001b[36m177.2110\u001b[0m      \u001b[32m316.8640\u001b[0m     +  0.0001  0.2502\n",
      "     20      \u001b[36m177.0223\u001b[0m      \u001b[32m316.8081\u001b[0m     +  0.0000  0.1315\n",
      "     21      185.9130      321.4896        0.0050  0.1298\n",
      "     22      190.9357      321.1649        0.0050  0.1355\n",
      "     23      177.5573      \u001b[32m306.5210\u001b[0m     +  0.0049  0.1128\n",
      "     24      180.6770      314.9045        0.0047  0.1158\n",
      "     25      \u001b[36m174.1592\u001b[0m      \u001b[32m300.9258\u001b[0m     +  0.0045  0.1184\n",
      "     26      \u001b[36m173.1133\u001b[0m      301.1472        0.0043  0.1132\n",
      "     27      \u001b[36m170.0009\u001b[0m      \u001b[32m298.0537\u001b[0m     +  0.0040  0.1150\n",
      "     28      \u001b[36m168.1492\u001b[0m      \u001b[32m295.0515\u001b[0m     +  0.0036  0.1185\n",
      "     29      \u001b[36m165.9847\u001b[0m      \u001b[32m292.1443\u001b[0m     +  0.0033  0.1102\n",
      "     30      \u001b[36m164.3407\u001b[0m      \u001b[32m290.4104\u001b[0m     +  0.0029  0.1257\n",
      "     31      \u001b[36m162.5153\u001b[0m      \u001b[32m288.4850\u001b[0m     +  0.0025  0.1346\n",
      "     32      \u001b[36m161.2578\u001b[0m      \u001b[32m287.3097\u001b[0m     +  0.0021  0.1387\n",
      "     33      \u001b[36m159.8701\u001b[0m      \u001b[32m286.3858\u001b[0m     +  0.0017  0.1361\n",
      "     34      \u001b[36m158.9131\u001b[0m      \u001b[32m285.5151\u001b[0m     +  0.0014  0.1397\n",
      "     35      \u001b[36m158.0048\u001b[0m      \u001b[32m284.8969\u001b[0m     +  0.0010  0.2360\n",
      "     36      \u001b[36m157.2908\u001b[0m      \u001b[32m284.4480\u001b[0m     +  0.0007  0.1138\n",
      "     37      \u001b[36m156.7552\u001b[0m      \u001b[32m284.1532\u001b[0m     +  0.0005  0.1221\n",
      "     38      \u001b[36m156.3598\u001b[0m      \u001b[32m283.9876\u001b[0m     +  0.0003  0.1188\n",
      "     39      \u001b[36m156.0904\u001b[0m      \u001b[32m283.9132\u001b[0m     +  0.0001  0.1209\n",
      "     40      \u001b[36m155.9364\u001b[0m      \u001b[32m283.8943\u001b[0m     +  0.0000  0.1332\n",
      "     41      164.8501      292.0791        0.0050  0.1127\n",
      "     42      180.6158      298.7736        0.0050  0.1117\n",
      "     43      167.6424      \u001b[32m281.6264\u001b[0m     +  0.0049  0.1286\n",
      "     44      161.9666      285.2225        0.0047  0.2057\n",
      "     45      163.5695      \u001b[32m278.2628\u001b[0m     +  0.0045  0.1613\n",
      "     46      158.9804      \u001b[32m276.7872\u001b[0m     +  0.0043  0.2095\n",
      "     47      160.3302      278.5874        0.0040  0.2317\n",
      "     48      156.9834      \u001b[32m274.3682\u001b[0m     +  0.0036  0.1631\n",
      "     49      157.1988      \u001b[32m274.2177\u001b[0m     +  0.0033  0.1370\n",
      "     50      \u001b[36m154.3166\u001b[0m      \u001b[32m272.8167\u001b[0m     +  0.0029  0.1403\n",
      "     51      154.8909      \u001b[32m272.1934\u001b[0m     +  0.0025  0.2611\n",
      "     52      \u001b[36m152.0063\u001b[0m      \u001b[32m271.4568\u001b[0m     +  0.0021  0.1216\n",
      "     53      152.8282      271.6972        0.0017  0.1163\n",
      "     54      \u001b[36m150.7239\u001b[0m      \u001b[32m270.8420\u001b[0m     +  0.0014  0.1323\n",
      "     55      \u001b[36m150.6168\u001b[0m      270.9304        0.0010  0.1440\n",
      "     56      \u001b[36m150.1420\u001b[0m      270.9206        0.0007  0.1687\n",
      "     57      \u001b[36m149.5933\u001b[0m      \u001b[32m270.7466\u001b[0m     +  0.0005  0.1242\n",
      "     58      \u001b[36m149.2708\u001b[0m      \u001b[32m270.6912\u001b[0m     +  0.0003  0.1217\n",
      "     59      \u001b[36m149.0695\u001b[0m      270.7016        0.0001  0.1280\n",
      "     60      \u001b[36m148.9464\u001b[0m      270.7103        0.0000  0.1326\n",
      "     61      156.7339      275.9625        0.0050  0.1886\n",
      "     62      170.5264      280.8694        0.0050  0.1471\n",
      "     63      161.6298      \u001b[32m266.1399\u001b[0m     +  0.0049  0.1531\n",
      "     64      154.3896      269.3590        0.0047  0.1686\n",
      "     65      157.3933      \u001b[32m265.8374\u001b[0m     +  0.0045  0.1570\n",
      "     66      153.2831      \u001b[32m264.5183\u001b[0m     +  0.0043  0.2373\n",
      "     67      154.2616      265.1073        0.0040  0.1215\n",
      "     68      152.0737      \u001b[32m260.7743\u001b[0m     +  0.0036  0.1166\n",
      "     69      151.3736      261.9896        0.0033  0.1149\n",
      "     70      149.6199      261.5569        0.0029  0.1198\n",
      "     71      149.4124      261.1330        0.0025  0.1149\n",
      "     72      \u001b[36m147.4630\u001b[0m      \u001b[32m260.5981\u001b[0m     +  0.0021  0.1116\n",
      "     73      147.7537      260.9915        0.0017  0.1180\n",
      "     74      \u001b[36m146.0210\u001b[0m      \u001b[32m260.1820\u001b[0m     +  0.0014  0.1142\n",
      "     75      \u001b[36m146.0048\u001b[0m      260.9299        0.0010  0.1218\n",
      "     76      \u001b[36m145.4080\u001b[0m      261.0335        0.0007  0.1200\n",
      "     77      \u001b[36m144.9308\u001b[0m      260.9652        0.0005  0.1171\n",
      "     78      \u001b[36m144.6750\u001b[0m      261.0393        0.0003  0.1203\n",
      "     79      \u001b[36m144.4935\u001b[0m      261.1178        0.0001  0.1113\n",
      "     80      \u001b[36m144.3790\u001b[0m      261.1440        0.0000  0.1166\n",
      "     81      151.4062      263.1218        0.0050  0.2171\n",
      "     82      164.5408      267.4967        0.0050  0.1111\n",
      "     83      158.7118      \u001b[32m257.3134\u001b[0m     +  0.0049  0.1097\n",
      "     84      148.6743      258.6984        0.0047  0.1224\n",
      "     85      153.1505      \u001b[32m256.6206\u001b[0m     +  0.0045  0.1262\n",
      "     86      150.2165      \u001b[32m253.8750\u001b[0m     +  0.0043  0.1211\n",
      "     87      148.5214      254.9565        0.0040  0.1184\n",
      "     88      148.3501      \u001b[32m253.3309\u001b[0m     +  0.0036  0.1135\n",
      "     89      146.4965      254.3326        0.0033  0.1161\n",
      "     90      146.1063      \u001b[32m253.0587\u001b[0m     +  0.0029  0.1216\n",
      "     91      144.6633      253.4505        0.0025  0.1141\n",
      "     92      \u001b[36m144.3789\u001b[0m      254.2280        0.0021  0.1145\n",
      "     93      \u001b[36m143.1144\u001b[0m      253.6128        0.0017  0.1127\n",
      "     94      \u001b[36m143.0160\u001b[0m      254.3634        0.0014  0.1113\n",
      "     95      \u001b[36m142.1245\u001b[0m      254.0265        0.0010  0.1127\n",
      "     96      \u001b[36m141.8411\u001b[0m      254.3772        0.0007  0.1244\n",
      "     97      \u001b[36m141.5416\u001b[0m      254.6743        0.0005  0.2291\n",
      "     98      \u001b[36m141.2689\u001b[0m      254.7568        0.0003  0.1140\n",
      "     99      \u001b[36m141.0918\u001b[0m      254.7890        0.0001  0.1164\n",
      "    100      \u001b[36m140.9923\u001b[0m      254.8024        0.0000  0.1092\n",
      "    101      146.7999      253.7884        0.0050  0.1106\n",
      "    102      159.0807      259.2994        0.0050  0.1137\n",
      "    103      156.7690      \u001b[32m247.8067\u001b[0m     +  0.0049  0.1107\n",
      "    104      144.9493      253.2572        0.0047  0.1136\n",
      "    105      149.3018      251.5683        0.0045  0.1191\n",
      "    106      149.8485      \u001b[32m245.1692\u001b[0m     +  0.0043  0.1114\n",
      "    107      143.6981      247.8663        0.0040  0.1125\n",
      "    108      145.0763      249.1515        0.0036  0.1172\n",
      "    109      143.6127      247.5729        0.0033  0.1226\n",
      "    110      142.3381      246.4374        0.0029  0.1178\n",
      "    111      141.2778      245.7319        0.0025  0.1223\n",
      "    112      \u001b[36m140.8066\u001b[0m      246.6718        0.0021  0.2288\n",
      "    113      \u001b[36m139.5641\u001b[0m      246.2106        0.0017  0.1198\n",
      "    114      139.5764      246.5328        0.0014  0.1155\n",
      "    115      \u001b[36m138.4520\u001b[0m      245.3204        0.0010  0.1121\n",
      "    116      \u001b[36m138.3471\u001b[0m      246.1066        0.0007  0.1338\n",
      "    117      \u001b[36m137.9905\u001b[0m      246.3643        0.0005  0.1128\n",
      "    118      \u001b[36m137.6956\u001b[0m      246.3121        0.0003  0.1100\n",
      "    119      \u001b[36m137.5319\u001b[0m      246.3056        0.0001  0.1112\n",
      "    120      \u001b[36m137.4426\u001b[0m      246.3145        0.0000  0.1294\n",
      "    121      142.6844      \u001b[32m242.6745\u001b[0m     +  0.0050  0.1107\n",
      "    122      153.4959      247.1070        0.0050  0.1242\n",
      "    123      155.2304      \u001b[32m236.3655\u001b[0m     +  0.0049  0.1191\n",
      "    124      142.5375      245.7770        0.0047  0.1187\n",
      "    125      143.6226      239.3038        0.0045  0.1644\n",
      "    126      152.2396      240.4316        0.0043  0.1108\n",
      "    127      145.6873      245.6672        0.0040  0.1197\n",
      "    128      141.7164      247.9410        0.0036  0.2238\n",
      "    129      141.9058      237.3076        0.0033  0.1134\n",
      "    130      137.9876      \u001b[32m233.6723\u001b[0m     +  0.0029  0.1315\n",
      "    131      137.6503      \u001b[32m233.5806\u001b[0m     +  0.0025  0.1140\n",
      "    132      \u001b[36m135.7552\u001b[0m      \u001b[32m232.4165\u001b[0m     +  0.0021  0.1140\n",
      "    133      136.1056      233.3998        0.0017  0.1192\n",
      "    134      \u001b[36m134.3391\u001b[0m      233.0052        0.0014  0.1116\n",
      "    135      134.9187      236.4378        0.0010  0.1092\n",
      "    136      \u001b[36m133.6823\u001b[0m      234.3764        0.0007  0.1153\n",
      "    137      \u001b[36m133.5644\u001b[0m      234.3709        0.0005  0.1145\n",
      "    138      \u001b[36m133.4269\u001b[0m      234.8989        0.0003  0.1153\n",
      "    139      \u001b[36m133.2577\u001b[0m      235.0773        0.0001  0.1162\n",
      "    140      \u001b[36m133.1655\u001b[0m      235.0991        0.0000  0.1163\n",
      "    141      137.6429      \u001b[32m232.3224\u001b[0m     +  0.0050  0.1104\n",
      "    142      147.0958      238.3316        0.0050  0.1083\n",
      "    143      151.5649      \u001b[32m227.4506\u001b[0m     +  0.0049  0.1239\n",
      "    144      140.4460      237.5970        0.0047  0.2278\n",
      "    145      138.5928      235.4774        0.0045  0.1130\n",
      "    146      148.0798      \u001b[32m224.7972\u001b[0m     +  0.0043  0.1137\n",
      "    147      142.3351      234.6627        0.0040  0.1174\n",
      "    148      136.9828      241.2031        0.0036  0.1087\n",
      "    149      138.0541      232.9341        0.0033  0.1100\n",
      "    150      133.8440      228.7136        0.0029  0.1103\n",
      "    151      134.1520      226.2358        0.0025  0.1111\n",
      "    152      \u001b[36m132.2996\u001b[0m      228.0636        0.0021  0.1105\n",
      "    153      133.6596      228.9358        0.0017  0.1169\n",
      "    154      \u001b[36m130.8890\u001b[0m      226.1993        0.0014  0.1742\n",
      "    155      132.3330      233.9060        0.0010  0.1305\n",
      "    156      \u001b[36m129.7701\u001b[0m      227.2083        0.0007  0.1134\n",
      "    157      129.9769      227.3627        0.0005  0.1131\n",
      "    158      129.8610      228.7071        0.0003  0.1128\n",
      "    159      \u001b[36m129.6540\u001b[0m      229.1062        0.0001  0.1139\n",
      "    160      \u001b[36m129.5646\u001b[0m      229.1296        0.0000  0.2197\n",
      "    161      132.9956      227.2921        0.0050  0.1113\n",
      "    162      140.1444      230.3513        0.0050  0.1158\n",
      "    163      144.7937      \u001b[32m222.4052\u001b[0m     +  0.0049  0.1181\n",
      "    164      136.7039      230.7950        0.0047  0.1118\n",
      "    165      132.1338      229.2157        0.0045  0.1119\n",
      "    166      140.7364      224.8278        0.0043  0.1154\n",
      "    167      140.2613      223.0467        0.0040  0.1174\n",
      "    168      133.1856      240.5181        0.0036  0.1138\n",
      "    169      137.2458      232.9893        0.0033  0.1133\n",
      "    170      137.4481      226.9135        0.0029  0.1083\n",
      "    171      130.8957      224.2192        0.0025  0.1276\n",
      "    172      \u001b[36m129.1339\u001b[0m      226.7345        0.0021  0.1136\n",
      "    173      129.1860      225.9692        0.0017  0.1130\n",
      "    174      \u001b[36m127.8794\u001b[0m      224.5579        0.0014  0.1136\n",
      "    175      128.3900      228.0282        0.0010  0.2198\n",
      "    176      \u001b[36m127.1642\u001b[0m      225.4010        0.0007  0.1086\n",
      "    177      127.2257      226.1275        0.0005  0.1107\n",
      "    178      \u001b[36m127.0767\u001b[0m      226.9736        0.0003  0.1087\n",
      "    179      \u001b[36m126.9288\u001b[0m      227.1546        0.0001  0.1107\n",
      "    180      \u001b[36m126.8589\u001b[0m      227.1604        0.0000  0.1235\n",
      "    181      130.1319      226.7722        0.0050  0.1132\n",
      "    182      135.7532      227.3694        0.0050  0.1098\n",
      "    183      141.8540      \u001b[32m222.1791\u001b[0m     +  0.0049  0.1104\n",
      "    184      135.2166      224.1051        0.0047  0.1106\n",
      "    185      129.5793      232.1628        0.0045  0.1194\n",
      "    186      135.6710      225.9955        0.0043  0.1084\n",
      "    187      139.5767      \u001b[32m216.4245\u001b[0m     +  0.0040  0.1074\n",
      "    188      132.3175      242.4998        0.0036  0.1133\n",
      "    189      135.9291      232.5530        0.0033  0.1188\n",
      "    190      137.5932      225.4745        0.0029  0.2231\n",
      "    191      128.4082      222.9575        0.0025  0.1104\n",
      "    192      127.2869      225.9800        0.0021  0.1172\n",
      "    193      \u001b[36m126.3558\u001b[0m      224.6616        0.0017  0.1097\n",
      "    194      126.4163      225.6882        0.0014  0.1124\n",
      "    195      \u001b[36m125.4967\u001b[0m      224.9043        0.0010  0.1104\n",
      "    196      \u001b[36m125.4886\u001b[0m      227.1739        0.0007  0.1125\n",
      "    197      \u001b[36m125.0813\u001b[0m      226.9102        0.0005  0.1081\n",
      "    198      \u001b[36m124.9336\u001b[0m      226.7012        0.0003  0.1088\n",
      "    199      \u001b[36m124.8669\u001b[0m      226.8023        0.0001  0.1120\n",
      "    200      \u001b[36m124.8134\u001b[0m      226.8594        0.0000  0.1129\n",
      "Fold 4 of 5\n",
      "Training Lasso\n",
      "Training Ridge\n",
      "Training XGBoost\n",
      "Training Random Forest\n",
      "Training MLP\n",
      "  epoch    train_loss    valid_loss    cp      lr     dur\n",
      "-------  ------------  ------------  ----  ------  ------\n",
      "      1      \u001b[36m321.8621\u001b[0m      \u001b[32m436.7098\u001b[0m     +  0.0050  0.1121\n",
      "      2      \u001b[36m291.1903\u001b[0m      \u001b[32m396.4717\u001b[0m     +  0.0050  0.1128\n",
      "      3      \u001b[36m260.5829\u001b[0m      426.9520        0.0049  0.1187\n",
      "      4      \u001b[36m246.0853\u001b[0m      400.1318        0.0047  0.2182\n",
      "      5      \u001b[36m240.6658\u001b[0m      \u001b[32m384.9546\u001b[0m     +  0.0045  0.1113\n",
      "      6      \u001b[36m231.5301\u001b[0m      \u001b[32m383.7911\u001b[0m     +  0.0043  0.1097\n",
      "      7      \u001b[36m219.6696\u001b[0m      \u001b[32m366.7444\u001b[0m     +  0.0040  0.1096\n",
      "      8      \u001b[36m213.5431\u001b[0m      \u001b[32m360.2230\u001b[0m     +  0.0036  0.1098\n",
      "      9      \u001b[36m205.8853\u001b[0m      \u001b[32m352.5447\u001b[0m     +  0.0033  0.1196\n",
      "     10      \u001b[36m200.7080\u001b[0m      \u001b[32m343.1468\u001b[0m     +  0.0029  0.1152\n",
      "     11      \u001b[36m196.5514\u001b[0m      \u001b[32m340.2734\u001b[0m     +  0.0025  0.1095\n",
      "     12      \u001b[36m192.8668\u001b[0m      \u001b[32m335.9988\u001b[0m     +  0.0021  0.1082\n",
      "     13      \u001b[36m190.3642\u001b[0m      \u001b[32m331.9616\u001b[0m     +  0.0017  0.1155\n",
      "     14      \u001b[36m188.2579\u001b[0m      \u001b[32m329.8590\u001b[0m     +  0.0014  0.1095\n",
      "     15      \u001b[36m186.5638\u001b[0m      \u001b[32m328.4140\u001b[0m     +  0.0010  0.1079\n",
      "     16      \u001b[36m185.3418\u001b[0m      \u001b[32m327.2271\u001b[0m     +  0.0007  0.1102\n",
      "     17      \u001b[36m184.4595\u001b[0m      \u001b[32m326.4172\u001b[0m     +  0.0005  0.1110\n",
      "     18      \u001b[36m183.8407\u001b[0m      \u001b[32m325.9230\u001b[0m     +  0.0003  0.1116\n",
      "     19      \u001b[36m183.4488\u001b[0m      \u001b[32m325.6714\u001b[0m     +  0.0001  0.1121\n",
      "     20      \u001b[36m183.2457\u001b[0m      \u001b[32m325.6119\u001b[0m     +  0.0000  0.2157\n",
      "     21      192.3780      335.0689        0.0050  0.1086\n",
      "     22      195.3506      \u001b[32m324.0769\u001b[0m     +  0.0050  0.1073\n",
      "     23      \u001b[36m182.6414\u001b[0m      \u001b[32m319.2974\u001b[0m     +  0.0049  0.1083\n",
      "     24      187.1120      319.4134        0.0047  0.1097\n",
      "     25      \u001b[36m177.4964\u001b[0m      \u001b[32m309.6513\u001b[0m     +  0.0045  0.1132\n",
      "     26      180.1729      314.2347        0.0043  0.1116\n",
      "     27      \u001b[36m173.0816\u001b[0m      \u001b[32m303.4970\u001b[0m     +  0.0040  0.1095\n",
      "     28      174.8351      307.1393        0.0036  0.1115\n",
      "     29      \u001b[36m168.9055\u001b[0m      \u001b[32m301.1911\u001b[0m     +  0.0033  0.1101\n",
      "     30      170.6006      \u001b[32m300.4430\u001b[0m     +  0.0029  0.1084\n",
      "     31      \u001b[36m165.4580\u001b[0m      \u001b[32m299.6087\u001b[0m     +  0.0025  0.1108\n",
      "     32      166.6458      \u001b[32m297.8831\u001b[0m     +  0.0021  0.1096\n",
      "     33      \u001b[36m163.4444\u001b[0m      \u001b[32m297.2122\u001b[0m     +  0.0017  0.1146\n",
      "     34      \u001b[36m162.9771\u001b[0m      \u001b[32m296.6319\u001b[0m     +  0.0014  0.1073\n",
      "     35      \u001b[36m162.2258\u001b[0m      \u001b[32m296.2413\u001b[0m     +  0.0010  0.1124\n",
      "     36      \u001b[36m161.2320\u001b[0m      \u001b[32m295.8473\u001b[0m     +  0.0007  0.2303\n",
      "     37      \u001b[36m160.6503\u001b[0m      \u001b[32m295.6199\u001b[0m     +  0.0005  0.1174\n",
      "     38      \u001b[36m160.2757\u001b[0m      \u001b[32m295.5268\u001b[0m     +  0.0003  0.1132\n",
      "     39      \u001b[36m160.0112\u001b[0m      \u001b[32m295.4940\u001b[0m     +  0.0001  0.1099\n",
      "     40      \u001b[36m159.8553\u001b[0m      \u001b[32m295.4869\u001b[0m     +  0.0000  0.1088\n",
      "     41      168.4882      301.2981        0.0050  0.1128\n",
      "     42      179.3891      298.7521        0.0050  0.1076\n",
      "     43      167.0633      \u001b[32m291.4581\u001b[0m     +  0.0049  0.1125\n",
      "     44      167.5483      293.8916        0.0047  0.1080\n",
      "     45      165.3959      \u001b[32m288.8668\u001b[0m     +  0.0045  0.1076\n",
      "     46      162.5707      289.4928        0.0043  0.1082\n",
      "     47      162.3872      \u001b[32m284.6422\u001b[0m     +  0.0040  0.1111\n",
      "     48      \u001b[36m159.4801\u001b[0m      287.0156        0.0036  0.1095\n",
      "     49      \u001b[36m159.1742\u001b[0m      \u001b[32m282.2001\u001b[0m     +  0.0033  0.1092\n",
      "     50      \u001b[36m156.5180\u001b[0m      284.6228        0.0029  0.1140\n",
      "     51      156.5332      \u001b[32m281.5031\u001b[0m     +  0.0025  0.1090\n",
      "     52      \u001b[36m153.9049\u001b[0m      \u001b[32m281.2743\u001b[0m     +  0.0021  0.2178\n",
      "     53      154.0610      281.9940        0.0017  0.1133\n",
      "     54      \u001b[36m152.2170\u001b[0m      \u001b[32m280.5823\u001b[0m     +  0.0014  0.1193\n",
      "     55      \u001b[36m151.7335\u001b[0m      280.8039        0.0010  0.1088\n",
      "     56      \u001b[36m151.1247\u001b[0m      281.1538        0.0007  0.1073\n",
      "     57      \u001b[36m150.5240\u001b[0m      281.1246        0.0005  0.1089\n",
      "     58      \u001b[36m150.1231\u001b[0m      281.0923        0.0003  0.1117\n",
      "     59      \u001b[36m149.8615\u001b[0m      281.1027        0.0001  0.1144\n",
      "     60      \u001b[36m149.7080\u001b[0m      281.1115        0.0000  0.1090\n",
      "     61      157.9176      \u001b[32m280.2990\u001b[0m     +  0.0050  0.1094\n",
      "     62      176.3293      287.5266        0.0050  0.1117\n",
      "     63      164.2628      \u001b[32m275.4262\u001b[0m     +  0.0049  0.1099\n",
      "     64      155.9965      277.4651        0.0047  0.1103\n",
      "     65      161.3450      \u001b[32m273.5462\u001b[0m     +  0.0045  0.1104\n",
      "     66      153.7390      273.7858        0.0043  0.1158\n",
      "     67      156.5672      \u001b[32m266.5121\u001b[0m     +  0.0040  0.1089\n",
      "     68      153.8594      272.5519        0.0036  0.2210\n",
      "     69      151.4426      269.6294        0.0033  0.1116\n",
      "     70      \u001b[36m149.2867\u001b[0m      \u001b[32m266.3134\u001b[0m     +  0.0029  0.1089\n",
      "     71      149.7193      266.7853        0.0025  0.1079\n",
      "     72      \u001b[36m146.8491\u001b[0m      \u001b[32m265.6725\u001b[0m     +  0.0021  0.1105\n",
      "     73      147.1864      266.5049        0.0017  0.1091\n",
      "     74      \u001b[36m144.8761\u001b[0m      \u001b[32m265.1395\u001b[0m     +  0.0014  0.1108\n",
      "     75      144.9734      265.7992        0.0010  0.1119\n",
      "     76      \u001b[36m144.2220\u001b[0m      265.7177        0.0007  0.1133\n",
      "     77      \u001b[36m143.6220\u001b[0m      265.5520        0.0005  0.1064\n",
      "     78      \u001b[36m143.3090\u001b[0m      265.6563        0.0003  0.1078\n",
      "     79      \u001b[36m143.0823\u001b[0m      265.7605        0.0001  0.1124\n",
      "     80      \u001b[36m142.9400\u001b[0m      265.7942        0.0000  0.1083\n",
      "     81      150.3754      265.2185        0.0050  0.1180\n",
      "     82      167.5536      270.6365        0.0050  0.1097\n",
      "     83      158.6344      \u001b[32m261.5655\u001b[0m     +  0.0049  0.2189\n",
      "     84      147.4191      262.0407        0.0047  0.1285\n",
      "     85      154.8600      \u001b[32m257.5140\u001b[0m     +  0.0045  0.1213\n",
      "     86      149.9164      259.3881        0.0043  0.1174\n",
      "     87      147.2170      \u001b[32m253.3700\u001b[0m     +  0.0040  0.1147\n",
      "     88      147.5786      255.8494        0.0036  0.1068\n",
      "     89      144.6033      253.6284        0.0033  0.1088\n",
      "     90      144.0793      \u001b[32m253.0885\u001b[0m     +  0.0029  0.1179\n",
      "     91      \u001b[36m142.5197\u001b[0m      \u001b[32m252.4913\u001b[0m     +  0.0025  0.1099\n",
      "     92      \u001b[36m141.7899\u001b[0m      252.8116        0.0021  0.1350\n",
      "     93      \u001b[36m140.6216\u001b[0m      253.0728        0.0017  0.1290\n",
      "     94      \u001b[36m140.1195\u001b[0m      253.6250        0.0014  0.1158\n",
      "     95      \u001b[36m139.2267\u001b[0m      253.4746        0.0010  0.1134\n",
      "     96      \u001b[36m138.8285\u001b[0m      253.9538        0.0007  0.1092\n",
      "     97      \u001b[36m138.4091\u001b[0m      254.2469        0.0005  0.1108\n",
      "     98      \u001b[36m138.0850\u001b[0m      254.3455        0.0003  0.2237\n",
      "     99      \u001b[36m137.8798\u001b[0m      254.4166        0.0001  0.1095\n",
      "    100      \u001b[36m137.7614\u001b[0m      254.4470        0.0000  0.1095\n",
      "    101      144.1785      \u001b[32m252.2643\u001b[0m     +  0.0050  0.1116\n",
      "    102      159.9309      263.7151        0.0050  0.1129\n",
      "    103      159.3940      \u001b[32m250.1851\u001b[0m     +  0.0049  0.1126\n",
      "    104      142.4384      251.1153        0.0047  0.1102\n",
      "    105      147.3713      251.6838        0.0045  0.1106\n",
      "    106      150.3904      \u001b[32m247.2303\u001b[0m     +  0.0043  0.1091\n",
      "    107      140.6236      \u001b[32m246.5563\u001b[0m     +  0.0040  0.1140\n",
      "    108      142.8743      248.1025        0.0036  0.1148\n",
      "    109      142.4613      \u001b[32m246.2328\u001b[0m     +  0.0033  0.1111\n",
      "    110      138.8049      \u001b[32m245.0040\u001b[0m     +  0.0029  0.1113\n",
      "    111      138.5831      \u001b[32m243.7643\u001b[0m     +  0.0025  0.1107\n",
      "    112      \u001b[36m137.3282\u001b[0m      246.1999        0.0021  0.1074\n",
      "    113      \u001b[36m137.0464\u001b[0m      245.5587        0.0017  0.1065\n",
      "    114      \u001b[36m135.8896\u001b[0m      245.4756        0.0014  0.2180\n",
      "    115      \u001b[36m135.5780\u001b[0m      246.9019        0.0010  0.1074\n",
      "    116      \u001b[36m135.0286\u001b[0m      246.9727        0.0007  0.1098\n",
      "    117      \u001b[36m134.6754\u001b[0m      247.0094        0.0005  0.1100\n",
      "    118      \u001b[36m134.4475\u001b[0m      247.2660        0.0003  0.1105\n",
      "    119      \u001b[36m134.2736\u001b[0m      247.4421        0.0001  0.1122\n",
      "    120      \u001b[36m134.1679\u001b[0m      247.4961        0.0000  0.1109\n",
      "    121      139.8566      243.8936        0.0050  0.1115\n",
      "    122      153.4012      255.0380        0.0050  0.1084\n",
      "    123      155.5969      \u001b[32m237.8035\u001b[0m     +  0.0049  0.1107\n",
      "    124      139.9352      244.2304        0.0047  0.1117\n",
      "    125      141.5396      244.3198        0.0045  0.1145\n",
      "    126      147.5681      \u001b[32m235.6255\u001b[0m     +  0.0043  0.1156\n",
      "    127      139.4885      243.4381        0.0040  0.1150\n",
      "    128      138.8892      242.6246        0.0036  0.1116\n",
      "    129      139.8155      244.7147        0.0033  0.2212\n",
      "    130      135.1125      238.0166        0.0029  0.1202\n",
      "    131      135.6611      239.2173        0.0025  0.1112\n",
      "    132      \u001b[36m134.1173\u001b[0m      239.8582        0.0021  0.1121\n",
      "    133      \u001b[36m134.1056\u001b[0m      242.6539        0.0017  0.1136\n",
      "    134      \u001b[36m132.6900\u001b[0m      240.5028        0.0014  0.1151\n",
      "    135      132.9257      242.4625        0.0010  0.1113\n",
      "    136      \u001b[36m132.0505\u001b[0m      241.9549        0.0007  0.1092\n",
      "    137      \u001b[36m131.8505\u001b[0m      242.1155        0.0005  0.1109\n",
      "    138      \u001b[36m131.6842\u001b[0m      242.5401        0.0003  0.1115\n",
      "    139      \u001b[36m131.5119\u001b[0m      242.7505        0.0001  0.1116\n",
      "    140      \u001b[36m131.4115\u001b[0m      242.8036        0.0000  0.1094\n",
      "    141      136.4114      238.8504        0.0050  0.1090\n",
      "    142      148.3323      246.7571        0.0050  0.1096\n",
      "    143      151.7385      \u001b[32m232.2270\u001b[0m     +  0.0049  0.1086\n",
      "    144      137.9129      238.9089        0.0047  0.1081\n",
      "    145      137.0983      245.7814        0.0045  0.2142\n",
      "    146      146.3050      \u001b[32m229.4587\u001b[0m     +  0.0043  0.1092\n",
      "    147      139.1417      247.5078        0.0040  0.1152\n",
      "    148      136.3337      234.5087        0.0036  0.1110\n",
      "    149      137.0537      239.5053        0.0033  0.1123\n",
      "    150      132.5034      236.2979        0.0029  0.1123\n",
      "    151      132.9377      235.2911        0.0025  0.1129\n",
      "    152      \u001b[36m131.3125\u001b[0m      238.5703        0.0021  0.1071\n",
      "    153      131.6778      240.1749        0.0017  0.1082\n",
      "    154      \u001b[36m129.9687\u001b[0m      238.2505        0.0014  0.1102\n",
      "    155      130.6213      241.7138        0.0010  0.1084\n",
      "    156      \u001b[36m129.3759\u001b[0m      239.3319        0.0007  0.1099\n",
      "    157      \u001b[36m129.3470\u001b[0m      239.7663        0.0005  0.1118\n",
      "    158      \u001b[36m129.2134\u001b[0m      240.6722        0.0003  0.1069\n",
      "    159      \u001b[36m129.0285\u001b[0m      240.9681        0.0001  0.1107\n",
      "    160      \u001b[36m128.9318\u001b[0m      241.0118        0.0000  0.1096\n",
      "    161      133.2191      235.7852        0.0050  0.2221\n",
      "    162      144.0299      241.9069        0.0050  0.1099\n",
      "    163      149.1159      \u001b[32m228.3805\u001b[0m     +  0.0049  0.1094\n",
      "    164      136.9617      242.1981        0.0047  0.1092\n",
      "    165      133.6521      238.1488        0.0045  0.1126\n",
      "    166      144.4729      231.3936        0.0043  0.1097\n",
      "    167      139.5141      242.3801        0.0040  0.1207\n",
      "    168      133.7303      242.0746        0.0036  0.1147\n",
      "    169      135.0597      238.1410        0.0033  0.1110\n",
      "    170      130.3848      236.3317        0.0029  0.1092\n",
      "    171      130.7936      232.8564        0.0025  0.1101\n",
      "    172      129.0390      237.7971        0.0021  0.1109\n",
      "    173      130.1428      239.6671        0.0017  0.1113\n",
      "    174      \u001b[36m127.8078\u001b[0m      235.7126        0.0014  0.1120\n",
      "    175      129.0986      243.4157        0.0010  0.1069\n",
      "    176      \u001b[36m126.9838\u001b[0m      237.8630        0.0007  0.1181\n",
      "    177      127.2286      238.3022        0.0005  0.2167\n",
      "    178      127.1210      239.7597        0.0003  0.1144\n",
      "    179      \u001b[36m126.9117\u001b[0m      240.1828        0.0001  0.1089\n",
      "    180      \u001b[36m126.8173\u001b[0m      240.2162        0.0000  0.1063\n",
      "    181      130.3648      235.2530        0.0050  0.1153\n",
      "    182      139.4051      238.1818        0.0050  0.1121\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Fold 5 of 5\n",
      "Training Lasso\n",
      "Training Ridge\n",
      "Training XGBoost\n",
      "Training Random Forest\n",
      "Training MLP\n",
      "  epoch    train_loss    valid_loss    cp      lr     dur\n",
      "-------  ------------  ------------  ----  ------  ------\n",
      "      1      \u001b[36m345.9966\u001b[0m      \u001b[32m430.5289\u001b[0m     +  0.0050  0.0999\n",
      "      2      \u001b[36m280.1041\u001b[0m      \u001b[32m420.8863\u001b[0m     +  0.0050  0.1034\n",
      "      3      \u001b[36m262.8557\u001b[0m      \u001b[32m410.6586\u001b[0m     +  0.0049  0.0997\n",
      "      4      \u001b[36m247.1818\u001b[0m      \u001b[32m388.2034\u001b[0m     +  0.0047  0.1071\n",
      "      5      \u001b[36m240.3266\u001b[0m      \u001b[32m386.8566\u001b[0m     +  0.0045  0.1046\n",
      "      6      \u001b[36m234.2340\u001b[0m      \u001b[32m386.8340\u001b[0m     +  0.0043  0.1012\n",
      "      7      \u001b[36m226.0948\u001b[0m      \u001b[32m370.5475\u001b[0m     +  0.0040  0.1037\n",
      "      8      \u001b[36m219.9539\u001b[0m      \u001b[32m363.0004\u001b[0m     +  0.0036  0.0999\n",
      "      9      \u001b[36m213.9977\u001b[0m      \u001b[32m361.7092\u001b[0m     +  0.0033  0.1036\n",
      "     10      \u001b[36m208.9452\u001b[0m      \u001b[32m354.2684\u001b[0m     +  0.0029  0.1983\n",
      "     11      \u001b[36m204.8220\u001b[0m      \u001b[32m347.1415\u001b[0m     +  0.0025  0.1056\n",
      "     12      \u001b[36m201.0610\u001b[0m      \u001b[32m342.8596\u001b[0m     +  0.0021  0.0991\n",
      "     13      \u001b[36m198.1364\u001b[0m      \u001b[32m339.5594\u001b[0m     +  0.0017  0.1003\n",
      "     14      \u001b[36m195.9314\u001b[0m      \u001b[32m336.8855\u001b[0m     +  0.0014  0.1130\n",
      "     15      \u001b[36m194.1840\u001b[0m      \u001b[32m334.8433\u001b[0m     +  0.0010  0.1054\n",
      "     16      \u001b[36m192.8403\u001b[0m      \u001b[32m333.3385\u001b[0m     +  0.0007  0.1029\n",
      "     17      \u001b[36m191.8711\u001b[0m      \u001b[32m332.3395\u001b[0m     +  0.0005  0.1034\n",
      "     18      \u001b[36m191.2158\u001b[0m      \u001b[32m331.7590\u001b[0m     +  0.0003  0.1012\n",
      "     19      \u001b[36m190.8132\u001b[0m      \u001b[32m331.4924\u001b[0m     +  0.0001  0.1019\n",
      "     20      \u001b[36m190.6071\u001b[0m      \u001b[32m331.4233\u001b[0m     +  0.0000  0.1106\n",
      "     21      199.4978      339.6105        0.0050  0.1013\n",
      "     22      200.3683      \u001b[32m327.2256\u001b[0m     +  0.0050  0.1019\n",
      "     23      \u001b[36m189.3115\u001b[0m      \u001b[32m319.3991\u001b[0m     +  0.0049  0.1001\n",
      "     24      192.2479      324.0302        0.0047  0.1016\n",
      "     25      \u001b[36m183.8570\u001b[0m      \u001b[32m306.0384\u001b[0m     +  0.0045  0.2026\n",
      "     26      184.2951      316.6411        0.0043  0.1017\n",
      "     27      \u001b[36m179.6491\u001b[0m      \u001b[32m300.3672\u001b[0m     +  0.0040  0.1026\n",
      "     28      \u001b[36m177.7197\u001b[0m      304.1758        0.0036  0.1006\n",
      "     29      \u001b[36m174.8961\u001b[0m      \u001b[32m299.3084\u001b[0m     +  0.0033  0.1029\n",
      "     30      \u001b[36m172.9172\u001b[0m      \u001b[32m295.6418\u001b[0m     +  0.0029  0.1015\n",
      "     31      \u001b[36m170.5818\u001b[0m      \u001b[32m294.9704\u001b[0m     +  0.0025  0.1056\n",
      "     32      \u001b[36m169.2437\u001b[0m      \u001b[32m293.1791\u001b[0m     +  0.0021  0.1061\n",
      "     33      \u001b[36m167.4166\u001b[0m      \u001b[32m291.3695\u001b[0m     +  0.0017  0.1016\n",
      "     34      \u001b[36m166.2103\u001b[0m      \u001b[32m290.4285\u001b[0m     +  0.0014  0.1052\n",
      "     35      \u001b[36m165.1578\u001b[0m      \u001b[32m290.0284\u001b[0m     +  0.0010  0.1010\n",
      "     36      \u001b[36m164.2570\u001b[0m      \u001b[32m289.6489\u001b[0m     +  0.0007  0.1072\n",
      "     37      \u001b[36m163.5970\u001b[0m      \u001b[32m289.3269\u001b[0m     +  0.0005  0.0985\n",
      "     38      \u001b[36m163.1283\u001b[0m      \u001b[32m289.1415\u001b[0m     +  0.0003  0.0991\n",
      "     39      \u001b[36m162.8149\u001b[0m      \u001b[32m289.0650\u001b[0m     +  0.0001  0.1003\n",
      "     40      \u001b[36m162.6386\u001b[0m      \u001b[32m289.0471\u001b[0m     +  0.0000  0.1047\n",
      "     41      172.1873      300.3290        0.0050  0.2067\n",
      "     42      188.1236      299.9426        0.0050  0.0998\n",
      "     43      171.5885      \u001b[32m283.9670\u001b[0m     +  0.0049  0.1023\n",
      "     44      169.6427      303.2378        0.0047  0.1028\n",
      "     45      174.4415      \u001b[32m277.4672\u001b[0m     +  0.0045  0.1038\n",
      "     46      163.2547      288.9562        0.0043  0.0991\n",
      "     47      169.5619      284.9472        0.0040  0.1078\n",
      "     48      \u001b[36m161.2289\u001b[0m      \u001b[32m275.9937\u001b[0m     +  0.0036  0.1013\n",
      "     49      164.7102      282.8510        0.0033  0.1013\n",
      "     50      \u001b[36m158.3984\u001b[0m      277.2711        0.0029  0.1026\n",
      "     51      161.7675      277.6920        0.0025  0.1013\n",
      "     52      \u001b[36m155.4353\u001b[0m      277.4561        0.0021  0.1033\n",
      "     53      158.4421      278.6034        0.0017  0.0997\n",
      "     54      \u001b[36m154.5923\u001b[0m      276.2869        0.0014  0.1063\n",
      "     55      \u001b[36m154.5915\u001b[0m      276.6240        0.0010  0.1013\n",
      "     56      \u001b[36m154.2879\u001b[0m      277.1876        0.0007  0.0999\n",
      "     57      \u001b[36m153.5605\u001b[0m      277.0082        0.0005  0.1954\n",
      "     58      \u001b[36m153.1157\u001b[0m      276.8639        0.0003  0.1001\n",
      "     59      \u001b[36m152.8798\u001b[0m      276.8495        0.0001  0.1149\n",
      "     60      \u001b[36m152.7413\u001b[0m      276.8576        0.0000  0.1455\n",
      "     61      161.1771      282.7841        0.0050  0.1056\n",
      "     62      176.8322      279.3898        0.0050  0.1055\n",
      "     63      165.3537      \u001b[32m272.7685\u001b[0m     +  0.0049  0.1101\n",
      "     64      159.9942      279.1109        0.0047  0.1043\n",
      "     65      162.5032      \u001b[32m269.7446\u001b[0m     +  0.0045  0.1053\n",
      "     66      157.3470      274.6839        0.0043  0.1012\n",
      "     67      159.4405      \u001b[32m268.9663\u001b[0m     +  0.0040  0.1039\n",
      "     68      155.3038      \u001b[32m267.9913\u001b[0m     +  0.0036  0.1076\n",
      "     69      156.4982      270.2363        0.0033  0.1036\n",
      "     70      153.0073      \u001b[32m267.3390\u001b[0m     +  0.0029  0.1036\n",
      "     71      154.1022      267.7554        0.0025  0.1096\n",
      "     72      \u001b[36m150.6914\u001b[0m      \u001b[32m266.3603\u001b[0m     +  0.0021  0.2044\n",
      "     73      151.8799      267.3714        0.0017  0.1093\n",
      "     74      \u001b[36m149.0630\u001b[0m      \u001b[32m265.3765\u001b[0m     +  0.0014  0.0988\n",
      "     75      149.3791      266.4835        0.0010  0.1029\n",
      "     76      \u001b[36m148.7015\u001b[0m      266.3709        0.0007  0.1027\n",
      "     77      \u001b[36m148.0410\u001b[0m      265.8734        0.0005  0.0993\n",
      "     78      \u001b[36m147.7346\u001b[0m      265.8464        0.0003  0.1079\n",
      "     79      \u001b[36m147.5320\u001b[0m      265.9414        0.0001  0.1029\n",
      "     80      \u001b[36m147.3989\u001b[0m      265.9799        0.0000  0.1024\n",
      "     81      155.3135      268.3329        0.0050  0.0989\n",
      "     82      169.5615      268.0296        0.0050  0.0986\n",
      "     83      160.0965      \u001b[32m259.7472\u001b[0m     +  0.0049  0.0981\n",
      "     84      153.9548      262.6332        0.0047  0.1021\n",
      "     85      158.1646      260.2700        0.0045  0.1028\n",
      "     86      152.1758      \u001b[32m259.4967\u001b[0m     +  0.0043  0.0990\n",
      "     87      152.9705      \u001b[32m250.6760\u001b[0m     +  0.0040  0.2014\n",
      "     88      152.9618      257.4799        0.0036  0.1027\n",
      "     89      148.2671      254.6764        0.0033  0.1015\n",
      "     90      148.9077      \u001b[32m250.5263\u001b[0m     +  0.0029  0.1012\n",
      "     91      \u001b[36m146.8951\u001b[0m      251.1325        0.0025  0.0995\n",
      "     92      \u001b[36m146.6679\u001b[0m      253.2339        0.0021  0.0985\n",
      "     93      \u001b[36m144.6721\u001b[0m      251.3848        0.0017  0.1012\n",
      "     94      144.8621      252.7036        0.0014  0.1011\n",
      "     95      \u001b[36m143.5293\u001b[0m      252.1331        0.0010  0.1008\n",
      "     96      \u001b[36m143.2464\u001b[0m      252.4641        0.0007  0.1023\n",
      "     97      \u001b[36m142.8802\u001b[0m      252.8539        0.0005  0.0982\n",
      "     98      \u001b[36m142.5268\u001b[0m      252.9628        0.0003  0.1022\n",
      "     99      \u001b[36m142.3100\u001b[0m      253.0047        0.0001  0.1046\n",
      "    100      \u001b[36m142.1934\u001b[0m      253.0223        0.0000  0.1064\n",
      "    101      148.4921      252.4121        0.0050  0.0992\n",
      "    102      162.7020      256.9716        0.0050  0.1018\n",
      "    103      158.6857      \u001b[32m241.8222\u001b[0m     +  0.0049  0.2176\n",
      "    104      146.3328      256.6056        0.0047  0.1045\n",
      "    105      152.3014      244.3260        0.0045  0.1042\n",
      "    106      150.0778      246.6011        0.0043  0.1056\n",
      "    107      146.1849      \u001b[32m238.3325\u001b[0m     +  0.0040  0.0994\n",
      "    108      146.7538      247.9292        0.0036  0.0997\n",
      "    109      143.9982      245.6860        0.0033  0.1017\n",
      "    110      143.7068      243.2999        0.0029  0.1045\n",
      "    111      \u001b[36m142.1373\u001b[0m      243.4717        0.0025  0.1023\n",
      "    112      \u001b[36m141.1706\u001b[0m      245.0947        0.0021  0.1023\n",
      "    113      \u001b[36m140.0915\u001b[0m      244.9062        0.0017  0.1027\n",
      "    114      \u001b[36m139.9005\u001b[0m      245.4656        0.0014  0.1013\n",
      "    115      \u001b[36m138.7662\u001b[0m      244.6092        0.0010  0.1019\n",
      "    116      \u001b[36m138.5879\u001b[0m      245.2826        0.0007  0.1064\n",
      "    117      \u001b[36m138.2160\u001b[0m      245.5640        0.0005  0.1021\n",
      "    118      \u001b[36m137.9086\u001b[0m      245.5624        0.0003  0.1962\n",
      "    119      \u001b[36m137.7285\u001b[0m      245.5980        0.0001  0.1040\n",
      "    120      \u001b[36m137.6276\u001b[0m      245.6231        0.0000  0.1021\n",
      "    121      143.5590      244.5947        0.0050  0.1040\n",
      "    122      156.2308      250.3526        0.0050  0.1027\n",
      "    123      156.9716      \u001b[32m236.3238\u001b[0m     +  0.0049  0.1043\n",
      "    124      142.9230      246.2153        0.0047  0.1010\n",
      "    125      145.5230      242.4188        0.0045  0.1016\n",
      "    126      148.9199      \u001b[32m234.6872\u001b[0m     +  0.0043  0.1022\n",
      "    127      141.5309      239.7025        0.0040  0.1010\n",
      "    128      141.0840      245.9397        0.0036  0.1008\n",
      "    129      141.5232      237.4774        0.0033  0.0977\n",
      "    130      138.0217      241.2415        0.0029  0.1056\n",
      "    131      138.8920      235.1379        0.0025  0.1026\n",
      "    132      \u001b[36m136.4275\u001b[0m      239.0615        0.0021  0.0995\n",
      "    133      137.2216      242.0029        0.0017  0.1022\n",
      "    134      \u001b[36m135.0498\u001b[0m      237.7593        0.0014  0.2007\n",
      "    135      135.7191      241.0459        0.0010  0.1043\n",
      "    136      \u001b[36m134.5057\u001b[0m      240.1773        0.0007  0.1001\n",
      "    137      \u001b[36m134.2915\u001b[0m      240.4637        0.0005  0.1025\n",
      "    138      \u001b[36m134.1563\u001b[0m      241.0969        0.0003  0.1020\n",
      "    139      \u001b[36m133.9741\u001b[0m      241.3582        0.0001  0.1055\n",
      "    140      \u001b[36m133.8684\u001b[0m      241.4073        0.0000  0.1083\n",
      "    141      139.1416      239.8106        0.0050  0.1016\n",
      "    142      150.2325      243.8815        0.0050  0.1039\n",
      "    143      153.5784      \u001b[32m232.5208\u001b[0m     +  0.0049  0.1033\n",
      "    144      141.5526      239.9224        0.0047  0.1006\n",
      "    145      139.0465      241.0839        0.0045  0.0955\n",
      "    146      146.4500      \u001b[32m230.5176\u001b[0m     +  0.0043  0.0934\n",
      "    147      141.0991      239.2923        0.0040  0.0922\n",
      "    148      136.2134      246.1803        0.0036  0.0976\n",
      "    149      138.9547      235.0669        0.0033  0.0938\n",
      "    150      135.4497      235.5853        0.0029  0.2019\n",
      "    151      135.4150      234.8457        0.0025  0.0961\n",
      "    152      \u001b[36m133.6461\u001b[0m      237.7472        0.0021  0.0965\n",
      "    153      134.4660      239.5889        0.0017  0.0926\n",
      "    154      \u001b[36m132.2083\u001b[0m      235.0955        0.0014  0.0939\n",
      "    155      133.1479      239.7983        0.0010  0.0931\n",
      "    156      \u001b[36m131.5810\u001b[0m      237.3767        0.0007  0.0954\n",
      "    157      \u001b[36m131.5536\u001b[0m      237.7810        0.0005  0.0954\n",
      "    158      \u001b[36m131.4538\u001b[0m      238.7527        0.0003  0.0934\n",
      "    159      \u001b[36m131.2617\u001b[0m      239.0883        0.0001  0.0946\n",
      "    160      \u001b[36m131.1617\u001b[0m      239.1351        0.0000  0.0933\n",
      "    161      135.7685      236.2265        0.0050  0.0934\n",
      "    162      145.0270      237.9497        0.0050  0.0942\n",
      "    163      149.8437      \u001b[32m229.5925\u001b[0m     +  0.0049  0.0967\n",
      "    164      140.2416      238.1283        0.0047  0.0930\n",
      "    165      134.6509      239.9889        0.0045  0.0945\n",
      "    166      142.7725      231.8830        0.0043  0.1960\n",
      "    167      140.7388      235.8697        0.0040  0.1025\n",
      "    168      133.1455      245.8024        0.0036  0.1036\n",
      "    169      136.0339      237.6120        0.0033  0.1074\n",
      "    170      134.9639      235.7947        0.0029  0.1011\n",
      "    171      131.9822      235.3384        0.0025  0.0984\n",
      "    172      131.5587      237.9158        0.0021  0.1010\n",
      "    173      \u001b[36m131.1131\u001b[0m      238.0791        0.0017  0.1064\n",
      "    174      \u001b[36m130.1871\u001b[0m      236.0562        0.0014  0.1047\n",
      "    175      130.3788      239.0366        0.0010  0.1019\n",
      "    176      \u001b[36m129.4678\u001b[0m      237.7251        0.0007  0.1066\n",
      "    177      \u001b[36m129.4200\u001b[0m      238.0295        0.0005  0.1016\n",
      "    178      \u001b[36m129.2507\u001b[0m      238.5754        0.0003  0.1026\n",
      "    179      \u001b[36m129.0992\u001b[0m      238.7405        0.0001  0.1103\n",
      "    180      \u001b[36m129.0216\u001b[0m      238.7660        0.0000  0.1079\n",
      "    181      133.1088      235.0388        0.0050  0.2064\n",
      "    182      141.3743      235.9056        0.0050  0.0986\n",
      "    183      147.2156      \u001b[32m229.3220\u001b[0m     +  0.0049  0.1009\n",
      "    184      139.1070      235.8599        0.0047  0.1021\n",
      "    185      132.7541      252.7646        0.0045  0.0975\n",
      "    186      142.8264      233.7118        0.0043  0.1012\n",
      "    187      141.8080      241.7394        0.0040  0.0982\n",
      "    188      132.9806      237.0123        0.0036  0.0991\n",
      "    189      134.2603      231.3679        0.0033  0.0966\n",
      "    190      133.6814      237.5325        0.0029  0.1008\n",
      "    191      130.5278      236.8520        0.0025  0.0949\n",
      "    192      130.0146      239.1361        0.0021  0.0978\n",
      "    193      129.5180      237.4303        0.0017  0.0962\n",
      "    194      \u001b[36m128.4933\u001b[0m      236.7340        0.0014  0.0965\n",
      "    195      128.8779      240.5185        0.0010  0.0997\n",
      "    196      \u001b[36m127.7618\u001b[0m      238.0619        0.0007  0.2048\n",
      "    197      127.8221      238.7294        0.0005  0.1149\n",
      "    198      \u001b[36m127.6592\u001b[0m      239.5530        0.0003  0.1059\n",
      "    199      \u001b[36m127.4893\u001b[0m      239.7154        0.0001  0.1111\n",
      "    200      \u001b[36m127.4097\u001b[0m      239.7190        0.0000  0.1051\n"
     ]
    }
   ],
   "source": [
    "# test the effects of undersampling\n",
    "\n",
    "from os.path import join as pjoin\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import collections\n",
    "from utils.stats_utils import get_pearson_and_spearman_correlation\n",
    "from utils.data_utils import k_fold_cross_validation_split\n",
    "from models.conventional_ml_models import lasso_regression, ridge_regression, mlp, xgboost, random_forest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "import skorch\n",
    "from models.conventional_ml_models import MLP\n",
    "import os\n",
    "\n",
    "# datas = ['ml-dp-hek293t-pe2.csv', 'ml-pd-hek293t-pe2.csv', 'ml-pd-adv-pe2.csv', 'ml-pd-k562-pe2.csv', 'ml-pd-k562mlh1d-pe2.csv']\n",
    "datas = ['ml-pd-k562mlh1dn-pe2.csv']\n",
    "\n",
    "performance = {}\n",
    "\n",
    "def undersample(features, target):\n",
    "    # sample 10% of the training data with editing efficiency of less than 10\n",
    "    indices = np.where(target < 10)[0]\n",
    "    seed = 42\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.choice(indices, int(len(indices) * 0.1), replace=False)\n",
    "    features = np.delete(features, indices, axis=0)\n",
    "    target = np.delete(target, indices, axis=0)\n",
    "    return features, target\n",
    "\n",
    "for data in datas:\n",
    "    dataset = pd.read_csv(pjoin('models', 'data', 'conventional-ml', data))\n",
    "    cell_line = '-'.join(data.split('-')[1:3]).split('.')[0]\n",
    "    data_source = '-'.join(data.split('-')[1:]).split('.')[0]\n",
    "\n",
    "    # 5 fold cross validation\n",
    "    fold = 5\n",
    "\n",
    "    features = dataset.iloc[:, :24].values\n",
    "    target = dataset.iloc[:, -2].values\n",
    "\n",
    "    # standardize the features\n",
    "    # scaler = StandardScaler()\n",
    "    # features = scaler.fit_transform(features)\n",
    "\n",
    "    correlations = collections.defaultdict(list)\n",
    "    \n",
    "    for i in range(0, fold):\n",
    "        print(f'Fold {i+1} of {fold}')\n",
    "        train_features = features[dataset['fold'] != i]\n",
    "        train_target = target[dataset['fold'] != i]\n",
    "        \n",
    "        # ============================\n",
    "        # Lasso\n",
    "        # ============================\n",
    "        print('Training Lasso')\n",
    "        lasso_model = lasso_regression()\n",
    "        lasso_model.fit(train_features, train_target)\n",
    "        # pickle the model\n",
    "        pickle.dump(lasso_model, open(pjoin('models', 'trained-models', 'conventional-ml', f'lasso-{data_source}-fold-{i+1}.pkl'), 'wb'))\n",
    "\n",
    "        # ============================\n",
    "        # Ridge\n",
    "        # ============================\n",
    "        print('Training Ridge')\n",
    "        ridge_model = ridge_regression()\n",
    "        ridge_model.fit(train_features, train_target)\n",
    "        # pickle the model\n",
    "        pickle.dump(ridge_model, open(pjoin('models', 'trained-models', 'conventional-ml', f'ridge-{data_source}-fold-{i+1}.pkl'), 'wb'))\n",
    "\n",
    "        # ============================\n",
    "        # xgboost\n",
    "        # ============================\n",
    "        print('Training XGBoost')\n",
    "        xgboost_model = xgboost()\n",
    "        xgboost_model.fit(train_features, train_target)\n",
    "        # pickle the model\n",
    "        pickle.dump(xgboost_model, open(pjoin('models', 'trained-models', 'conventional-ml', f'xgboost-{data_source}-fold-{i+1}.pkl'), 'wb'))\n",
    "\n",
    "        # ============================\n",
    "        # random forest\n",
    "        # ============================\n",
    "        print('Training Random Forest')\n",
    "        rf_model = random_forest()\n",
    "        rf_model.fit(train_features, train_target)\n",
    "        # pickle the model\n",
    "        pickle.dump(rf_model, open(pjoin('models', 'trained-models', 'conventional-ml', f'random_forest-{data_source}-fold-{i+1}.pkl'), 'wb'))\n",
    "        \n",
    "        # ============================\n",
    "        # MLP\n",
    "        # ============================\n",
    "        \n",
    "        print('Training MLP')\n",
    "        mlp_model = skorch.NeuralNetRegressor(\n",
    "            module=MLP,\n",
    "            criterion=torch.nn.MSELoss,\n",
    "            optimizer=torch.optim.Adam,\n",
    "            max_epochs=200,\n",
    "            module__activation='relu',\n",
    "            lr=0.005,\n",
    "            device='cuda',\n",
    "            batch_size=2048,\n",
    "            train_split=skorch.dataset.ValidSplit(cv=5),\n",
    "            module__hidden_layer_sizes = (64, 64,),\n",
    "            # early stopping\n",
    "            callbacks=[\n",
    "                skorch.callbacks.EarlyStopping(patience=20),\n",
    "                skorch.callbacks.Checkpoint(monitor='valid_loss_best', f_pickle=None, f_criterion=None, f_optimizer=os.path.join('models', 'trained-models', 'conventional-ml', f'mlp-{data_source}-fold-{i+1}-optimizer.pkl'), f_history=os.path.join('models', 'trained-models', 'conventional-ml', f'mlp-{data_source}-fold-{i+1}-history.json'), f_params=os.path.join('models', 'trained-models', 'conventional-ml', f'mlp-{data_source}-fold-{i+1}.pkl'), event_name='event_cp'),\n",
    "                skorch.callbacks.LRScheduler(policy=torch.optim.lr_scheduler.CosineAnnealingWarmRestarts, monitor='valid_loss', T_0=20, T_mult=1),\n",
    "            ]\n",
    "        )\n",
    "        # convert features and target to float32\n",
    "        train_features = train_features.astype(np.float32)\n",
    "        train_target = train_target.astype(np.float32)\n",
    "        # add a dimension to the target\n",
    "        train_target = train_target[:, np.newaxis]\n",
    "        mlp_model.fit(train_features, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pickled models and evaluate on the corresponding validation set\n",
    "from os.path import join as pjoin\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import collections\n",
    "from utils.stats_utils import get_pearson_and_spearman_correlation\n",
    "from utils.data_utils import k_fold_cross_validation_split\n",
    "from models.conventional_ml_models import lasso_regression, ridge_regression, mlp, xgboost, random_forest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy\n",
    "import pickle\n",
    "import torch\n",
    "import skorch\n",
    "import os\n",
    "\n",
    "from models.conventional_ml_models import MLP\n",
    "\n",
    "datas = ['ml-dp-hek293t-pe2.csv', 'ml-pd-hek293t-pe2.csv', 'ml-pd-adv-pe2.csv', 'ml-pd-k562-pe2.csv', 'ml-pd-k562mlh1dn-pe2.csv']\n",
    "\n",
    "performance = {}\n",
    "\n",
    "for data in datas:\n",
    "    dataset = pd.read_csv(pjoin('models', 'data', 'conventional-ml', data))\n",
    "    cell_line = '-'.join(data.split('-')[1:3]).split('.')[0]\n",
    "    \n",
    "    data_source = '-'.join(data.split('-')[1:]).split('.')[0]\n",
    "\n",
    "    # 5 fold cross validation\n",
    "    fold = 5\n",
    "\n",
    "    features = dataset.iloc[:, :24].values\n",
    "    target = dataset.iloc[:, -2].values\n",
    "\n",
    "    correlations = collections.defaultdict(list)\n",
    "    \n",
    "    for i in range(0, fold):\n",
    "        X_test = features[dataset['fold'] == i]\n",
    "        y_test = target[dataset['fold'] == i]\n",
    "        print(f'Fold {i+1} of {fold}')\n",
    "\n",
    "        # ============================\n",
    "        # Lasso\n",
    "        # ============================\n",
    "        lasso_model = pickle.load(open(pjoin('models', 'trained-models', 'conventional-ml', f'lasso-{data_source}-fold-{i+1}.pkl'), 'rb'))\n",
    "        lasso_pred = lasso_model.predict(X_test)\n",
    "        pearson, spearman = get_pearson_and_spearman_correlation(y_test, lasso_pred)\n",
    "        correlations['Lasso'].append((pearson, spearman))\n",
    "        print(f'Lasso: Pearson: {pearson}, Spearman: {spearman}')\n",
    "\n",
    "        # ============================\n",
    "        # Ridge\n",
    "        # ============================\n",
    "        ridge_model = pickle.load(open(pjoin('models', 'trained-models', 'conventional-ml', f'ridge-{data_source}-fold-{i+1}.pkl'), 'rb'))\n",
    "        ridge_pred = ridge_model.predict(X_test)\n",
    "        pearson, spearman = get_pearson_and_spearman_correlation(y_test, ridge_pred)\n",
    "        correlations['Ridge'].append((pearson, spearman))\n",
    "        print(f'Ridge: Pearson: {pearson}, Spearman: {spearman}')\n",
    "        \n",
    "        # ============================\n",
    "        # xgboost\n",
    "        # ============================\n",
    "        xgboost_model = pickle.load(open(pjoin('models', 'trained-models', 'conventional-ml', f'xgboost-{data_source}-fold-{i+1}.pkl'), 'rb'))\n",
    "        xgboost_pred = xgboost_model.predict(X_test)\n",
    "        pearson, spearman = get_pearson_and_spearman_correlation(y_test, xgboost_pred)\n",
    "        correlations['XGBoost'].append((pearson, spearman))\n",
    "        print(f'XGBoost: Pearson: {pearson}, Spearman: {spearman}')\n",
    "        \n",
    "        # ============================\n",
    "        # random forest\n",
    "        # ============================\n",
    "        rf_model = pickle.load(open(pjoin('models', 'trained-models', 'conventional-ml', f'random_forest-{data_source}-fold-{i+1}.pkl'), 'rb'))\n",
    "        rf_pred = rf_model.predict(X_test)\n",
    "        pearson, spearman = get_pearson_and_spearman_correlation(y_test, rf_pred)\n",
    "        correlations['RF'].append((pearson, spearman))\n",
    "        print(f'Random Forest: Pearson: {pearson}, Spearman: {spearman}')\n",
    "        \n",
    "        # ============================\n",
    "        # MLP\n",
    "        # ============================\n",
    "        # load the pickled model\n",
    "        mlp_model = skorch.NeuralNetRegressor(\n",
    "            module=MLP,\n",
    "            criterion=torch.nn.MSELoss,\n",
    "            optimizer=torch.optim.Adam,\n",
    "            max_epochs=200,\n",
    "            module__activation='relu',\n",
    "            lr=0.001,\n",
    "            device='cuda',\n",
    "            batch_size=2048,\n",
    "            module__hidden_layer_sizes = (64, 64,),\n",
    "        )\n",
    "        mlp_model.initialize()\n",
    "        mlp_model.load_params(\n",
    "            f_optimizer=os.path.join('models', 'trained-models', 'conventional-ml', f'mlp-{data_source}-fold-{i+1}-optimizer.pkl'), \n",
    "            f_history=os.path.join('models', 'trained-models', 'conventional-ml', f'mlp-{data_source}-fold-{i+1}-history.json'), \n",
    "            f_params=os.path.join('models', 'trained-models', 'conventional-ml', f'mlp-{data_source}-fold-{i+1}.pkl')\n",
    "        )\n",
    "        X_test = X_test.astype(np.float32)\n",
    "        X_test = torch.tensor(X_test)\n",
    "        mlp_pred = mlp_model.predict(X_test)\n",
    "        # flatten the prediction\n",
    "        mlp_pred = mlp_pred.flatten()\n",
    "        pearson, spearman = get_pearson_and_spearman_correlation(y_test, mlp_pred)\n",
    "        correlations['MLP'].append((pearson, spearman))\n",
    "        print(f'MLP: Pearson: {pearson}, Spearman: {spearman}')\n",
    "        \n",
    "        print('-----------------------------------')\n",
    "    \n",
    "    print(correlations)\n",
    "    performance[cell_line] = correlations\n",
    "\n",
    "# save the performance\n",
    "save_file = pjoin('models', 'data', 'performance', 'conventional-ml-performance.csv')\n",
    "performance_df = pd.DataFrame(performance)\n",
    "print(performance_df)\n",
    "performance_df.to_csv(save_file)\n",
    "print(f'Performance saved to {save_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the performance in two heat maps, one for pearson and one for spearman\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "from os.path import join as pjoin\n",
    "\n",
    "# cell line vs model\n",
    "performance_data = pjoin('models', 'data', 'performance', 'conventional-ml-performance.csv')\n",
    "df = pd.read_csv(performance_data, index_col=0)\n",
    "\n",
    "cell_lines = df.columns\n",
    "models = df.index\n",
    "\n",
    "# plot the mean of the pearson and spearman correlation\n",
    "pearson = np.zeros((len(models), len(cell_lines)))\n",
    "spearman = np.zeros((len(models), len(cell_lines)))\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    for j, cell_line in enumerate(cell_lines):\n",
    "        performance = df.loc[model, cell_line]\n",
    "        # string to list\n",
    "        performance = ast.literal_eval(performance)\n",
    "        pearson[i, j] = np.mean([x[0] for x in performance])\n",
    "        spearman[i, j] = np.mean([x[1] for x in performance])\n",
    "\n",
    "font_size = 18\n",
    "\n",
    "# pearson and spearman correlation share the same color bar\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 7), width_ratios=[1, 1])\n",
    "sns.heatmap(pearson, ax=axes[0], annot=True, xticklabels=cell_lines, yticklabels=models, cmap='icefire', cbar=False, vmin=0, vmax=1, annot_kws={'size': font_size})\n",
    "axes[0].set_title('Pearson')\n",
    "sns.heatmap(spearman, ax=axes[1], annot=True, xticklabels=cell_lines, yticklabels=models, cmap='icefire', cbar=False, vmin=0, vmax=1, annot_kws={'size': font_size})\n",
    "axes[1].set_title('Spearman')\n",
    "\n",
    "\n",
    "# increase the font size\n",
    "for ax in axes:\n",
    "    for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] + ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        item.set_fontsize(font_size)\n",
    "# # increase color bar font size\n",
    "# cbar = axes[1].collections[0].colorbar\n",
    "# cbar.ax.tick_params(labelsize=font_size)\n",
    "# cbar.set_label('Correlation', fontsize=font_size)\n",
    "\n",
    "\n",
    "# rotate the x labels\n",
    "for ax in axes:\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "# rotate the y labels\n",
    "for ax in axes:\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), rotation=0, horizontalalignment='right')\n",
    "# make sure the figure saved is not cut off\n",
    "plt.tight_layout()\n",
    "\n",
    "# save the figure\n",
    "plt.savefig(pjoin('dissertation', 'figures', 'conventional_ml_models_performance.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepPrime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Training DeepPrime\n",
    "'''\n",
    "from os.path import join as pjoin, basename\n",
    "\n",
    "from models.deepprime import DeepPrime, preprocess_deep_prime, train_deep_prime\n",
    "\n",
    "fname = pjoin('dp-pd-hek293t-pe2.csv')\n",
    "train_deep_prime(fname, hidden_size=128, num_features=24, num_layers=1, dropout=0.05, epochs=500, batch_size=1024, lr=0.005, patience=10, device='cuda', num_runs=5, adjustment='none')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Testing DeepPrime on the clinvar dataset\n",
    "'''\n",
    "from os.path import join as pjoin, basename\n",
    "\n",
    "from models.deepprime import DeepPrime, preprocess_deep_prime, train_deep_prime, predict_deep_prime\n",
    "\n",
    "data = 'dp-pd-hek293t-pe2.csv'\n",
    "predict_deep_prime(data, hidden_size=128, num_layers=1, dropout=0.05, num_features=24, adjustment='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test log and undersample adjustments on deep prime\n",
    "from models.deepprime import DeepPrime, preprocess_deep_prime, train_deep_prime, predict_deep_prime\n",
    "import pandas as pd\n",
    "\n",
    "datas = ['dp-dp-hek293t-pe2.csv', 'dp-pd-hek293t-pe2.csv', 'dp-pd-adv-pe2.csv', 'dp-pd-k562-pe2.csv', 'dp-pd-k562mlh1d-pe2.csv']\n",
    "\n",
    "for data in datas:        \n",
    "    print('Training Log')\n",
    "    deepprime_log = train_deep_prime(data, hidden_size=128, num_features=24, num_layers=1, dropout=0.05, epochs=500, batch_size=2048, lr=0.005, patience=20, device='cuda', adjustment='log')\n",
    "    \n",
    "    print('Training Undersample')\n",
    "    deepprime_undersample = train_deep_prime(data, hidden_size=128, num_features=24, num_layers=1, dropout=0.05, epochs=500, batch_size=2048, lr=0.005, patience=20, device='cuda', adjustment='undersample')\n",
    "    \n",
    "    print('Training Org')\n",
    "    deepprime_org = train_deep_prime(data, hidden_size=128, num_features=24, num_layers=1, dropout=0.05, epochs=500, batch_size=2048, lr=0.005, patience=20, device='cuda')\n",
    "        \n",
    "    print('-----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test log and undersample adjustments on deep prime\n",
    "from models.deepprime import DeepPrime, preprocess_deep_prime, train_deep_prime, predict_deep_prime\n",
    "from os.path import join as pjoin\n",
    "import pandas as pd\n",
    "\n",
    "datas = ['dp-pd-hek293t-pe2.csv', 'dp-pd-adv-pe2.csv', 'dp-pd-k562-pe2.csv', 'dp-pd-k562mlh1d-pe2.csv', 'dp-dp-hek293t-pe2.csv']\n",
    "\n",
    "performance_pearson = {}\n",
    "performance_spearman = {}\n",
    "\n",
    "\n",
    "for data in datas:\n",
    "    print('-----------------------------------')\n",
    "    print('Log adjustment')\n",
    "    # log\n",
    "    deepprime_log_pred, deepprime_log_performance = predict_deep_prime(data, hidden_size=128, num_layers=1, dropout=0.05, num_features=24, adjustment='log')\n",
    "    \n",
    "    print('-----------------------------------')\n",
    "    print('Undersample adjustment')\n",
    "    # undersample\n",
    "    deepprime_undersample_pred, deepprime_undersample_performance = predict_deep_prime(data, hidden_size=128, num_layers=1, dropout=0.05, num_features=24, adjustment='undersample')\n",
    "    \n",
    "    print('-----------------------------------')\n",
    "    print('Original')\n",
    "    # original\n",
    "    deepprime_org_pred, deepprime_og_performance = predict_deep_prime(data, hidden_size=128, num_layers=1, dropout=0.05, num_features=24)\n",
    "    \n",
    "    # save the predictions\n",
    "    performance_pearson[data.split('.')[0]] = {\n",
    "        'orginal': [x[0] for x in deepprime_og_performance],\n",
    "        'log': [x[0] for x in deepprime_log_performance],\n",
    "        'undersample': [x[0] for x in deepprime_undersample_performance]\n",
    "    }\n",
    "    \n",
    "    performance_spearman[data.split('.')[0]] = {\n",
    "        'orginal': [x[1] for x in deepprime_og_performance],\n",
    "        'log': [x[1] for x in deepprime_log_performance],\n",
    "        'undersample': [x[1] for x in deepprime_undersample_performance]\n",
    "    }\n",
    "    \n",
    "# save the performance\n",
    "performance_pearson_df = pd.DataFrame(performance_pearson)\n",
    "performance_spearman_df = pd.DataFrame(performance_spearman)\n",
    "\n",
    "# invert the x and y axis\n",
    "performance_pearson_df = performance_pearson_df.T\n",
    "performance_spearman_df = performance_spearman_df.T\n",
    "\n",
    "performance_pearson_df.to_csv(pjoin('models', 'data', 'performance', 'deep-prime-performance-pearson.csv'))\n",
    "performance_spearman_df.to_csv(pjoin('models', 'data', 'performance', 'deep-prime-performance-spearman.csv'))\n",
    "\n",
    "print('Performance saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot both the pearson and spearman correlation performance\n",
    "# each figure contains the performance of the model on the different datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import join as pjoin\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ast\n",
    "import scipy\n",
    "\n",
    "performance_pearson = pd.read_csv(pjoin('models', 'data', 'performance', 'deep-prime-performance-pearson.csv'), index_col=0)\n",
    "performance_spearman = pd.read_csv(pjoin('models', 'data', 'performance', 'deep-prime-performance-spearman.csv'), index_col=0)\n",
    "f_size = 14\n",
    "\n",
    "adjustment_shorthands = {\n",
    "    'weighted-mse': '   WMSE',\n",
    "    'quantile-transform': '     QT',\n",
    "    'original': '     OG',\n",
    "    'log': '     LA',\n",
    "    'undersample': '     US'\n",
    "}\n",
    "\n",
    "# convert all list strings to list\n",
    "import ast\n",
    "\n",
    "performance_pearson = performance_pearson.map(lambda x: ast.literal_eval(x))\n",
    "performance_spearman = performance_spearman.map(lambda x: ast.literal_eval(x))\n",
    "\n",
    "for i, data in enumerate(['dp-dp-hek293t-pe2', 'dp-pd-hek293t-pe2', 'dp-pd-k562-pe2', 'dp-pd-k562mlh1d-pe2', 'dp-pd-adv-pe2']):\n",
    "    # plot the performance of the model on the different datasets\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 1.8), width_ratios=(1.2, 1))\n",
    "    cell_data_pearson = performance_pearson.loc[data]\n",
    "    # into a dataframe of two columns, adjustment and performance\n",
    "    cell_data_pearson = pd.DataFrame(np.array(cell_data_pearson.tolist()).T, columns=['original', 'log', 'undersample'])\n",
    "    \n",
    "    cell_data_spearman = performance_spearman.loc[data]\n",
    "    cell_data_spearman = pd.DataFrame(np.array(cell_data_spearman.tolist()).T, columns=['original', 'log', 'undersample'])\n",
    "    \n",
    "    for ind, performance in enumerate([performance_pearson, performance_spearman]):\n",
    "        # plot the mean of each adjustment as barplot\n",
    "        # plot stripplot of the performance of each fold on top\n",
    "        # of the barplot\n",
    "        cell_data = cell_data_pearson if ind == 0 else cell_data_spearman\n",
    "        sns.barplot(data=cell_data, ax=ax[ind], palette=iter(sns.color_palette('icefire', 3)),alpha=0.5, orient='h', errorbar=None)\n",
    "        sns.stripplot(data=cell_data, ax=ax[ind], palette=iter(sns.color_palette('icefire', 3)), size=5, jitter=True, orient='h')\n",
    "        # ax[ind].set_title('Pearson' if ind == 0 else 'Spearman')\n",
    "        metric = 'Pearson' if ind == 0 else 'Spearman'\n",
    "        ax[ind].set_xlabel(f'{metric}', fontsize=f_size)\n",
    "        # ax[ind].set_ylabel('Adjustment')\n",
    "        \n",
    "        # fix y axis to 0 to 1\n",
    "        ax[ind].set_xlim(0, 1)\n",
    "        \n",
    "        # remove top and right spines\n",
    "        ax[ind].spines['top'].set_visible(False)\n",
    "        if ind == 0:\n",
    "            ax[ind].spines['left'].set_visible(False)\n",
    "        else:\n",
    "            ax[ind].spines['right'].set_visible(False)\n",
    "            \n",
    "        # test the significance between each adjustment with the original\n",
    "        # using the paired t-test\n",
    "        original = cell_data['original']\n",
    "        for adj in cell_data.columns:\n",
    "            if adj != 'original':\n",
    "                if np.mean(cell_data[adj]) < np.mean(cell_data['original']):\n",
    "                    t_stat, p_val = scipy.stats.ttest_rel(cell_data[adj], original)\n",
    "                    if p_val < 0.05:\n",
    "                        print(f'{data} {metric} {adjustment_shorthands[adj]}: {p_val}')\n",
    "    \n",
    "    ax[0].set_yticks(range(len(cell_data.columns)))\n",
    "    ax[0].set_yticklabels([adjustment_shorthands[adj] for adj in cell_data.columns])\n",
    "    # ax 1 should have no y labels\n",
    "    ax[1].set_yticklabels([])\n",
    "    # flip the x axis of ax 0\n",
    "    ax[0].invert_xaxis()\n",
    "    # move the y axis of ax 0 to the right\n",
    "    ax[0].yaxis.tick_right()\n",
    "    \n",
    "    # change the font of x tick labels\n",
    "    ax[0].tick_params(axis='both', which='major', labelsize=f_size)\n",
    "    ax[1].tick_params(axis='both', which='major', labelsize=f_size)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # save the figure\n",
    "    data_name = '-'.join(data.split('-')[1:3])\n",
    "    plt.savefig(pjoin('dissertation/figures', f'adjustment-deepprime-{data_name}-performance.png'), dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRIDICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.pridict import train_pridict\n",
    "\n",
    "train_pridict('pd-pd-hek293t-pe2.csv', lr=0.005, batch_size=2048, epochs=200, patience=20, num_runs=5, adjustment='none', num_features=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.pridict import predict_pridict\n",
    "\n",
    "predict_pridict('pd-pd-hek293t-pe2.csv', num_features=24, device='cuda', adjustment='none')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
