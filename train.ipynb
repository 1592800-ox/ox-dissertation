{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the effects of undersampling\n",
    "\n",
    "from os.path import join as pjoin\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import collections\n",
    "from utils.stats_utils import get_pearson_and_spearman_correlation\n",
    "from utils.data_utils import k_fold_cross_validation_split\n",
    "from models.conventional_ml_models import lasso_regression, ridge_regression, mlp, xgboost, random_forest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "import skorch\n",
    "from models.conventional_ml_models import MLP\n",
    "import os\n",
    "\n",
    "datas = ['ml-dp-hek293t-pe2.csv', 'ml-pd-hek293t-pe2.csv', 'ml-pd-adv-pe2.csv', 'ml-pd-k562-pe2.csv', 'ml-pd-k562mlh1d-pe2.csv']\n",
    "\n",
    "performance = {}\n",
    "\n",
    "def undersample(features, target):\n",
    "    # sample 10% of the training data with editing efficiency of less than 10\n",
    "    indices = np.where(target < 10)[0]\n",
    "    seed = 42\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.choice(indices, int(len(indices) * 0.1), replace=False)\n",
    "    features = np.delete(features, indices, axis=0)\n",
    "    target = np.delete(target, indices, axis=0)\n",
    "    return features, target\n",
    "\n",
    "for data in datas:\n",
    "    dataset = pd.read_csv(pjoin('models', 'data', 'conventional-ml', data))\n",
    "    cell_line = '-'.join(data.split('-')[1:3]).split('.')[0]\n",
    "    data_source = '-'.join(data.split('-')[1:]).split('.')[0]\n",
    "\n",
    "    # 5 fold cross validation\n",
    "    fold = 5\n",
    "\n",
    "    features = dataset.iloc[:, :24].values\n",
    "    target = dataset.iloc[:, -2].values\n",
    "\n",
    "    # standardize the features\n",
    "    # scaler = StandardScaler()\n",
    "    # features = scaler.fit_transform(features)\n",
    "\n",
    "    correlations = collections.defaultdict(list)\n",
    "    \n",
    "    for i in range(0, fold):\n",
    "        print(f'Fold {i+1} of {fold}')\n",
    "        train_features = features[dataset['fold'] != i]\n",
    "        train_target = target[dataset['fold'] != i]\n",
    "        \n",
    "        # undersample the training data\n",
    "        train_features, train_target = undersample(train_features, train_target)\n",
    "        \n",
    "        # ============================\n",
    "        # Lasso\n",
    "        # ============================\n",
    "        print('Training Lasso')\n",
    "        lasso_model = lasso_regression(train_features, train_target)\n",
    "        lasso_model.fit(train_features, train_target)\n",
    "        # pickle the model\n",
    "        pickle.dump(lasso_model, open(pjoin('models', 'trained-models', 'conventional-ml', f'lasso-{data_source}-fold-{i+1}.pkl'), 'wb'))\n",
    "\n",
    "        # ============================\n",
    "        # Ridge\n",
    "        # ============================\n",
    "        print('Training Ridge')\n",
    "        ridge_model = ridge_regression(train_features, train_target)\n",
    "        ridge_model.fit(train_features, train_target)\n",
    "        # pickle the model\n",
    "        pickle.dump(ridge_model, open(pjoin('models', 'trained-models', 'conventional-ml', f'ridge-{data_source}-fold-{i+1}.pkl'), 'wb'))\n",
    "\n",
    "        # ============================\n",
    "        # xgboost\n",
    "        # ============================\n",
    "        print('Training XGBoost')\n",
    "        xgboost_model = xgboost(train_features, train_target)\n",
    "        xgboost_model.fit(train_features, train_target)\n",
    "        # pickle the model\n",
    "        pickle.dump(xgboost_model, open(pjoin('models', 'trained-models', 'conventional-ml', f'xgboost-{data_source}-fold-{i+1}.pkl'), 'wb'))\n",
    "\n",
    "        # ============================\n",
    "        # random forest\n",
    "        # ============================\n",
    "        print('Training Random Forest')\n",
    "        rf_model = random_forest(train_features, train_target)\n",
    "        rf_model.fit(train_features, train_target)\n",
    "        # pickle the model\n",
    "        pickle.dump(rf_model, open(pjoin('models', 'trained-models', 'conventional-ml', f'random_forest-{data_source}-fold-{i+1}.pkl'), 'wb'))\n",
    "        \n",
    "        # ============================\n",
    "        # MLP\n",
    "        # ============================\n",
    "        \n",
    "        print('Training MLP')\n",
    "        mlp_model = skorch.NeuralNetRegressor(\n",
    "            module=MLP,\n",
    "            criterion=torch.nn.MSELoss,\n",
    "            optimizer=torch.optim.Adam,\n",
    "            max_epochs=200,\n",
    "            module__activation='relu',\n",
    "            lr=0.005,\n",
    "            device='cuda',\n",
    "            batch_size=2048,\n",
    "            train_split=skorch.dataset.ValidSplit(cv=5),\n",
    "            module__hidden_layer_sizes = (64, 64,),\n",
    "            # early stopping\n",
    "            callbacks=[\n",
    "                skorch.callbacks.EarlyStopping(patience=30),\n",
    "                skorch.callbacks.Checkpoint(monitor='valid_loss_best', f_pickle=None, f_criterion=None, f_optimizer=os.path.join('models', 'trained-models', 'conventional-ml', f'mlp-{data_source}-fold-{i+1}-optimizer.pkl'), f_history=os.path.join('models', 'trained-models', 'conventional-ml', f'mlp-{data_source}-fold-{i+1}-history.json'), f_params=os.path.join('models', 'trained-models', 'conventional-ml', f'mlp-{data_source}-fold-{i+1}.pkl'), event_name='event_cp'),\n",
    "                skorch.callbacks.LRScheduler(policy=torch.optim.lr_scheduler.CosineAnnealingWarmRestarts, monitor='valid_loss', T_0=20, T_mult=1),\n",
    "            ]\n",
    "        )\n",
    "        # convert features and target to float32\n",
    "        train_features = train_features.astype(np.float32)\n",
    "        train_target = train_target.astype(np.float32)\n",
    "        # add a dimension to the target\n",
    "        train_target = train_target[:, np.newaxis]\n",
    "        mlp_model.fit(train_features, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pickled models and evaluate on the corresponding validation set\n",
    "from os.path import join as pjoin\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import collections\n",
    "from utils.stats_utils import get_pearson_and_spearman_correlation\n",
    "from utils.data_utils import k_fold_cross_validation_split\n",
    "from models.conventional_ml_models import lasso_regression, ridge_regression, mlp, xgboost, random_forest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy\n",
    "import pickle\n",
    "import torch\n",
    "import skorch\n",
    "\n",
    "datas = ['ml-dp-hek293t-pe2.csv', 'ml-pd-hek293t-pe2.csv', 'ml-pd-adv-pe2.csv', 'ml-pd-k562-pe2.csv', 'ml-pd-k562mlh1d-pe2.csv']\n",
    "\n",
    "performance = {}\n",
    "\n",
    "for data in datas:\n",
    "    dataset = pd.read_csv(pjoin('models', 'data', 'conventional-ml', data))\n",
    "    cell_line = '-'.join(data.split('-')[1:3]).split('.')[0]\n",
    "    \n",
    "    data_source = '-'.join(data.split('-')[1:]).split('.')[0]\n",
    "\n",
    "    # 5 fold cross validation\n",
    "    fold = 5\n",
    "\n",
    "    features = dataset.iloc[:, :24].values\n",
    "    target = dataset.iloc[:, -2].values\n",
    "\n",
    "    correlations = collections.defaultdict(list)\n",
    "    \n",
    "    for i in range(0, fold):\n",
    "        X_test = features[dataset['fold'] == i]\n",
    "        y_test = target[dataset['fold'] == i]\n",
    "        print(f'Fold {i+1} of {fold}')\n",
    "\n",
    "        # ============================\n",
    "        # Lasso\n",
    "        # ============================\n",
    "        lasso_model = pickle.load(open(pjoin('models', 'trained-models', 'conventional-ml', f'lasso-{data_source}-fold-{i+1}.pkl'), 'rb'))\n",
    "        lasso_pred = lasso_model.predict(X_test)\n",
    "        pearson, spearman = get_pearson_and_spearman_correlation(y_test, lasso_pred)\n",
    "        correlations['Lasso'].append((pearson, spearman))\n",
    "        print(f'Lasso: Pearson: {pearson}, Spearman: {spearman}')\n",
    "\n",
    "        # ============================\n",
    "        # Ridge\n",
    "        # ============================\n",
    "        ridge_model = pickle.load(open(pjoin('models', 'trained-models', 'conventional-ml', f'ridge-{data_source}-fold-{i+1}.pkl'), 'rb'))\n",
    "        ridge_pred = ridge_model.predict(X_test)\n",
    "        pearson, spearman = get_pearson_and_spearman_correlation(y_test, ridge_pred)\n",
    "        correlations['Ridge'].append((pearson, spearman))\n",
    "        print(f'Ridge: Pearson: {pearson}, Spearman: {spearman}')\n",
    "        \n",
    "        # ============================\n",
    "        # xgboost\n",
    "        # ============================\n",
    "        xgboost_model = pickle.load(open(pjoin('models', 'trained-models', 'conventional-ml', f'xgboost-{data_source}-fold-{i+1}.pkl'), 'rb'))\n",
    "        xgboost_pred = xgboost_model.predict(X_test)\n",
    "        pearson, spearman = get_pearson_and_spearman_correlation(y_test, xgboost_pred)\n",
    "        correlations['XGBoost'].append((pearson, spearman))\n",
    "        print(f'XGBoost: Pearson: {pearson}, Spearman: {spearman}')\n",
    "        \n",
    "        # ============================\n",
    "        # random forest\n",
    "        # ============================\n",
    "        rf_model = pickle.load(open(pjoin('models', 'trained-models', 'conventional-ml', f'random_forest-{data_source}-fold-{i+1}.pkl'), 'rb'))\n",
    "        rf_pred = rf_model.predict(X_test)\n",
    "        pearson, spearman = get_pearson_and_spearman_correlation(y_test, rf_pred)\n",
    "        correlations['RF'].append((pearson, spearman))\n",
    "        print(f'Random Forest: Pearson: {pearson}, Spearman: {spearman}')\n",
    "        \n",
    "        # ============================\n",
    "        # MLP\n",
    "        # ============================\n",
    "        # load the pickled model\n",
    "        mlp_model = skorch.NeuralNetRegressor(\n",
    "            module=MLP,\n",
    "            criterion=torch.nn.MSELoss,\n",
    "            optimizer=torch.optim.Adam,\n",
    "            max_epochs=200,\n",
    "            module__activation='relu',\n",
    "            lr=0.001,\n",
    "            device='cuda',\n",
    "            batch_size=2048,\n",
    "            module__hidden_layer_sizes = (64, 64,),\n",
    "        )\n",
    "        mlp_model.initialize()\n",
    "        mlp_model.load_params(\n",
    "            f_optimizer=os.path.join('models', 'trained-models', 'conventional-ml', f'mlp-{data_source}-fold-{i+1}-optimizer.pkl'), \n",
    "            f_history=os.path.join('models', 'trained-models', 'conventional-ml', f'mlp-{data_source}-fold-{i+1}-history.json'), \n",
    "            f_params=os.path.join('models', 'trained-models', 'conventional-ml', f'mlp-{data_source}-fold-{i+1}.pkl')\n",
    "        )\n",
    "        X_test = X_test.astype(np.float32)\n",
    "        X_test = torch.tensor(X_test)\n",
    "        mlp_pred = mlp_model.predict(X_test)\n",
    "        # flatten the prediction\n",
    "        mlp_pred = mlp_pred.flatten()\n",
    "        pearson, spearman = get_pearson_and_spearman_correlation(y_test, mlp_pred)\n",
    "        correlations['MLP'].append((pearson, spearman))\n",
    "        print(f'MLP: Pearson: {pearson}, Spearman: {spearman}')\n",
    "        \n",
    "        print('-----------------------------------')\n",
    "    \n",
    "    print(correlations)\n",
    "    performance[cell_line] = correlations\n",
    "\n",
    "# save the performance\n",
    "save_file = pjoin('models', 'data', 'performance', 'conventional-ml-performance.csv')\n",
    "performance_df = pd.DataFrame(performance)\n",
    "print(performance_df)\n",
    "performance_df.to_csv(save_file)\n",
    "print(f'Performance saved to {save_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the performance in two heat maps, one for pearson and one for spearman\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "from os.path import join as pjoin\n",
    "\n",
    "# cell line vs model\n",
    "performance_data = pjoin('models', 'data', 'performance', 'conventional-ml-performance.csv')\n",
    "df = pd.read_csv(performance_data, index_col=0)\n",
    "\n",
    "cell_lines = df.columns\n",
    "models = df.index\n",
    "\n",
    "# plot the mean of the pearson and spearman correlation\n",
    "pearson = np.zeros((len(models), len(cell_lines)))\n",
    "spearman = np.zeros((len(models), len(cell_lines)))\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    for j, cell_line in enumerate(cell_lines):\n",
    "        performance = df.loc[model, cell_line]\n",
    "        # string to list\n",
    "        performance = ast.literal_eval(performance)\n",
    "        pearson[i, j] = np.mean([x[0] for x in performance])\n",
    "        spearman[i, j] = np.mean([x[1] for x in performance])\n",
    "\n",
    "font_size = 18\n",
    "\n",
    "# pearson and spearman correlation share the same color bar\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 7), width_ratios=[1, 1])\n",
    "sns.heatmap(pearson, ax=axes[0], annot=True, xticklabels=cell_lines, yticklabels=models, cmap='icefire', cbar=False, vmin=0, vmax=1, annot_kws={'size': font_size})\n",
    "axes[0].set_title('Pearson')\n",
    "sns.heatmap(spearman, ax=axes[1], annot=True, xticklabels=cell_lines, yticklabels=models, cmap='icefire', cbar=False, vmin=0, vmax=1, annot_kws={'size': font_size})\n",
    "axes[1].set_title('Spearman')\n",
    "\n",
    "\n",
    "# increase the font size\n",
    "for ax in axes:\n",
    "    for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] + ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        item.set_fontsize(font_size)\n",
    "# # increase color bar font size\n",
    "# cbar = axes[1].collections[0].colorbar\n",
    "# cbar.ax.tick_params(labelsize=font_size)\n",
    "# cbar.set_label('Correlation', fontsize=font_size)\n",
    "\n",
    "\n",
    "# rotate the x labels\n",
    "for ax in axes:\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "# rotate the y labels\n",
    "for ax in axes:\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), rotation=0, horizontalalignment='right')\n",
    "# make sure the figure saved is not cut off\n",
    "plt.tight_layout()\n",
    "\n",
    "# save the figure\n",
    "plt.savefig(pjoin('dissertation', 'figures', 'conventional_ml_models_performance.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepPrime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['wt-sequence', 'mut-sequence', 'pbs-length', 'rt-length',\n",
      "       'extension-length', 'edit-length', 'rha-length', 'edit-position',\n",
      "       'edit-type-replacement', 'edit-type-insertion', 'edit-type-deletion',\n",
      "       'pbs-melting-temperature', 'rtt-wt-cdna-melting-temperature',\n",
      "       'rtt-wt-cdna-new-melting-temperature', 'rtt-cdna-melting-temperature',\n",
      "       'rtt-melting-temperature', 'delta-melting-temperature',\n",
      "       'pbs-gc-content', 'rtt-gc-content', 'extension-gc-content',\n",
      "       'pbs-gc-count', 'rtt-gc-count', 'extension-gc-count',\n",
      "       'extension-minimum-free-energy', 'spacer-minimum-free-energy',\n",
      "       'spcas9-score', 'group-id', 'editing-efficiency', 'fold'],\n",
      "      dtype='object')\n",
      "Fold 1 of 5\n",
      "Training DeepPrime model...\n",
      "Run 1 of 5\n",
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1     \u001b[36m1346.4830\u001b[0m     \u001b[32m1799.0755\u001b[0m     +  1.1482\n",
      "      2      \u001b[36m987.9143\u001b[0m     \u001b[32m1076.5977\u001b[0m     +  0.6170\n",
      "      3      \u001b[36m771.1346\u001b[0m     \u001b[32m1028.3992\u001b[0m     +  0.6128\n",
      "      4      \u001b[36m682.3322\u001b[0m      \u001b[32m835.4685\u001b[0m     +  0.7714\n",
      "      5      \u001b[36m650.4126\u001b[0m      \u001b[32m776.9991\u001b[0m     +  0.6285\n",
      "      6      \u001b[36m641.1981\u001b[0m      \u001b[32m707.9892\u001b[0m     +  0.8457\n",
      "      7      650.8590      \u001b[32m686.2810\u001b[0m     +  0.6982\n",
      "      8      \u001b[36m608.0444\u001b[0m      781.0513        0.6623\n",
      "      9      \u001b[36m581.7786\u001b[0m      721.0816        0.8313\n",
      "     10      \u001b[36m578.0518\u001b[0m      \u001b[32m632.6130\u001b[0m     +  0.6469\n",
      "     11      \u001b[36m569.0985\u001b[0m      710.4952        0.6618\n",
      "     12      \u001b[36m552.2526\u001b[0m      \u001b[32m612.6359\u001b[0m     +  0.6765\n",
      "     13      554.9070      819.3160        0.8589\n",
      "     14      575.7539     1055.3039        0.6578\n",
      "     15      554.4913     1428.7877        0.6587\n",
      "     16      \u001b[36m522.8192\u001b[0m      639.4769        0.7570\n",
      "     17      \u001b[36m510.3147\u001b[0m      681.1492        0.6326\n",
      "     18      531.5877      \u001b[32m561.6291\u001b[0m     +  0.6398\n",
      "     19      \u001b[36m503.1440\u001b[0m      711.7478        0.6123\n",
      "     20      \u001b[36m498.6394\u001b[0m      722.5682        0.7354\n",
      "     21      \u001b[36m486.2537\u001b[0m      620.5016        0.6089\n",
      "     22      \u001b[36m459.0212\u001b[0m      617.1857        0.6218\n",
      "     23      \u001b[36m444.2511\u001b[0m      828.9379        0.7698\n",
      "     24      466.0940      762.4866        0.6306\n",
      "     25      452.8187      826.6067        0.6229\n",
      "     26      \u001b[36m440.1460\u001b[0m      580.1777        0.6286\n",
      "     27      \u001b[36m414.9490\u001b[0m      603.3985        0.7471\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Validation loss improved from inf to 561.6291203446209\n",
      "Run 2 of 5\n",
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1     \u001b[36m1356.4960\u001b[0m     \u001b[32m1961.9851\u001b[0m     +  0.7692\n",
      "      2      \u001b[36m987.8161\u001b[0m     \u001b[32m1260.7087\u001b[0m     +  0.6365\n",
      "      3      \u001b[36m776.8235\u001b[0m     \u001b[32m1079.7335\u001b[0m     +  0.6374\n",
      "      4      \u001b[36m700.0592\u001b[0m      \u001b[32m832.1263\u001b[0m     +  0.7923\n",
      "      5      \u001b[36m692.7403\u001b[0m      \u001b[32m693.9396\u001b[0m     +  0.6561\n",
      "      6      \u001b[36m669.5066\u001b[0m      757.8126        0.6473\n",
      "      7      \u001b[36m636.0914\u001b[0m      734.3472        0.8020\n",
      "      8      \u001b[36m609.0525\u001b[0m      \u001b[32m644.0658\u001b[0m     +  0.6433\n",
      "      9      \u001b[36m588.5279\u001b[0m      \u001b[32m608.9704\u001b[0m     +  0.6381\n",
      "     10      \u001b[36m576.1708\u001b[0m      620.4643        0.6371\n",
      "     11      \u001b[36m559.4207\u001b[0m      \u001b[32m588.9506\u001b[0m     +  0.7677\n",
      "     12      567.6624      594.3245        0.6396\n",
      "     13      \u001b[36m558.3470\u001b[0m      624.6859        0.6374\n",
      "     14      \u001b[36m533.3103\u001b[0m      \u001b[32m544.2403\u001b[0m     +  0.7799\n",
      "     15      \u001b[36m518.3461\u001b[0m      \u001b[32m511.0348\u001b[0m     +  0.6421\n",
      "     16      521.4284      768.9133        0.6395\n",
      "     17      520.5487      671.7572        0.7713\n",
      "     18      527.9467      654.4343        0.6468\n",
      "     19      554.5685     1394.3253        0.6431\n",
      "     20      \u001b[36m497.7807\u001b[0m      798.2499        0.7787\n",
      "     21      \u001b[36m468.0068\u001b[0m      607.6784        0.6479\n",
      "     22      \u001b[36m465.5454\u001b[0m      767.6816        0.6565\n",
      "     23      \u001b[36m433.4976\u001b[0m      600.8750        0.7907\n",
      "     24      \u001b[36m427.0917\u001b[0m      761.6053        0.6472\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Validation loss improved from 561.6291203446209 to 511.03482985338627\n",
      "Run 3 of 5\n",
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1     \u001b[36m1347.5454\u001b[0m     \u001b[32m1975.4090\u001b[0m     +  0.7830\n",
      "      2      \u001b[36m972.0120\u001b[0m     \u001b[32m1055.2750\u001b[0m     +  0.7879\n",
      "      3      \u001b[36m764.8602\u001b[0m     1056.5888        0.6404\n",
      "      4      \u001b[36m692.9277\u001b[0m      \u001b[32m857.0689\u001b[0m     +  0.6626\n",
      "      5      706.2627      \u001b[32m692.9841\u001b[0m     +  0.7900\n",
      "      6      \u001b[36m678.1851\u001b[0m      \u001b[32m692.0675\u001b[0m     +  0.6557\n",
      "      7      \u001b[36m641.4771\u001b[0m      \u001b[32m680.5875\u001b[0m     +  0.6672\n",
      "      8      \u001b[36m619.2164\u001b[0m      689.3572        0.8427\n",
      "      9      \u001b[36m614.4712\u001b[0m      \u001b[32m573.5267\u001b[0m     +  0.7012\n",
      "     10      \u001b[36m599.5981\u001b[0m      755.4407        0.6729\n",
      "     11      603.2334      651.9329        0.6848\n",
      "     12      \u001b[36m577.0620\u001b[0m      589.4963        0.7938\n",
      "     13      \u001b[36m565.4501\u001b[0m      731.9970        0.6590\n",
      "     14      \u001b[36m548.3087\u001b[0m      789.4820        0.7782\n",
      "     15      \u001b[36m523.6982\u001b[0m      604.1865        0.6549\n",
      "     16      \u001b[36m481.6871\u001b[0m      727.9842        0.6414\n",
      "     17      \u001b[36m456.9870\u001b[0m      \u001b[32m523.6186\u001b[0m     +  0.7761\n",
      "     18      \u001b[36m432.3970\u001b[0m      547.5205        0.6472\n",
      "     19      \u001b[36m411.8996\u001b[0m      675.1074        0.6499\n",
      "     20      \u001b[36m399.6857\u001b[0m      \u001b[32m491.3347\u001b[0m     +  0.7831\n",
      "     21      \u001b[36m381.4410\u001b[0m      495.1386        0.7789\n",
      "     22      \u001b[36m374.0738\u001b[0m      711.9074        0.6441\n",
      "     23      495.0956     1205.3371        0.6394\n",
      "     24      503.9982      642.3469        0.6501\n",
      "     25      430.1628      614.5403        0.7889\n",
      "     26      395.1608      603.7460        0.6494\n",
      "     27      \u001b[36m369.6079\u001b[0m      536.9278        0.6487\n",
      "     28      \u001b[36m351.6822\u001b[0m      518.9478        0.8043\n",
      "     29      \u001b[36m334.7773\u001b[0m      677.5388        0.6493\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Validation loss improved from 511.03482985338627 to 491.33474253134483\n",
      "Run 4 of 5\n",
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1     \u001b[36m1347.1235\u001b[0m     \u001b[32m1956.9943\u001b[0m     +  0.6628\n",
      "      2      \u001b[36m983.7525\u001b[0m     \u001b[32m1444.6062\u001b[0m     +  0.8034\n",
      "      3      \u001b[36m772.1348\u001b[0m     \u001b[32m1005.8761\u001b[0m     +  0.6688\n",
      "      4      \u001b[36m695.1371\u001b[0m      \u001b[32m767.5497\u001b[0m     +  0.8050\n",
      "      5      \u001b[36m683.0007\u001b[0m      \u001b[32m738.8850\u001b[0m     +  0.6684\n",
      "      6      \u001b[36m645.9828\u001b[0m      \u001b[32m710.7746\u001b[0m     +  0.6530\n",
      "      7      \u001b[36m611.0174\u001b[0m      \u001b[32m663.3893\u001b[0m     +  0.6545\n",
      "      8      \u001b[36m592.8823\u001b[0m      704.2679        0.7923\n",
      "      9      \u001b[36m577.7237\u001b[0m      \u001b[32m654.2869\u001b[0m     +  0.6629\n",
      "     10      \u001b[36m567.5340\u001b[0m      \u001b[32m594.9647\u001b[0m     +  0.8069\n",
      "     11      603.8684      874.1147        0.6565\n",
      "     12      593.4195      627.1929        0.6546\n",
      "     13      \u001b[36m559.6793\u001b[0m      640.7842        0.7994\n",
      "     14      \u001b[36m536.0602\u001b[0m      672.9715        0.6622\n",
      "     15      542.9377      722.0796        0.6567\n",
      "     16      \u001b[36m530.2400\u001b[0m      \u001b[32m526.1737\u001b[0m     +  0.7962\n",
      "     17      532.4737      543.2382        0.6587\n",
      "     18      \u001b[36m511.6055\u001b[0m      715.7182        0.6551\n",
      "     19      529.9876      780.7086        0.7914\n",
      "     20      \u001b[36m501.2789\u001b[0m      606.4171        0.6700\n",
      "     21      \u001b[36m477.8770\u001b[0m      546.0037        0.6639\n",
      "     22      \u001b[36m469.3255\u001b[0m      611.3743        0.8005\n",
      "     23      477.7552      671.5449        0.6639\n",
      "     24      \u001b[36m458.5841\u001b[0m      599.9576        0.8138\n",
      "     25      \u001b[36m448.4503\u001b[0m      648.7291        0.6654\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Validation loss did not improve from 491.33474253134483\n",
      "Run 5 of 5\n",
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1     \u001b[36m1350.4917\u001b[0m     \u001b[32m1901.3170\u001b[0m     +  0.7836\n",
      "      2      \u001b[36m996.3894\u001b[0m     \u001b[32m1183.9112\u001b[0m     +  0.6546\n",
      "      3      \u001b[36m790.3538\u001b[0m     \u001b[32m1006.2021\u001b[0m     +  0.6460\n",
      "      4      \u001b[36m710.3272\u001b[0m     1103.9652        0.7826\n",
      "      5      \u001b[36m681.5315\u001b[0m      \u001b[32m736.6167\u001b[0m     +  0.6457\n",
      "      6      \u001b[36m660.3196\u001b[0m      \u001b[32m610.9027\u001b[0m     +  0.6621\n",
      "      7      \u001b[36m627.3840\u001b[0m      720.4386        0.7797\n",
      "      8      \u001b[36m596.3628\u001b[0m      646.6799        0.6463\n",
      "      9      \u001b[36m588.5357\u001b[0m      650.2504        0.6530\n",
      "     10      596.3336      \u001b[32m591.9831\u001b[0m     +  0.7844\n",
      "     11      \u001b[36m583.4433\u001b[0m      694.1100        0.6839\n",
      "     12      \u001b[36m549.8891\u001b[0m      630.9440        0.6546\n",
      "     13      \u001b[36m543.1390\u001b[0m      645.7149        0.6585\n",
      "     14      \u001b[36m531.6899\u001b[0m      733.0616        0.7933\n",
      "     15      \u001b[36m520.3111\u001b[0m      711.3129        0.6516\n",
      "     16      \u001b[36m509.1286\u001b[0m      \u001b[32m544.5055\u001b[0m     +  0.6599\n",
      "     17      520.3363      872.8310        0.7986\n",
      "     18      524.8992      600.8613        0.6602\n",
      "     19      593.0494      \u001b[32m497.3596\u001b[0m     +  0.6635\n",
      "     20      \u001b[36m503.9116\u001b[0m      743.7330        0.7981\n",
      "     21      \u001b[36m499.3850\u001b[0m      570.1438        0.6753\n",
      "     22      \u001b[36m483.0005\u001b[0m      697.5236        0.6609\n",
      "     23      \u001b[36m456.1390\u001b[0m      713.2558        0.8011\n",
      "     24      \u001b[36m444.7951\u001b[0m      596.8744        0.6710\n",
      "     25      \u001b[36m422.0946\u001b[0m      574.8098        0.8064\n",
      "     26      \u001b[36m405.7690\u001b[0m      628.7447        0.6592\n",
      "     27      \u001b[36m389.9240\u001b[0m      609.8616        0.6760\n",
      "     28      \u001b[36m387.0603\u001b[0m      646.2426        0.8231\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Validation loss did not improve from 491.33474253134483\n",
      "Training done.\n",
      "Fold 2 of 5\n",
      "Training DeepPrime model...\n",
      "Run 1 of 5\n",
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1     \u001b[36m1336.6963\u001b[0m     \u001b[32m2043.7882\u001b[0m     +  0.6922\n",
      "      2      \u001b[36m963.5075\u001b[0m     \u001b[32m1207.2842\u001b[0m     +  0.8360\n",
      "      3      \u001b[36m754.4362\u001b[0m      \u001b[32m991.0801\u001b[0m     +  0.6669\n",
      "      4      \u001b[36m691.7778\u001b[0m      \u001b[32m804.3770\u001b[0m     +  0.6766\n",
      "      5      \u001b[36m666.0246\u001b[0m      \u001b[32m781.1641\u001b[0m     +  0.6860\n",
      "      6      \u001b[36m645.7845\u001b[0m      \u001b[32m702.5972\u001b[0m     +  0.9514\n",
      "      7      656.2900      \u001b[32m702.3766\u001b[0m     +  0.6928\n",
      "      8      \u001b[36m644.7237\u001b[0m      714.4674        0.8451\n",
      "      9      \u001b[36m635.5567\u001b[0m      \u001b[32m549.6839\u001b[0m     +  0.6965\n",
      "     10      \u001b[36m609.8385\u001b[0m      769.0405        0.6946\n",
      "     11      \u001b[36m563.3859\u001b[0m      618.3394        0.8367\n",
      "     12      \u001b[36m508.3775\u001b[0m      \u001b[32m505.6401\u001b[0m     +  0.7284\n",
      "     13      \u001b[36m506.1602\u001b[0m      745.6469        0.7070\n",
      "     14      \u001b[36m475.6251\u001b[0m      637.2372        0.8348\n",
      "     15      \u001b[36m454.3618\u001b[0m      588.1412        0.6619\n",
      "     16      \u001b[36m417.3179\u001b[0m      598.6010        0.6571\n",
      "     17      \u001b[36m408.5241\u001b[0m      620.4161        0.6560\n",
      "     18      \u001b[36m391.8292\u001b[0m      616.2478        0.7976\n",
      "     19      \u001b[36m384.1273\u001b[0m      599.0853        0.6577\n",
      "     20      \u001b[36m369.1706\u001b[0m      613.3026        0.8017\n",
      "     21      \u001b[36m353.1783\u001b[0m      545.9156        0.6511\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Validation loss improved from inf to 505.64009678248\n",
      "Run 2 of 5\n",
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1     \u001b[36m1325.0639\u001b[0m     \u001b[32m1927.0056\u001b[0m     +  0.6614\n",
      "      2      \u001b[36m950.6581\u001b[0m     \u001b[32m1154.3554\u001b[0m     +  0.8147\n",
      "      3      \u001b[36m754.1501\u001b[0m     \u001b[32m1030.2776\u001b[0m     +  0.6789\n",
      "      4      \u001b[36m690.2079\u001b[0m      \u001b[32m930.1403\u001b[0m     +  0.6684\n",
      "      5      \u001b[36m647.7660\u001b[0m      \u001b[32m767.8335\u001b[0m     +  0.6645\n",
      "      6      \u001b[36m615.2060\u001b[0m      \u001b[32m652.5957\u001b[0m     +  0.8372\n",
      "      7      625.1894      721.5325        0.6756\n",
      "      8      \u001b[36m613.0095\u001b[0m      812.0622        0.8045\n",
      "      9      \u001b[36m604.6723\u001b[0m      695.4807        0.6718\n",
      "     10      \u001b[36m587.5406\u001b[0m      772.6763        0.6593\n",
      "     11      \u001b[36m565.9637\u001b[0m      \u001b[32m650.8237\u001b[0m     +  0.8105\n",
      "     12      \u001b[36m534.1736\u001b[0m      651.8915        0.6596\n",
      "     13      \u001b[36m523.5350\u001b[0m      \u001b[32m628.8567\u001b[0m     +  0.6594\n",
      "     14      532.5820      735.0428        0.8161\n",
      "     15      543.2466      \u001b[32m609.2127\u001b[0m     +  0.6878\n",
      "     16      524.1438      \u001b[32m549.4313\u001b[0m     +  0.8232\n",
      "     17      \u001b[36m492.4272\u001b[0m      736.3359        0.6779\n",
      "     18      497.5338      962.0048        0.6673\n",
      "     19      \u001b[36m485.7115\u001b[0m      640.6046        0.6785\n",
      "     20      \u001b[36m456.0157\u001b[0m      588.1972        0.8218\n",
      "     21      \u001b[36m437.6749\u001b[0m      614.5947        0.6851\n",
      "     22      \u001b[36m422.5594\u001b[0m      656.4167        0.8925\n",
      "     23      444.2018      656.1519        0.6878\n",
      "     24      424.1540      \u001b[32m483.9900\u001b[0m     +  0.9057\n",
      "     25      \u001b[36m410.9961\u001b[0m      551.8793        0.7172\n",
      "     26      \u001b[36m379.9185\u001b[0m      612.4958        0.8920\n",
      "     27      \u001b[36m369.8201\u001b[0m      494.8046        0.9511\n",
      "     28      381.1207      605.2524        0.9509\n",
      "     29      \u001b[36m368.7922\u001b[0m      578.8306        0.8810\n",
      "     30      \u001b[36m362.5014\u001b[0m      582.4160        0.6875\n",
      "     31      \u001b[36m349.9652\u001b[0m      645.9485        0.8223\n",
      "     32      \u001b[36m336.2161\u001b[0m      591.8562        0.7118\n",
      "     33      \u001b[36m315.3592\u001b[0m      626.7504        0.8256\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Validation loss improved from 505.64009678248 to 483.9900304925068\n",
      "Run 3 of 5\n",
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1     \u001b[36m1333.4475\u001b[0m     \u001b[32m1761.7858\u001b[0m     +  0.6742\n",
      "      2      \u001b[36m970.1812\u001b[0m     \u001b[32m1163.2449\u001b[0m     +  0.8176\n",
      "      3      \u001b[36m768.0061\u001b[0m      \u001b[32m874.0072\u001b[0m     +  0.6604\n",
      "      4      \u001b[36m698.1870\u001b[0m      946.7109        0.8313\n",
      "      5      \u001b[36m660.9007\u001b[0m      \u001b[32m855.6147\u001b[0m     +  0.6730\n",
      "      6      \u001b[36m636.8178\u001b[0m      \u001b[32m744.9539\u001b[0m     +  0.6773\n",
      "      7      637.1497      \u001b[32m583.5131\u001b[0m     +  0.8002\n",
      "      8      \u001b[36m624.1049\u001b[0m      716.0682        0.6675\n",
      "      9      \u001b[36m598.4767\u001b[0m      797.2704        0.6819\n",
      "     10      \u001b[36m591.3616\u001b[0m      681.4579        0.6909\n",
      "     11      \u001b[36m559.9060\u001b[0m      659.9314        0.8324\n",
      "     12      \u001b[36m539.9869\u001b[0m      625.4449        0.6638\n",
      "     13      \u001b[36m538.4349\u001b[0m      615.9603        0.6899\n",
      "     14      \u001b[36m526.9153\u001b[0m      641.0312        0.6851\n",
      "     15      \u001b[36m504.0554\u001b[0m      \u001b[32m524.1221\u001b[0m     +  0.8278\n",
      "     16      529.2690     1010.6663        0.6951\n",
      "     17      519.5098      955.6558        0.7164\n",
      "     18      504.1984      687.7332        0.8289\n",
      "     19      522.3026      631.6827        0.7008\n",
      "     20      509.4809      705.9971        0.8541\n",
      "     21      \u001b[36m490.1623\u001b[0m      559.4046        0.6928\n",
      "     22      \u001b[36m458.5030\u001b[0m      661.9523        0.8537\n",
      "     23      \u001b[36m456.1789\u001b[0m      571.3587        0.6825\n",
      "     24      \u001b[36m445.3499\u001b[0m      797.2724        0.6742\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Validation loss did not improve from 483.9900304925068\n",
      "Run 4 of 5\n",
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1     \u001b[36m1303.4037\u001b[0m     \u001b[32m1717.4290\u001b[0m     +  0.6849\n",
      "      2      \u001b[36m946.1075\u001b[0m     \u001b[32m1165.7858\u001b[0m     +  0.8490\n",
      "      3      \u001b[36m752.1875\u001b[0m      \u001b[32m811.7194\u001b[0m     +  0.6818\n",
      "      4      \u001b[36m708.1038\u001b[0m      904.0305        0.7835\n",
      "      5      \u001b[36m666.9512\u001b[0m     1075.2988        0.9800\n",
      "      6      \u001b[36m646.4507\u001b[0m      \u001b[32m644.1547\u001b[0m     +  0.7955\n",
      "      7      \u001b[36m635.8253\u001b[0m      695.2337        0.8640\n",
      "      8      \u001b[36m603.7684\u001b[0m      662.7381        1.1153\n",
      "      9      \u001b[36m582.0912\u001b[0m      \u001b[32m635.2813\u001b[0m     +  0.7994\n",
      "     10      585.2250      692.4486        0.7651\n",
      "     11      \u001b[36m572.6413\u001b[0m      810.2962        0.7853\n",
      "     12      \u001b[36m551.0053\u001b[0m      637.9228        0.8939\n",
      "     13      558.0556      729.9005        0.7186\n",
      "     14      555.1210      714.1523        0.6896\n",
      "     15      563.9294      \u001b[32m568.3712\u001b[0m     +  0.8321\n",
      "     16      \u001b[36m529.0017\u001b[0m      729.1464        0.7085\n",
      "     17      \u001b[36m504.5713\u001b[0m      665.4081        0.6918\n",
      "     18      \u001b[36m492.3412\u001b[0m      609.7689        0.6861\n",
      "     19      \u001b[36m472.0096\u001b[0m      607.9236        0.6720\n",
      "     20      \u001b[36m443.4322\u001b[0m      605.8637        0.8227\n",
      "     21      \u001b[36m431.3581\u001b[0m      636.5311        0.6914\n",
      "     22      \u001b[36m426.2090\u001b[0m      662.4460        0.8259\n",
      "     23      489.8155      691.7545        0.6997\n",
      "     24      455.6677     1027.8981        0.6719\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Validation loss did not improve from 483.9900304925068\n",
      "Run 5 of 5\n",
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1     \u001b[36m1331.4243\u001b[0m     \u001b[32m2076.2209\u001b[0m     +  0.7093\n",
      "      2      \u001b[36m959.8843\u001b[0m     \u001b[32m1100.7176\u001b[0m     +  0.8732\n",
      "      3      \u001b[36m750.9929\u001b[0m      \u001b[32m981.9900\u001b[0m     +  0.6828\n",
      "      4      \u001b[36m676.4612\u001b[0m      \u001b[32m841.5535\u001b[0m     +  0.7150\n",
      "      5      \u001b[36m661.1767\u001b[0m      857.6829        0.6812\n",
      "      6      670.8389      \u001b[32m718.3477\u001b[0m     +  0.7955\n",
      "      7      665.6995      \u001b[32m553.0618\u001b[0m     +  0.6597\n",
      "      8      \u001b[36m628.8590\u001b[0m      755.9676        0.6801\n",
      "      9      \u001b[36m606.8932\u001b[0m      721.3963        0.7991\n",
      "     10      \u001b[36m571.5475\u001b[0m      678.7169        0.6576\n",
      "     11      \u001b[36m548.1670\u001b[0m      616.8208        0.9099\n",
      "     12      \u001b[36m520.3423\u001b[0m      638.7624        0.6740\n",
      "     13      \u001b[36m502.7992\u001b[0m      631.6034        0.8186\n",
      "     14      \u001b[36m469.7820\u001b[0m      599.2443        0.7436\n",
      "     15      \u001b[36m454.0449\u001b[0m      718.8121        0.6950\n",
      "     16      \u001b[36m431.5217\u001b[0m      600.3303        0.7919\n",
      "     17      450.7142      \u001b[32m478.0604\u001b[0m     +  0.6643\n",
      "     18      432.2159      616.1612        0.8166\n",
      "     19      \u001b[36m408.1504\u001b[0m     1079.8285        0.6709\n",
      "     20      \u001b[36m405.1927\u001b[0m      542.6310        0.8179\n",
      "     21      \u001b[36m385.3164\u001b[0m      534.3229        0.6855\n",
      "     22      \u001b[36m365.4576\u001b[0m      613.1504        0.8185\n",
      "     23      368.7637      642.0369        0.6815\n",
      "     24      \u001b[36m344.5109\u001b[0m      704.0975        0.9164\n",
      "     25      \u001b[36m332.4485\u001b[0m      582.3495        0.6965\n",
      "     26      \u001b[36m327.8177\u001b[0m      734.7444        0.7104\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Validation loss improved from 483.9900304925068 to 478.0603519792263\n",
      "Training done.\n",
      "Fold 3 of 5\n",
      "Training DeepPrime model...\n",
      "Run 1 of 5\n",
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1     \u001b[36m1315.3272\u001b[0m     \u001b[32m1913.9983\u001b[0m     +  0.7642\n",
      "      2      \u001b[36m954.2597\u001b[0m      \u001b[32m973.2840\u001b[0m     +  1.0311\n",
      "      3      \u001b[36m747.9329\u001b[0m     1060.9787        0.7406\n",
      "      4      \u001b[36m680.7633\u001b[0m      \u001b[32m878.2533\u001b[0m     +  0.6977\n",
      "      5      \u001b[36m655.0789\u001b[0m      \u001b[32m663.4831\u001b[0m     +  0.8972\n",
      "      6      \u001b[36m635.3964\u001b[0m      \u001b[32m617.1337\u001b[0m     +  0.7181\n",
      "      7      \u001b[36m625.3049\u001b[0m      634.4749        0.7225\n",
      "      8      \u001b[36m608.7323\u001b[0m      750.9214        0.8189\n",
      "      9      \u001b[36m596.6477\u001b[0m      636.8303        0.6439\n",
      "     10      \u001b[36m571.7434\u001b[0m      \u001b[32m614.3227\u001b[0m     +  0.6623\n",
      "     11      \u001b[36m558.4041\u001b[0m      633.4392        0.7826\n",
      "     12      \u001b[36m539.8014\u001b[0m      630.7240        0.6488\n",
      "     13      \u001b[36m531.7939\u001b[0m      685.9827        0.7844\n",
      "     14      \u001b[36m529.5657\u001b[0m      714.3114        0.6541\n",
      "     15      \u001b[36m520.7896\u001b[0m      642.7744        0.6779\n",
      "     16      524.5272      \u001b[32m588.0510\u001b[0m     +  0.6656\n",
      "     17      \u001b[36m483.0006\u001b[0m      590.9303        0.7855\n",
      "     18      \u001b[36m456.9851\u001b[0m      662.0923        0.6518\n",
      "     19      \u001b[36m410.4248\u001b[0m      \u001b[32m499.6419\u001b[0m     +  0.7938\n",
      "     20      \u001b[36m390.3194\u001b[0m      \u001b[32m473.2567\u001b[0m     +  0.6678\n",
      "     21      \u001b[36m370.4671\u001b[0m      556.1858        0.6657\n",
      "     22      \u001b[36m356.5347\u001b[0m      693.8852        0.7924\n",
      "     23      \u001b[36m355.9373\u001b[0m      546.3125        0.6850\n",
      "     24      \u001b[36m345.3863\u001b[0m      650.4957        0.6731\n",
      "     25      \u001b[36m334.6841\u001b[0m      568.5768        0.7832\n",
      "     26      \u001b[36m315.4569\u001b[0m      493.6428        0.6599\n",
      "     27      \u001b[36m288.7613\u001b[0m      605.1414        0.6796\n",
      "     28      \u001b[36m264.0698\u001b[0m      605.3810        0.7974\n",
      "     29      \u001b[36m247.8361\u001b[0m      663.6208        0.6719\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Validation loss improved from inf to 473.2567259260098\n",
      "Run 2 of 5\n",
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1     \u001b[36m1314.9793\u001b[0m     \u001b[32m1841.2180\u001b[0m     +  0.8294\n",
      "      2      \u001b[36m953.6493\u001b[0m     \u001b[32m1251.7894\u001b[0m     +  0.6833\n",
      "      3      \u001b[36m755.9786\u001b[0m     \u001b[32m1001.2673\u001b[0m     +  0.6868\n",
      "      4      \u001b[36m697.8495\u001b[0m      \u001b[32m798.5736\u001b[0m     +  0.8607\n",
      "      5      \u001b[36m658.9480\u001b[0m      897.4998        0.7079\n",
      "      6      685.3419      \u001b[32m594.2009\u001b[0m     +  0.7160\n",
      "      7      \u001b[36m649.3112\u001b[0m      679.7218        0.8304\n",
      "      8      \u001b[36m609.7305\u001b[0m      791.1704        0.6785\n",
      "      9      \u001b[36m599.5857\u001b[0m      604.5515        0.6779\n",
      "     10      \u001b[36m583.0872\u001b[0m      608.6378        0.8047\n",
      "     11      \u001b[36m560.2252\u001b[0m      629.8415        0.6739\n",
      "     12      \u001b[36m547.7230\u001b[0m      661.0167        0.6733\n",
      "     13      \u001b[36m544.3276\u001b[0m      \u001b[32m590.8742\u001b[0m     +  0.7995\n",
      "     14      \u001b[36m535.9364\u001b[0m      951.1940        0.6648\n",
      "     15      542.0053      599.0744        0.6661\n",
      "     16      \u001b[36m529.0806\u001b[0m      652.0554        0.8024\n",
      "     17      \u001b[36m488.7478\u001b[0m      \u001b[32m590.0125\u001b[0m     +  0.6740\n",
      "     18      \u001b[36m474.6453\u001b[0m      662.2652        0.6962\n",
      "     19      495.8926      662.8856        0.7907\n",
      "     20      522.3364      764.9858        0.6684\n",
      "     21      508.5488      959.0925        0.7932\n",
      "     22      478.7445      798.9342        0.6972\n",
      "     23      \u001b[36m471.1980\u001b[0m      \u001b[32m571.0726\u001b[0m     +  0.6761\n",
      "     24      \u001b[36m427.5289\u001b[0m      766.0739        0.9254\n",
      "     25      \u001b[36m407.1561\u001b[0m      593.9055        0.7312\n",
      "     26      \u001b[36m399.5648\u001b[0m      615.8358        0.7135\n",
      "     27      \u001b[36m385.3003\u001b[0m      654.7026        0.6833\n",
      "     28      \u001b[36m373.1260\u001b[0m      712.0036        0.7837\n",
      "     29      \u001b[36m371.7212\u001b[0m      711.8344        0.6520\n",
      "     30      \u001b[36m366.2479\u001b[0m      \u001b[32m563.4946\u001b[0m     +  0.7663\n",
      "     31      369.4699      580.1318        0.6468\n",
      "     32      368.5736      660.0525        0.6550\n",
      "     33      \u001b[36m364.7568\u001b[0m      607.9425        0.8078\n",
      "     34      367.3303      653.9661        0.6560\n",
      "     35      \u001b[36m341.7498\u001b[0m      744.7005        0.6777\n",
      "     36      \u001b[36m334.1382\u001b[0m      653.6529        0.7959\n",
      "     37      \u001b[36m318.4854\u001b[0m      581.4663        0.6596\n",
      "     38      \u001b[36m304.3226\u001b[0m      728.1946        0.6672\n",
      "     39      \u001b[36m288.3787\u001b[0m      707.3846        0.8006\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Validation loss did not improve from 473.2567259260098\n",
      "Run 3 of 5\n",
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1     \u001b[36m1312.1674\u001b[0m     \u001b[32m1810.6053\u001b[0m     +  0.7066\n",
      "      2      \u001b[36m951.7120\u001b[0m     \u001b[32m1122.2882\u001b[0m     +  0.8352\n",
      "      3      \u001b[36m747.7852\u001b[0m      \u001b[32m861.7288\u001b[0m     +  0.6685\n",
      "      4      \u001b[36m682.7345\u001b[0m      885.2546        0.6713\n",
      "      5      \u001b[36m656.4349\u001b[0m      \u001b[32m816.9055\u001b[0m     +  0.8478\n",
      "      6      \u001b[36m650.7247\u001b[0m      \u001b[32m644.5046\u001b[0m     +  0.7113\n",
      "      7      664.9175      712.5953        0.7117\n",
      "      8      \u001b[36m617.8867\u001b[0m      786.2039        0.8042\n",
      "      9      \u001b[36m603.3867\u001b[0m      \u001b[32m596.8098\u001b[0m     +  0.6863\n",
      "     10      \u001b[36m582.8984\u001b[0m      668.8062        0.6734\n",
      "     11      \u001b[36m559.0375\u001b[0m      697.8937        0.8212\n",
      "     12      \u001b[36m551.4817\u001b[0m      \u001b[32m581.0376\u001b[0m     +  0.6929\n",
      "     13      \u001b[36m549.8332\u001b[0m      601.1259        0.7113\n",
      "     14      \u001b[36m528.0574\u001b[0m      701.5008        0.6872\n",
      "     15      \u001b[36m504.0213\u001b[0m      659.4275        0.8565\n",
      "     16      \u001b[36m501.2767\u001b[0m      \u001b[32m553.3802\u001b[0m     +  0.7384\n",
      "     17      517.4331      667.6379        0.6874\n",
      "     18      \u001b[36m500.6516\u001b[0m      743.3820        0.8796\n",
      "     19      \u001b[36m496.5951\u001b[0m      570.5462        0.7047\n",
      "     20      \u001b[36m491.0392\u001b[0m      663.9733        0.7543\n",
      "     21      \u001b[36m449.2794\u001b[0m      668.1687        1.0374\n",
      "     22      \u001b[36m436.0072\u001b[0m      815.8816        0.7027\n",
      "     23      440.0329      682.7194        0.7057\n",
      "     24      502.2323      898.1596        0.8349\n",
      "     25      494.4834      660.2395        0.6876\n",
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Validation loss did not improve from 473.2567259260098\n",
      "Run 4 of 5\n",
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1     \u001b[36m1317.9917\u001b[0m     \u001b[32m1935.0103\u001b[0m     +  0.8113\n",
      "      2      \u001b[36m940.4950\u001b[0m     \u001b[32m1073.4125\u001b[0m     +  0.6586\n",
      "      3      \u001b[36m757.8332\u001b[0m      \u001b[32m818.7295\u001b[0m     +  0.6679\n",
      "      4      \u001b[36m698.9198\u001b[0m      965.9536        0.8185\n",
      "      5      \u001b[36m647.8673\u001b[0m      \u001b[32m794.7049\u001b[0m     +  0.6639\n",
      "      6      \u001b[36m643.1518\u001b[0m      \u001b[32m615.7154\u001b[0m     +  0.7909\n",
      "      7      645.7338      619.0785        0.7073\n",
      "      8      \u001b[36m615.6474\u001b[0m      849.0470        0.6773\n",
      "      9      \u001b[36m582.0973\u001b[0m      618.0938        0.8226\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Training DeepPrime\n",
    "'''\n",
    "from os.path import join as pjoin, basename\n",
    "\n",
    "from models.deepprime import DeepPrime, preprocess_deep_prime, train_deep_prime\n",
    "\n",
    "fname = pjoin('dp-pd-hek293t-pe2.csv')\n",
    "train_deep_prime(fname, hidden_size=128, num_features=24, num_layers=1, dropout=0.05, epochs=500, batch_size=1024, lr=0.005, patience=10, device='cuda', num_runs=5, adjustment='none')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Testing DeepPrime on the clinvar dataset\n",
    "'''\n",
    "from os.path import join as pjoin, basename\n",
    "\n",
    "from models.deepprime import DeepPrime, preprocess_deep_prime, train_deep_prime, predict_deep_prime\n",
    "\n",
    "data = 'dp-pd-hek293t-pe2.csv'\n",
    "predict_deep_prime(data, hidden_size=128, num_layers=1, dropout=0.05, num_features=24, adjustment='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test log and undersample adjustments on deep prime\n",
    "from models.deepprime import DeepPrime, preprocess_deep_prime, train_deep_prime, predict_deep_prime\n",
    "import pandas as pd\n",
    "\n",
    "datas = ['dp-dp-hek293t-pe2.csv', 'dp-pd-hek293t-pe2.csv', 'dp-pd-adv-pe2.csv', 'dp-pd-k562-pe2.csv', 'dp-pd-k562mlh1d-pe2.csv']\n",
    "\n",
    "for data in datas:        \n",
    "    print('Training Log')\n",
    "    deepprime_log = train_deep_prime(data, hidden_size=128, num_features=24, num_layers=1, dropout=0.05, epochs=500, batch_size=2048, lr=0.005, patience=20, device='cuda', adjustment='log')\n",
    "    \n",
    "    print('Training Undersample')\n",
    "    deepprime_undersample = train_deep_prime(data, hidden_size=128, num_features=24, num_layers=1, dropout=0.05, epochs=500, batch_size=2048, lr=0.005, patience=20, device='cuda', adjustment='undersample')\n",
    "    \n",
    "    print('Training Org')\n",
    "    deepprime_org = train_deep_prime(data, hidden_size=128, num_features=24, num_layers=1, dropout=0.05, epochs=500, batch_size=2048, lr=0.005, patience=20, device='cuda')\n",
    "        \n",
    "    print('-----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test log and undersample adjustments on deep prime\n",
    "from models.deepprime import DeepPrime, preprocess_deep_prime, train_deep_prime, predict_deep_prime\n",
    "from os.path import join as pjoin\n",
    "import pandas as pd\n",
    "\n",
    "datas = ['dp-pd-hek293t-pe2.csv', 'dp-pd-adv-pe2.csv', 'dp-pd-k562-pe2.csv', 'dp-pd-k562mlh1d-pe2.csv', 'dp-dp-hek293t-pe2.csv']\n",
    "\n",
    "performance_pearson = {}\n",
    "performance_spearman = {}\n",
    "\n",
    "\n",
    "for data in datas:\n",
    "    print('-----------------------------------')\n",
    "    print('Log adjustment')\n",
    "    # log\n",
    "    deepprime_log_pred, deepprime_log_performance = predict_deep_prime(data, hidden_size=128, num_layers=1, dropout=0.05, num_features=24, adjustment='log')\n",
    "    \n",
    "    print('-----------------------------------')\n",
    "    print('Undersample adjustment')\n",
    "    # undersample\n",
    "    deepprime_undersample_pred, deepprime_undersample_performance = predict_deep_prime(data, hidden_size=128, num_layers=1, dropout=0.05, num_features=24, adjustment='undersample')\n",
    "    \n",
    "    print('-----------------------------------')\n",
    "    print('Original')\n",
    "    # original\n",
    "    deepprime_org_pred, deepprime_og_performance = predict_deep_prime(data, hidden_size=128, num_layers=1, dropout=0.05, num_features=24)\n",
    "    \n",
    "    # save the predictions\n",
    "    performance_pearson[data.split('.')[0]] = {\n",
    "        'orginal': [x[0] for x in deepprime_og_performance],\n",
    "        'log': [x[0] for x in deepprime_log_performance],\n",
    "        'undersample': [x[0] for x in deepprime_undersample_performance]\n",
    "    }\n",
    "    \n",
    "    performance_spearman[data.split('.')[0]] = {\n",
    "        'orginal': [x[1] for x in deepprime_og_performance],\n",
    "        'log': [x[1] for x in deepprime_log_performance],\n",
    "        'undersample': [x[1] for x in deepprime_undersample_performance]\n",
    "    }\n",
    "    \n",
    "# save the performance\n",
    "performance_pearson_df = pd.DataFrame(performance_pearson)\n",
    "performance_spearman_df = pd.DataFrame(performance_spearman)\n",
    "\n",
    "# invert the x and y axis\n",
    "performance_pearson_df = performance_pearson_df.T\n",
    "performance_spearman_df = performance_spearman_df.T\n",
    "\n",
    "performance_pearson_df.to_csv(pjoin('models', 'data', 'performance', 'deep-prime-performance-pearson.csv'))\n",
    "performance_spearman_df.to_csv(pjoin('models', 'data', 'performance', 'deep-prime-performance-spearman.csv'))\n",
    "\n",
    "print('Performance saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot both the pearson and spearman correlation performance\n",
    "# each figure contains the performance of the model on the different datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import join as pjoin\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ast\n",
    "import scipy\n",
    "\n",
    "performance_pearson = pd.read_csv(pjoin('models', 'data', 'performance', 'deep-prime-performance-pearson.csv'), index_col=0)\n",
    "performance_spearman = pd.read_csv(pjoin('models', 'data', 'performance', 'deep-prime-performance-spearman.csv'), index_col=0)\n",
    "f_size = 14\n",
    "\n",
    "adjustment_shorthands = {\n",
    "    'weighted-mse': '   WMSE',\n",
    "    'quantile-transform': '     QT',\n",
    "    'original': '     OG',\n",
    "    'log': '     LA',\n",
    "    'undersample': '     US'\n",
    "}\n",
    "\n",
    "# convert all list strings to list\n",
    "import ast\n",
    "\n",
    "performance_pearson = performance_pearson.map(lambda x: ast.literal_eval(x))\n",
    "performance_spearman = performance_spearman.map(lambda x: ast.literal_eval(x))\n",
    "\n",
    "for i, data in enumerate(['dp-dp-hek293t-pe2', 'dp-pd-hek293t-pe2', 'dp-pd-k562-pe2', 'dp-pd-k562mlh1d-pe2', 'dp-pd-adv-pe2']):\n",
    "    # plot the performance of the model on the different datasets\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 1.8), width_ratios=(1.2, 1))\n",
    "    cell_data_pearson = performance_pearson.loc[data]\n",
    "    # into a dataframe of two columns, adjustment and performance\n",
    "    cell_data_pearson = pd.DataFrame(np.array(cell_data_pearson.tolist()).T, columns=['original', 'log', 'undersample'])\n",
    "    \n",
    "    cell_data_spearman = performance_spearman.loc[data]\n",
    "    cell_data_spearman = pd.DataFrame(np.array(cell_data_spearman.tolist()).T, columns=['original', 'log', 'undersample'])\n",
    "    \n",
    "    for ind, performance in enumerate([performance_pearson, performance_spearman]):\n",
    "        # plot the mean of each adjustment as barplot\n",
    "        # plot stripplot of the performance of each fold on top\n",
    "        # of the barplot\n",
    "        cell_data = cell_data_pearson if ind == 0 else cell_data_spearman\n",
    "        sns.barplot(data=cell_data, ax=ax[ind], palette=iter(sns.color_palette('icefire', 3)),alpha=0.5, orient='h', errorbar=None)\n",
    "        sns.stripplot(data=cell_data, ax=ax[ind], palette=iter(sns.color_palette('icefire', 3)), size=5, jitter=True, orient='h')\n",
    "        # ax[ind].set_title('Pearson' if ind == 0 else 'Spearman')\n",
    "        metric = 'Pearson' if ind == 0 else 'Spearman'\n",
    "        ax[ind].set_xlabel(f'{metric}', fontsize=f_size)\n",
    "        # ax[ind].set_ylabel('Adjustment')\n",
    "        \n",
    "        # fix y axis to 0 to 1\n",
    "        ax[ind].set_xlim(0, 1)\n",
    "        \n",
    "        # remove top and right spines\n",
    "        ax[ind].spines['top'].set_visible(False)\n",
    "        if ind == 0:\n",
    "            ax[ind].spines['left'].set_visible(False)\n",
    "        else:\n",
    "            ax[ind].spines['right'].set_visible(False)\n",
    "            \n",
    "        # test the significance between each adjustment with the original\n",
    "        # using the paired t-test\n",
    "        original = cell_data['original']\n",
    "        for adj in cell_data.columns:\n",
    "            if adj != 'original':\n",
    "                if np.mean(cell_data[adj]) < np.mean(cell_data['original']):\n",
    "                    t_stat, p_val = scipy.stats.ttest_rel(cell_data[adj], original)\n",
    "                    if p_val < 0.05:\n",
    "                        print(f'{data} {metric} {adjustment_shorthands[adj]}: {p_val}')\n",
    "    \n",
    "    ax[0].set_yticks(range(len(cell_data.columns)))\n",
    "    ax[0].set_yticklabels([adjustment_shorthands[adj] for adj in cell_data.columns])\n",
    "    # ax 1 should have no y labels\n",
    "    ax[1].set_yticklabels([])\n",
    "    # flip the x axis of ax 0\n",
    "    ax[0].invert_xaxis()\n",
    "    # move the y axis of ax 0 to the right\n",
    "    ax[0].yaxis.tick_right()\n",
    "    \n",
    "    # change the font of x tick labels\n",
    "    ax[0].tick_params(axis='both', which='major', labelsize=f_size)\n",
    "    ax[1].tick_params(axis='both', which='major', labelsize=f_size)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # save the figure\n",
    "    data_name = '-'.join(data.split('-')[1:3])\n",
    "    plt.savefig(pjoin('dissertation/figures', f'adjustment-deepprime-{data_name}-performance.png'), dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRIDICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 of 5\n",
      "Index(['wt-sequence', 'mut-sequence', 'edit-length', 'pbs-length',\n",
      "       'rtt-length', 'rha-length', 'mut-type', 'edit-melting-temperature',\n",
      "       'extension-melting-temperature', 'preedit-melting-temperature',\n",
      "       'protospacer-melting-temperature', 'rha-melting-temperature',\n",
      "       'pbs-melting-temperature', 'edit-sequence-zero-length',\n",
      "       'preedit-sequence-zero-length', 'protospacer-location',\n",
      "       'pbs-location-l-relative-protospacer',\n",
      "       'rtt-location-l-relative-protospacer',\n",
      "       'rha-location-l-relative-protospacer', 'extension-minimum-free-energy',\n",
      "       'extension-scaffold-minimum-free-energy', 'pbs-minimum-free-energy',\n",
      "       'spacer-minimum-free-energy',\n",
      "       'spacer-extension-scaffold-minimum-free-energy',\n",
      "       'spacer-scaffold-minimum-free-energy', 'rtt-minimum-free-energy'],\n",
      "      dtype='object')\n",
      "Training PRIDICT model...\n",
      "Run 1 of 5\n",
      "Re-initializing module.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1     \u001b[36m1714.9425\u001b[0m     \u001b[32m2349.1626\u001b[0m     +  2.5679\n",
      "      2     \u001b[36m1680.0353\u001b[0m     \u001b[32m2314.7852\u001b[0m     +  2.2713\n",
      "      3     \u001b[36m1649.6738\u001b[0m     \u001b[32m2268.5373\u001b[0m     +  2.0835\n",
      "      4     \u001b[36m1608.2659\u001b[0m     \u001b[32m2201.2517\u001b[0m     +  2.2363\n",
      "      5     \u001b[36m1546.5171\u001b[0m     \u001b[32m2101.6200\u001b[0m     +  2.2623\n",
      "      6     \u001b[36m1457.8163\u001b[0m     \u001b[32m1950.0536\u001b[0m     +  2.4538\n",
      "      7     \u001b[36m1331.2536\u001b[0m     \u001b[32m1726.5972\u001b[0m     +  2.5006\n",
      "      8     \u001b[36m1170.8990\u001b[0m     \u001b[32m1436.5555\u001b[0m     +  2.4790\n",
      "      9     \u001b[36m1089.9839\u001b[0m     \u001b[32m1206.4248\u001b[0m     +  2.2607\n",
      "     10     1136.1997     \u001b[32m1178.3910\u001b[0m     +  2.4881\n",
      "     11     1124.3688     1220.8319        2.3089\n",
      "     12     \u001b[36m1085.3031\u001b[0m     1229.5252        2.2224\n",
      "     13     \u001b[36m1082.1187\u001b[0m     1205.7909        2.4757\n",
      "     14     \u001b[36m1075.6358\u001b[0m     1187.2917        2.2997\n",
      "     15     \u001b[36m1029.5667\u001b[0m     \u001b[32m1102.1120\u001b[0m     +  2.6317\n",
      "     16     \u001b[36m1020.1550\u001b[0m     1233.1302        2.4853\n",
      "     17      \u001b[36m948.5768\u001b[0m     \u001b[32m1089.2777\u001b[0m     +  2.4118\n",
      "     18     1016.8901     \u001b[32m1033.0658\u001b[0m     +  2.4076\n",
      "     19      990.7676     1251.7992        2.3151\n",
      "     20      \u001b[36m930.6292\u001b[0m     1074.6188        2.6047\n",
      "     21      \u001b[36m924.3526\u001b[0m     1160.1829        2.3093\n",
      "     22      \u001b[36m876.8351\u001b[0m     1067.2845        2.2924\n",
      "     23      \u001b[36m858.9100\u001b[0m     1059.8323        2.4152\n",
      "     24      867.2171     1118.8731        2.3007\n",
      "     25      \u001b[36m799.9321\u001b[0m     \u001b[32m1002.5941\u001b[0m     +  2.3168\n",
      "     26      \u001b[36m798.4234\u001b[0m      \u001b[32m989.4816\u001b[0m     +  2.3274\n",
      "     27      \u001b[36m770.8707\u001b[0m      \u001b[32m976.9850\u001b[0m     +  2.3225\n",
      "     28      \u001b[36m745.7605\u001b[0m      \u001b[32m892.7445\u001b[0m     +  2.3342\n",
      "     29      \u001b[36m719.0604\u001b[0m      898.5721        2.3067\n",
      "     30      \u001b[36m712.1229\u001b[0m      \u001b[32m859.2781\u001b[0m     +  2.4528\n",
      "     31      \u001b[36m688.0560\u001b[0m      \u001b[32m804.6249\u001b[0m     +  2.4039\n",
      "     32      694.3907      \u001b[32m775.6866\u001b[0m     +  2.5684\n",
      "     33      \u001b[36m676.5427\u001b[0m      793.4893        2.1593\n",
      "     34      \u001b[36m650.1056\u001b[0m      \u001b[32m756.6836\u001b[0m     +  2.3880\n",
      "     35      672.6634      770.9681        2.4257\n",
      "     36      655.9601      \u001b[32m730.0895\u001b[0m     +  2.1726\n",
      "     37      \u001b[36m645.1208\u001b[0m      778.9072        2.4100\n",
      "     38      \u001b[36m637.5033\u001b[0m      732.6999        2.3031\n",
      "     39      649.7152      807.5654        2.5157\n",
      "     40      645.8102      \u001b[32m715.2781\u001b[0m     +  2.6700\n",
      "     41      667.8856      782.3809        2.4115\n",
      "     42      665.5712      764.7925        2.5209\n",
      "     43      646.8574      757.5936        2.3789\n",
      "     44      \u001b[36m628.0561\u001b[0m      721.9843        2.2752\n",
      "     45      652.8635      836.6019        2.4476\n",
      "     46      \u001b[36m624.4235\u001b[0m      758.8673        2.4428\n",
      "     47      827.4557      \u001b[32m705.3316\u001b[0m     +  2.3505\n",
      "     48      765.6542      992.3185        2.4344\n",
      "     49      816.7229     1032.5847        2.3268\n",
      "     50      763.7850     1044.6729        2.3535\n",
      "     51      699.2713      880.7496        2.3372\n",
      "     52      646.7546      938.1164        2.3549\n",
      "     53      628.9184      791.5198        2.3598\n",
      "     54      625.7360      826.9330        2.3847\n",
      "     55      \u001b[36m603.6508\u001b[0m      739.9768        2.3888\n",
      "     56      \u001b[36m602.0505\u001b[0m      775.7797        2.1067\n",
      "     57      \u001b[36m592.8128\u001b[0m      732.4716        2.4522\n",
      "     58      619.3535      737.5834        2.2698\n",
      "     59      \u001b[36m592.4828\u001b[0m      763.3191        2.2310\n",
      "     60      \u001b[36m568.9713\u001b[0m      727.3442        2.3526\n",
      "     61      603.6336      776.4828        2.3199\n",
      "     62      595.5360      752.1908        2.3074\n",
      "     63      602.1057      707.5106        2.2943\n",
      "     64      579.6976      835.5847        2.2141\n",
      "     65      574.3400      719.4219        2.4517\n",
      "     66      624.4692      837.8034        2.4692\n",
      "Stopping since valid_loss has not improved in the last 20 epochs.\n",
      "Best validation loss: 705.3315935813692\n",
      "Run 2 of 5\n",
      "Re-initializing module.\n",
      "Re-initializing criterion.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss    cp     dur\n",
      "-------  ------------  ------------  ----  ------\n",
      "      1     \u001b[36m1767.7480\u001b[0m     \u001b[32m2444.7635\u001b[0m     +  2.5879\n",
      "      2     \u001b[36m1754.4219\u001b[0m     \u001b[32m2426.9552\u001b[0m     +  2.3237\n",
      "      3     \u001b[36m1740.6084\u001b[0m     \u001b[32m2405.2692\u001b[0m     +  2.3572\n",
      "      4     \u001b[36m1722.2016\u001b[0m     \u001b[32m2376.0870\u001b[0m     +  2.4380\n",
      "      5     \u001b[36m1696.6541\u001b[0m     \u001b[32m2335.1493\u001b[0m     +  2.4982\n",
      "      6     \u001b[36m1660.2569\u001b[0m     \u001b[32m2274.2311\u001b[0m     +  2.3681\n",
      "      7     \u001b[36m1604.7744\u001b[0m     \u001b[32m2179.1141\u001b[0m     +  2.5323\n",
      "      8     \u001b[36m1512.9133\u001b[0m     \u001b[32m2025.6462\u001b[0m     +  2.2410\n",
      "      9     \u001b[36m1374.0670\u001b[0m     \u001b[32m1781.6431\u001b[0m     +  2.2981\n",
      "     10     \u001b[36m1198.6596\u001b[0m     \u001b[32m1445.0838\u001b[0m     +  2.2058\n",
      "     11     \u001b[36m1103.2331\u001b[0m     \u001b[32m1203.1175\u001b[0m     +  2.3554\n",
      "     12     1175.7510     1212.1419        2.3330\n",
      "     13     1152.1319     1278.8948        2.2101\n",
      "     14     1118.6246     1284.2367        2.3448\n",
      "     15     1111.6789     1267.8657        2.5019\n",
      "     16     1124.8137     1259.1582        2.1834\n",
      "     17     1117.4969     1263.4975        2.4630\n",
      "     18     \u001b[36m1094.9813\u001b[0m     1253.0637        2.4925\n",
      "     19     1115.7849     1241.9755        2.4143\n",
      "     20     1118.9130     1249.4507        2.2325\n",
      "     21     1103.1045     1258.2246        2.4790\n",
      "     22     \u001b[36m1077.0905\u001b[0m     1216.7031        2.3953\n",
      "     23     \u001b[36m1053.2118\u001b[0m     \u001b[32m1199.0338\u001b[0m     +  2.4466\n",
      "     24     \u001b[36m1034.9288\u001b[0m     \u001b[32m1157.4555\u001b[0m     +  2.6470\n",
      "     25     \u001b[36m1020.8106\u001b[0m     1178.9007        2.4626\n",
      "     26      \u001b[36m992.1178\u001b[0m     1235.4085        2.1648\n",
      "     27      \u001b[36m935.3268\u001b[0m     1220.8936        2.3266\n",
      "     28      \u001b[36m930.8608\u001b[0m     \u001b[32m1118.1125\u001b[0m     +  2.5062\n",
      "     29      949.4574     \u001b[32m1114.9517\u001b[0m     +  2.1045\n",
      "     30      \u001b[36m842.7302\u001b[0m     \u001b[32m1081.3209\u001b[0m     +  2.4025\n",
      "     31      884.1731     \u001b[32m1065.9189\u001b[0m     +  2.2777\n",
      "     32      \u001b[36m834.8704\u001b[0m     1174.5045        2.6750\n",
      "     33      \u001b[36m826.9065\u001b[0m      \u001b[32m970.0119\u001b[0m     +  2.4830\n",
      "     34      \u001b[36m822.8979\u001b[0m     1029.2322        2.8111\n",
      "     35      \u001b[36m805.7365\u001b[0m     1035.4382        2.6026\n",
      "     36      824.2142      \u001b[32m883.7961\u001b[0m     +  2.2912\n",
      "     37      829.7957     1065.9967        2.4289\n",
      "     38      813.1190      939.1014        2.5570\n",
      "     39      \u001b[36m796.7865\u001b[0m     1062.8087        2.4750\n",
      "     40      \u001b[36m781.8413\u001b[0m      971.1207        2.6115\n",
      "     41      805.7438     1065.4472        2.4282\n",
      "     42      808.5227      908.1671        2.9000\n",
      "     43      816.3241     1047.0301        2.3130\n",
      "     44      784.0806      \u001b[32m870.5587\u001b[0m     +  2.6000\n"
     ]
    }
   ],
   "source": [
    "from models.pridict import train_pridict\n",
    "\n",
    "train_pridict('pd-pd-hek293t-pe2.csv', lr=0.005, batch_size=2048, epochs=200, patience=20, num_runs=5, adjustment='none', num_features=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.pridict import predict_pridict\n",
    "\n",
    "predict_pridict('pd-pd-hek293t-pe2.csv', num_features=24, device='cuda', adjustment='none')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
