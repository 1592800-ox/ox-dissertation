{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join as pjoin\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import collections\n",
    "from utils.stats_utils import get_pearson_and_spearman_correlation\n",
    "from utils.data_utils import k_fold_cross_validation_split\n",
    "from models.conventional_ml_models import lasso_regression, ridge_regression, mlp, xgboost, random_forest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "import skorch\n",
    "from models.conventional_ml_models import MLP\n",
    "import os\n",
    "\n",
    "datas = ['ml-dp-hek293t-pe2.csv', 'ml-pd-hek293t-pe2.csv', 'ml-pd-adv-pe2.csv', 'ml-pd-k562-pe2.csv', 'ml-pd-k562mlh1d-pe2.csv']\n",
    "\n",
    "performance = {}\n",
    "\n",
    "def undersample(features, target):\n",
    "    # sample 10% of the training data with editing efficiency of less than 10\n",
    "    indices = np.where(target < 10)[0]\n",
    "    seed = 42\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.choice(indices, int(len(indices) * 0.1), replace=False)\n",
    "    features = np.delete(features, indices, axis=0)\n",
    "    target = np.delete(target, indices, axis=0)\n",
    "    return features, target\n",
    "\n",
    "for data in datas:\n",
    "    dataset = pd.read_csv(pjoin('models', 'data', 'conventional-ml', data))\n",
    "    cell_line = '-'.join(data.split('-')[1:3]).split('.')[0]\n",
    "    data_source = '-'.join(data.split('-')[1:]).split('.')[0]\n",
    "\n",
    "    # 5 fold cross validation\n",
    "    fold = 5\n",
    "\n",
    "    features = dataset.iloc[:, :24].values\n",
    "    target = dataset.iloc[:, -2].values\n",
    "\n",
    "    # standardize the features\n",
    "    # scaler = StandardScaler()\n",
    "    # features = scaler.fit_transform(features)\n",
    "\n",
    "    correlations = collections.defaultdict(list)\n",
    "    \n",
    "    for i in range(0, fold):\n",
    "        print(f'Fold {i+1} of {fold}')\n",
    "        train_features = features[dataset['fold'] != i]\n",
    "        train_target = target[dataset['fold'] != i]\n",
    "        \n",
    "        # undersample the training data\n",
    "        train_features, train_target = undersample(train_features, train_target)\n",
    "        \n",
    "        # ============================\n",
    "        # Lasso\n",
    "        # ============================\n",
    "        print('Training Lasso')\n",
    "        lasso_model = lasso_regression(train_features, train_target)\n",
    "        lasso_model.fit(train_features, train_target)\n",
    "        # pickle the model\n",
    "        pickle.dump(lasso_model, open(pjoin('models', 'trained-models', 'conventional-ml', f'lasso-{data_source}-fold-{i+1}.pkl'), 'wb'))\n",
    "\n",
    "        # ============================\n",
    "        # Ridge\n",
    "        # ============================\n",
    "        print('Training Ridge')\n",
    "        ridge_model = ridge_regression(train_features, train_target)\n",
    "        ridge_model.fit(train_features, train_target)\n",
    "        # pickle the model\n",
    "        pickle.dump(ridge_model, open(pjoin('models', 'trained-models', 'conventional-ml', f'ridge-{data_source}-fold-{i+1}.pkl'), 'wb'))\n",
    "\n",
    "        # ============================\n",
    "        # xgboost\n",
    "        # ============================\n",
    "        print('Training XGBoost')\n",
    "        xgboost_model = xgboost(train_features, train_target)\n",
    "        xgboost_model.fit(train_features, train_target)\n",
    "        # pickle the model\n",
    "        pickle.dump(xgboost_model, open(pjoin('models', 'trained-models', 'conventional-ml', f'xgboost-{data_source}-fold-{i+1}.pkl'), 'wb'))\n",
    "\n",
    "        # ============================\n",
    "        # random forest\n",
    "        # ============================\n",
    "        print('Training Random Forest')\n",
    "        rf_model = random_forest(train_features, train_target)\n",
    "        rf_model.fit(train_features, train_target)\n",
    "        # pickle the model\n",
    "        pickle.dump(rf_model, open(pjoin('models', 'trained-models', 'conventional-ml', f'random_forest-{data_source}-fold-{i+1}.pkl'), 'wb'))\n",
    "        \n",
    "        # ============================\n",
    "        # MLP\n",
    "        # ============================\n",
    "        \n",
    "        print('Training MLP')\n",
    "        mlp_model = skorch.NeuralNetRegressor(\n",
    "            module=MLP,\n",
    "            criterion=torch.nn.MSELoss,\n",
    "            optimizer=torch.optim.Adam,\n",
    "            max_epochs=200,\n",
    "            module__activation='relu',\n",
    "            lr=0.005,\n",
    "            device='cuda',\n",
    "            batch_size=2048,\n",
    "            train_split=skorch.dataset.ValidSplit(cv=5),\n",
    "            module__hidden_layer_sizes = (64, 64,),\n",
    "            # early stopping\n",
    "            callbacks=[\n",
    "                skorch.callbacks.EarlyStopping(patience=30),\n",
    "                skorch.callbacks.Checkpoint(monitor='valid_loss_best', f_pickle=None, f_criterion=None, f_optimizer=os.path.join('models', 'trained-models', 'conventional-ml', f'mlp-{data_source}-fold-{i+1}-optimizer.pkl'), f_history=os.path.join('models', 'trained-models', 'conventional-ml', f'mlp-{data_source}-fold-{i+1}-history.json'), f_params=os.path.join('models', 'trained-models', 'conventional-ml', f'mlp-{data_source}-fold-{i+1}.pkl'), event_name='event_cp'),\n",
    "                skorch.callbacks.LRScheduler(policy=torch.optim.lr_scheduler.CosineAnnealingWarmRestarts, monitor='valid_loss', T_0=20, T_mult=1),\n",
    "            ]\n",
    "        )\n",
    "        # convert features and target to float32\n",
    "        train_features = train_features.astype(np.float32)\n",
    "        train_target = train_target.astype(np.float32)\n",
    "        # add a dimension to the target\n",
    "        train_target = train_target[:, np.newaxis]\n",
    "        mlp_model.fit(train_features, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pickled models and evaluate on the corresponding validation set\n",
    "from os.path import join as pjoin\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import collections\n",
    "from utils.stats_utils import get_pearson_and_spearman_correlation\n",
    "from utils.data_utils import k_fold_cross_validation_split\n",
    "from models.conventional_ml_models import lasso_regression, ridge_regression, mlp, xgboost, random_forest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy\n",
    "import pickle\n",
    "import torch\n",
    "import skorch\n",
    "\n",
    "datas = ['ml-dp-hek293t-pe2.csv', 'ml-pd-hek293t-pe2.csv', 'ml-pd-adv-pe2.csv', 'ml-pd-k562-pe2.csv', 'ml-pd-k562mlh1d-pe2.csv']\n",
    "\n",
    "performance = {}\n",
    "\n",
    "for data in datas:\n",
    "    dataset = pd.read_csv(pjoin('models', 'data', 'conventional-ml', data))\n",
    "    cell_line = '-'.join(data.split('-')[1:3]).split('.')[0]\n",
    "    \n",
    "    data_source = '-'.join(data.split('-')[1:]).split('.')[0]\n",
    "\n",
    "    # 5 fold cross validation\n",
    "    fold = 5\n",
    "\n",
    "    features = dataset.iloc[:, :24].values\n",
    "    target = dataset.iloc[:, -2].values\n",
    "\n",
    "    correlations = collections.defaultdict(list)\n",
    "    \n",
    "    for i in range(0, fold):\n",
    "        X_test = features[dataset['fold'] == i]\n",
    "        y_test = target[dataset['fold'] == i]\n",
    "        print(f'Fold {i+1} of {fold}')\n",
    "\n",
    "        # ============================\n",
    "        # Lasso\n",
    "        # ============================\n",
    "        lasso_model = pickle.load(open(pjoin('models', 'trained-models', 'conventional-ml', f'lasso-{data_source}-fold-{i+1}.pkl'), 'rb'))\n",
    "        lasso_pred = lasso_model.predict(X_test)\n",
    "        pearson, spearman = get_pearson_and_spearman_correlation(y_test, lasso_pred)\n",
    "        correlations['Lasso'].append((pearson, spearman))\n",
    "        print(f'Lasso: Pearson: {pearson}, Spearman: {spearman}')\n",
    "\n",
    "        # ============================\n",
    "        # Ridge\n",
    "        # ============================\n",
    "        ridge_model = pickle.load(open(pjoin('models', 'trained-models', 'conventional-ml', f'ridge-{data_source}-fold-{i+1}.pkl'), 'rb'))\n",
    "        ridge_pred = ridge_model.predict(X_test)\n",
    "        pearson, spearman = get_pearson_and_spearman_correlation(y_test, ridge_pred)\n",
    "        correlations['Ridge'].append((pearson, spearman))\n",
    "        print(f'Ridge: Pearson: {pearson}, Spearman: {spearman}')\n",
    "        \n",
    "        # ============================\n",
    "        # xgboost\n",
    "        # ============================\n",
    "        xgboost_model = pickle.load(open(pjoin('models', 'trained-models', 'conventional-ml', f'xgboost-{data_source}-fold-{i+1}.pkl'), 'rb'))\n",
    "        xgboost_pred = xgboost_model.predict(X_test)\n",
    "        pearson, spearman = get_pearson_and_spearman_correlation(y_test, xgboost_pred)\n",
    "        correlations['XGBoost'].append((pearson, spearman))\n",
    "        print(f'XGBoost: Pearson: {pearson}, Spearman: {spearman}')\n",
    "        \n",
    "        # ============================\n",
    "        # random forest\n",
    "        # ============================\n",
    "        rf_model = pickle.load(open(pjoin('models', 'trained-models', 'conventional-ml', f'random_forest-{data_source}-fold-{i+1}.pkl'), 'rb'))\n",
    "        rf_pred = rf_model.predict(X_test)\n",
    "        pearson, spearman = get_pearson_and_spearman_correlation(y_test, rf_pred)\n",
    "        correlations['RF'].append((pearson, spearman))\n",
    "        print(f'Random Forest: Pearson: {pearson}, Spearman: {spearman}')\n",
    "        \n",
    "        # ============================\n",
    "        # MLP\n",
    "        # ============================\n",
    "        # load the pickled model\n",
    "        mlp_model = skorch.NeuralNetRegressor(\n",
    "            module=MLP,\n",
    "            criterion=torch.nn.MSELoss,\n",
    "            optimizer=torch.optim.Adam,\n",
    "            max_epochs=200,\n",
    "            module__activation='relu',\n",
    "            lr=0.001,\n",
    "            device='cuda',\n",
    "            batch_size=2048,\n",
    "            module__hidden_layer_sizes = (64, 64,),\n",
    "        )\n",
    "        mlp_model.initialize()\n",
    "        mlp_model.load_params(\n",
    "            f_optimizer=os.path.join('models', 'trained-models', 'conventional-ml', f'mlp-{data_source}-fold-{i+1}-optimizer.pkl'), \n",
    "            f_history=os.path.join('models', 'trained-models', 'conventional-ml', f'mlp-{data_source}-fold-{i+1}-history.json'), \n",
    "            f_params=os.path.join('models', 'trained-models', 'conventional-ml', f'mlp-{data_source}-fold-{i+1}.pkl')\n",
    "        )\n",
    "        X_test = X_test.astype(np.float32)\n",
    "        X_test = torch.tensor(X_test)\n",
    "        mlp_pred = mlp_model.predict(X_test)\n",
    "        # flatten the prediction\n",
    "        mlp_pred = mlp_pred.flatten()\n",
    "        pearson, spearman = get_pearson_and_spearman_correlation(y_test, mlp_pred)\n",
    "        correlations['MLP'].append((pearson, spearman))\n",
    "        print(f'MLP: Pearson: {pearson}, Spearman: {spearman}')\n",
    "        \n",
    "        print('-----------------------------------')\n",
    "    \n",
    "    print(correlations)\n",
    "    performance[cell_line] = correlations\n",
    "\n",
    "# save the performance\n",
    "save_file = pjoin('models', 'data', 'performance', 'conventional-ml-performance.csv')\n",
    "performance_df = pd.DataFrame(performance)\n",
    "print(performance_df)\n",
    "performance_df.to_csv(save_file)\n",
    "print(f'Performance saved to {save_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the performance in two heat maps, one for pearson and one for spearman\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "from os.path import join as pjoin\n",
    "\n",
    "# cell line vs model\n",
    "performance_data = pjoin('models', 'data', 'performance', 'conventional-ml-performance.csv')\n",
    "df = pd.read_csv(performance_data, index_col=0)\n",
    "\n",
    "cell_lines = df.columns\n",
    "models = df.index\n",
    "\n",
    "# plot the mean of the pearson and spearman correlation\n",
    "pearson = np.zeros((len(models), len(cell_lines)))\n",
    "spearman = np.zeros((len(models), len(cell_lines)))\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    for j, cell_line in enumerate(cell_lines):\n",
    "        performance = df.loc[model, cell_line]\n",
    "        # string to list\n",
    "        performance = ast.literal_eval(performance)\n",
    "        pearson[i, j] = np.mean([x[0] for x in performance])\n",
    "        spearman[i, j] = np.mean([x[1] for x in performance])\n",
    "\n",
    "font_size = 18\n",
    "\n",
    "# pearson and spearman correlation share the same color bar\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 7), width_ratios=[1, 1])\n",
    "sns.heatmap(pearson, ax=axes[0], annot=True, xticklabels=cell_lines, yticklabels=models, cmap='icefire', cbar=False, vmin=0, vmax=1, annot_kws={'size': font_size})\n",
    "axes[0].set_title('Pearson')\n",
    "sns.heatmap(spearman, ax=axes[1], annot=True, xticklabels=cell_lines, yticklabels=models, cmap='icefire', cbar=False, vmin=0, vmax=1, annot_kws={'size': font_size})\n",
    "axes[1].set_title('Spearman')\n",
    "\n",
    "\n",
    "# increase the font size\n",
    "for ax in axes:\n",
    "    for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] + ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        item.set_fontsize(font_size)\n",
    "# # increase color bar font size\n",
    "# cbar = axes[1].collections[0].colorbar\n",
    "# cbar.ax.tick_params(labelsize=font_size)\n",
    "# cbar.set_label('Correlation', fontsize=font_size)\n",
    "\n",
    "\n",
    "# rotate the x labels\n",
    "for ax in axes:\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "# rotate the y labels\n",
    "for ax in axes:\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), rotation=0, horizontalalignment='right')\n",
    "# make sure the figure saved is not cut off\n",
    "plt.tight_layout()\n",
    "\n",
    "# save the figure\n",
    "plt.savefig(pjoin('dissertation', 'figures', 'conventional_ml_models_performance.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepPrime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['wt-sequence', 'mut-sequence', 'pbs-length', 'rt-length',\n",
      "       'extension-length', 'edit-length', 'rha-length', 'edit-position',\n",
      "       'edit-type-replacement', 'edit-type-insertion', 'edit-type-deletion',\n",
      "       'pbs-melting-temperature', 'rtt-wt-cdna-melting-temperature',\n",
      "       'rtt-wt-cdna-new-melting-temperature', 'rtt-cdna-melting-temperature',\n",
      "       'rtt-melting-temperature', 'delta-melting-temperature',\n",
      "       'pbs-gc-content', 'rtt-gc-content', 'extension-gc-content',\n",
      "       'pbs-gc-count', 'rtt-gc-count', 'extension-gc-count',\n",
      "       'extension-minimum-free-energy', 'spacer-minimum-free-energy',\n",
      "       'spcas9-score', 'group-id', 'editing-efficiency', 'fold'],\n",
      "      dtype='object')\n",
      "Fold 1 of 5\n",
      "Training DeepPrime model...\n",
      "Run 1 of 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15ccb536857741709ab7ccd506948309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss    cp      lr     dur\n",
      "-------  ------------  ------------  ----  ------  ------\n",
      "      1     \u001b[36m1343.0504\u001b[0m     \u001b[32m1907.1928\u001b[0m     +  0.0050  1.2130\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d60484dd5dc94308aa3278a306261f46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2      \u001b[36m971.3505\u001b[0m      \u001b[32m936.1563\u001b[0m     +  0.0049  0.5697\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6999cc86e6ff4149a09fde5baf26b996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      3      \u001b[36m797.2092\u001b[0m      950.6600        0.0048  0.5821\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f70f7dfe24a9438690e858dc594d8f60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "Training DeepPrime\n",
    "'''\n",
    "from os.path import join as pjoin, basename\n",
    "\n",
    "from models.deepprime import DeepPrime, preprocess_deep_prime, train_deep_prime\n",
    "\n",
    "fname = pjoin('dp-pd-hek293t-pe2.csv')\n",
    "train_deep_prime(fname, hidden_size=128, num_features=24, num_layers=1, dropout=0.05, epochs=500, batch_size=1024, lr=0.005, patience=10, device='cuda', num_runs=5)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Testing DeepPrime on the clinvar dataset\n",
    "'''\n",
    "from os.path import join as pjoin, basename\n",
    "\n",
    "from models.deepprime import DeepPrime, preprocess_deep_prime, train_deep_prime, predict_deep_prime\n",
    "\n",
    "data = 'dp-pd-hek293t-pe2.csv'\n",
    "predict_deep_prime(data, hidden_size=128, num_layers=1, dropout=0.05, num_features=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test log and undersample adjustments on deep prime\n",
    "from models.deepprime import DeepPrime, preprocess_deep_prime, train_deep_prime, predict_deep_prime\n",
    "import pandas as pd\n",
    "\n",
    "datas = ['dp-dp-hek293t-pe2.csv', 'dp-pd-hek293t-pe2.csv', 'dp-pd-adv-pe2.csv', 'dp-pd-k562-pe2.csv', 'dp-pd-k562mlh1d-pe2.csv']\n",
    "\n",
    "for data in datas:        \n",
    "    print('Training Log')\n",
    "    deepprime_log = train_deep_prime(data, hidden_size=128, num_features=24, num_layers=1, dropout=0.05, epochs=500, batch_size=2048, lr=0.005, patience=20, device='cuda', adjustment='log')\n",
    "    \n",
    "    print('Training Undersample')\n",
    "    deepprime_undersample = train_deep_prime(data, hidden_size=128, num_features=24, num_layers=1, dropout=0.05, epochs=500, batch_size=2048, lr=0.005, patience=20, device='cuda', adjustment='undersample')\n",
    "    \n",
    "    print('Training Org')\n",
    "    deepprime_org = train_deep_prime(data, hidden_size=128, num_features=24, num_layers=1, dropout=0.05, epochs=500, batch_size=2048, lr=0.005, patience=20, device='cuda')\n",
    "        \n",
    "    print('-----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test log and undersample adjustments on deep prime\n",
    "from models.deepprime import DeepPrime, preprocess_deep_prime, train_deep_prime, predict_deep_prime\n",
    "from os.path import join as pjoin\n",
    "import pandas as pd\n",
    "\n",
    "datas = ['dp-pd-hek293t-pe2.csv', 'dp-pd-adv-pe2.csv', 'dp-pd-k562-pe2.csv', 'dp-pd-k562mlh1d-pe2.csv', 'dp-dp-hek293t-pe2.csv']\n",
    "\n",
    "performance_pearson = {}\n",
    "performance_spearman = {}\n",
    "\n",
    "\n",
    "for data in datas:\n",
    "    print('-----------------------------------')\n",
    "    print('Log adjustment')\n",
    "    # log\n",
    "    deepprime_log_pred, deepprime_log_performance = predict_deep_prime(data, hidden_size=128, num_layers=1, dropout=0.05, num_features=24, adjustment='log')\n",
    "    \n",
    "    print('-----------------------------------')\n",
    "    print('Undersample adjustment')\n",
    "    # undersample\n",
    "    deepprime_undersample_pred, deepprime_undersample_performance = predict_deep_prime(data, hidden_size=128, num_layers=1, dropout=0.05, num_features=24, adjustment='undersample')\n",
    "    \n",
    "    print('-----------------------------------')\n",
    "    print('Original')\n",
    "    # original\n",
    "    deepprime_org_pred, deepprime_og_performance = predict_deep_prime(data, hidden_size=128, num_layers=1, dropout=0.05, num_features=24)\n",
    "    \n",
    "    # save the predictions\n",
    "    performance_pearson[data.split('.')[0]] = {\n",
    "        'orginal': [x[0] for x in deepprime_og_performance],\n",
    "        'log': [x[0] for x in deepprime_log_performance],\n",
    "        'undersample': [x[0] for x in deepprime_undersample_performance]\n",
    "    }\n",
    "    \n",
    "    performance_spearman[data.split('.')[0]] = {\n",
    "        'orginal': [x[1] for x in deepprime_og_performance],\n",
    "        'log': [x[1] for x in deepprime_log_performance],\n",
    "        'undersample': [x[1] for x in deepprime_undersample_performance]\n",
    "    }\n",
    "    \n",
    "# save the performance\n",
    "performance_pearson_df = pd.DataFrame(performance_pearson)\n",
    "performance_spearman_df = pd.DataFrame(performance_spearman)\n",
    "\n",
    "# invert the x and y axis\n",
    "performance_pearson_df = performance_pearson_df.T\n",
    "performance_spearman_df = performance_spearman_df.T\n",
    "\n",
    "performance_pearson_df.to_csv(pjoin('models', 'data', 'performance', 'deep-prime-performance-pearson.csv'))\n",
    "performance_spearman_df.to_csv(pjoin('models', 'data', 'performance', 'deep-prime-performance-spearman.csv'))\n",
    "\n",
    "print('Performance saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot both the pearson and spearman correlation performance\n",
    "# each figure contains the performance of the model on the different datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import join as pjoin\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ast\n",
    "import scipy\n",
    "\n",
    "performance_pearson = pd.read_csv(pjoin('models', 'data', 'performance', 'deep-prime-performance-pearson.csv'), index_col=0)\n",
    "performance_spearman = pd.read_csv(pjoin('models', 'data', 'performance', 'deep-prime-performance-spearman.csv'), index_col=0)\n",
    "f_size = 14\n",
    "\n",
    "adjustment_shorthands = {\n",
    "    'weighted-mse': '   WMSE',\n",
    "    'quantile-transform': '     QT',\n",
    "    'original': '     OG',\n",
    "    'log': '     LA',\n",
    "    'undersample': '     US'\n",
    "}\n",
    "\n",
    "# convert all list strings to list\n",
    "import ast\n",
    "\n",
    "performance_pearson = performance_pearson.map(lambda x: ast.literal_eval(x))\n",
    "performance_spearman = performance_spearman.map(lambda x: ast.literal_eval(x))\n",
    "\n",
    "for i, data in enumerate(['dp-dp-hek293t-pe2', 'dp-pd-hek293t-pe2', 'dp-pd-k562-pe2', 'dp-pd-k562mlh1d-pe2', 'dp-pd-adv-pe2']):\n",
    "    # plot the performance of the model on the different datasets\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 1.8), width_ratios=(1.2, 1))\n",
    "    cell_data_pearson = performance_pearson.loc[data]\n",
    "    # into a dataframe of two columns, adjustment and performance\n",
    "    cell_data_pearson = pd.DataFrame(np.array(cell_data_pearson.tolist()).T, columns=['original', 'log', 'undersample'])\n",
    "    \n",
    "    cell_data_spearman = performance_spearman.loc[data]\n",
    "    cell_data_spearman = pd.DataFrame(np.array(cell_data_spearman.tolist()).T, columns=['original', 'log', 'undersample'])\n",
    "    \n",
    "    for ind, performance in enumerate([performance_pearson, performance_spearman]):\n",
    "        # plot the mean of each adjustment as barplot\n",
    "        # plot stripplot of the performance of each fold on top\n",
    "        # of the barplot\n",
    "        cell_data = cell_data_pearson if ind == 0 else cell_data_spearman\n",
    "        sns.barplot(data=cell_data, ax=ax[ind], palette=iter(sns.color_palette('icefire', 3)),alpha=0.5, orient='h', errorbar=None)\n",
    "        sns.stripplot(data=cell_data, ax=ax[ind], palette=iter(sns.color_palette('icefire', 3)), size=5, jitter=True, orient='h')\n",
    "        # ax[ind].set_title('Pearson' if ind == 0 else 'Spearman')\n",
    "        metric = 'Pearson' if ind == 0 else 'Spearman'\n",
    "        ax[ind].set_xlabel(f'{metric}', fontsize=f_size)\n",
    "        # ax[ind].set_ylabel('Adjustment')\n",
    "        \n",
    "        # fix y axis to 0 to 1\n",
    "        ax[ind].set_xlim(0, 1)\n",
    "        \n",
    "        # remove top and right spines\n",
    "        ax[ind].spines['top'].set_visible(False)\n",
    "        if ind == 0:\n",
    "            ax[ind].spines['left'].set_visible(False)\n",
    "        else:\n",
    "            ax[ind].spines['right'].set_visible(False)\n",
    "            \n",
    "        # test the significance between each adjustment with the original\n",
    "        # using the paired t-test\n",
    "        original = cell_data['original']\n",
    "        for adj in cell_data.columns:\n",
    "            if adj != 'original':\n",
    "                if np.mean(cell_data[adj]) < np.mean(cell_data['original']):\n",
    "                    t_stat, p_val = scipy.stats.ttest_rel(cell_data[adj], original)\n",
    "                    if p_val < 0.05:\n",
    "                        print(f'{data} {metric} {adjustment_shorthands[adj]}: {p_val}')\n",
    "    \n",
    "    ax[0].set_yticks(range(len(cell_data.columns)))\n",
    "    ax[0].set_yticklabels([adjustment_shorthands[adj] for adj in cell_data.columns])\n",
    "    # ax 1 should have no y labels\n",
    "    ax[1].set_yticklabels([])\n",
    "    # flip the x axis of ax 0\n",
    "    ax[0].invert_xaxis()\n",
    "    # move the y axis of ax 0 to the right\n",
    "    ax[0].yaxis.tick_right()\n",
    "    \n",
    "    # change the font of x tick labels\n",
    "    ax[0].tick_params(axis='both', which='major', labelsize=f_size)\n",
    "    ax[1].tick_params(axis='both', which='major', labelsize=f_size)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # save the figure\n",
    "    data_name = '-'.join(data.split('-')[1:3])\n",
    "    plt.savefig(pjoin('dissertation/figures', f'adjustment-deepprime-{data_name}-performance.png'), dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRIDICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 of 5\n",
      "Index(['wt-sequence', 'mut-sequence', 'edit-length', 'pbs-length',\n",
      "       'rtt-length', 'rha-length', 'mut-type', 'edit-melting-temperature',\n",
      "       'extension-melting-temperature', 'preedit-melting-temperature',\n",
      "       'protospacer-melting-temperature', 'rha-melting-temperature',\n",
      "       'pbs-melting-temperature', 'edit-sequence-zero-length',\n",
      "       'preedit-sequence-zero-length', 'protospacer-location',\n",
      "       'pbs-location-l-relative-protospacer',\n",
      "       'rtt-location-l-relative-protospacer',\n",
      "       'rha-location-l-relative-protospacer', 'extension-minimum-free-energy',\n",
      "       'extension-scaffold-minimum-free-energy', 'pbs-minimum-free-energy',\n",
      "       'spacer-minimum-free-energy',\n",
      "       'spacer-extension-scaffold-minimum-free-energy',\n",
      "       'spacer-scaffold-minimum-free-energy', 'rtt-minimum-free-energy'],\n",
      "      dtype='object')\n",
      "1        50.355050\n",
      "2        58.712296\n",
      "3        46.821564\n",
      "4         8.512176\n",
      "5        64.029816\n",
      "           ...    \n",
      "22613    28.024351\n",
      "22614    69.629050\n",
      "22616    22.749813\n",
      "22617    84.816160\n",
      "22618    54.873330\n",
      "Name: editing-efficiency, Length: 18116, dtype: float64\n",
      "Training PRIDICT model...\n",
      "Run 1 of 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afb70f38bee548e8bbffd8fa65f1aa1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss    cp      lr     dur\n",
      "-------  ------------  ------------  ----  ------  ------\n",
      "      1        \u001b[36m3.7760\u001b[0m        \u001b[32m2.0035\u001b[0m     +  0.0050  2.9143\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa342f3af11e49feabd364f89022c657",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2        \u001b[36m3.1502\u001b[0m        2.5801        0.0049  2.7895\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "903a81960b524d62965992d0a8c56c6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      3        \u001b[36m1.9860\u001b[0m        \u001b[32m1.7740\u001b[0m     +  0.0048  2.5255\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e68e39beeb7479a8f0e8e39c5519f6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      4        \u001b[36m1.7224\u001b[0m        1.7948        0.0045  2.5545\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaa7d8d3fb774eb48cd91eb810c83a75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        \u001b[36m1.4430\u001b[0m        1.8395        0.0042  2.6523\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2259a21d49c472cbc382e213b6876d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6        \u001b[36m1.3824\u001b[0m        1.8647        0.0037  2.5598\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee39fed870a0448b817103e5e11ad09f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7        \u001b[36m1.3151\u001b[0m        1.8947        0.0033  2.5449\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "027e35d7130a4d3e9690eae35779d823",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      8        \u001b[36m1.2738\u001b[0m        1.9522        0.0028  2.7280\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03eae462aa2e4f0593973ce5f9157689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      9        \u001b[36m1.2504\u001b[0m        1.9084        0.0022  2.6723\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92e3d12bb1fa44d6929e24a69575c99e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     10        \u001b[36m1.2019\u001b[0m        1.8662        0.0017  3.7128\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e27d993b4e44ca39b16992f05857b73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     11        \u001b[36m1.1775\u001b[0m        1.8469        0.0013  2.6021\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "035592ca06ee4193898add36f26be84f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     12        1.1789        1.8005        0.0008  2.5587\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81145bfe45ad4cf7a98c809273a4d959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     13        \u001b[36m1.1605\u001b[0m        \u001b[32m1.7668\u001b[0m     +  0.0005  2.3866\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e04c7b60ce54870a08b800fc5a8ac61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     14        \u001b[36m1.1573\u001b[0m        \u001b[32m1.7545\u001b[0m     +  0.0002  2.4713\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "351c068f5b2b4e3bace91962c7f4415a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     15        1.1599        \u001b[32m1.7518\u001b[0m     +  0.0001  2.5930\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8457392606014296867bb6fae15d62ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16        \u001b[36m1.1316\u001b[0m        \u001b[32m1.6350\u001b[0m     +  0.0050  2.4709\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eaa18400df5451a85f2380aea0a5084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     17        \u001b[36m1.1234\u001b[0m        \u001b[32m1.4506\u001b[0m     +  0.0049  2.6669\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1caefa20ff745b9a2868c5fced6113d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18        1.1537        1.7023        0.0048  2.6286\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dd54dde1e1a4b4781b5aa1823f21bab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     19        1.2588        1.6328        0.0045  2.5406\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b65cb2508087430e916a994c6ad54da1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     20        2.0346        2.2554        0.0042  2.6453\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52bb33fe627f437d90eccc61ec26fb03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     21        1.4688        1.7492        0.0037  2.7074\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d85dbd8937740759ac424315791be28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     22        1.4075        1.5569        0.0033  2.5981\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c534ff7f2de348a6b56ed449dae45cc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     23        1.2434        1.6276        0.0028  3.7706\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9105a64fd924e66ac38f1ece5d9a00d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     24        1.1828        1.7359        0.0022  2.7482\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "599f31c915d54aa88841613c43db28ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     25        1.1313        1.7777        0.0017  2.5166\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53840621d76849f2bcd05cb795208527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     26        \u001b[36m1.0778\u001b[0m        1.6997        0.0013  2.6766\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91c4935f1aa846629ccee14fc80b533f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping since valid_loss has not improved in the last 10 epochs.\n",
      "Run 2 of 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c0c5d4140e74fc6835468b64c23dec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss    cp      lr     dur\n",
      "-------  ------------  ------------  ----  ------  ------\n",
      "      1        \u001b[36m3.6900\u001b[0m        \u001b[32m2.1228\u001b[0m     +  0.0050  2.6334\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01f99fccd9af4177affbb7e260c57517",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2        \u001b[36m2.3826\u001b[0m        \u001b[32m1.6242\u001b[0m     +  0.0049  2.9241\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4860ae9003304d729bcac5d62a93b996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      3        \u001b[36m1.7863\u001b[0m        1.7782        0.0048  2.6251\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b3f800a83684480bc437f9970dd3525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      4        \u001b[36m1.4270\u001b[0m        1.8500        0.0045  2.7758\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "759583f92817446db969e3d154b57ff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        \u001b[36m1.4239\u001b[0m        1.6702        0.0042  2.7678\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "726a18ee689d49e496ea803008e1dda6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6        \u001b[36m1.2776\u001b[0m        1.7363        0.0037  2.8167\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfee93eecf9449e9a52fa5827ddd614b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7        \u001b[36m1.2516\u001b[0m        \u001b[32m1.5762\u001b[0m     +  0.0033  2.7060\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e758cf313da6461aa669fa9520a54333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      8        \u001b[36m1.2012\u001b[0m        \u001b[32m1.5652\u001b[0m     +  0.0028  4.0062\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc34a7ffa3004b18a77f85f6f71e92a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      9        \u001b[36m1.1768\u001b[0m        \u001b[32m1.4897\u001b[0m     +  0.0022  2.7163\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38b3b65f7ed14831bbade7a7c05aa7c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     10        \u001b[36m1.1354\u001b[0m        \u001b[32m1.4577\u001b[0m     +  0.0017  2.5681\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "debbb1945fd6456d8fa12d65273dd515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     11        \u001b[36m1.1320\u001b[0m        \u001b[32m1.4037\u001b[0m     +  0.0013  2.8527\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "465f1378f73e49bdb3779dc54dd7eafd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     12        \u001b[36m1.0996\u001b[0m        1.4129        0.0008  2.7618\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bbd68217ce94f6ea0cb02339ffa15fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     13        \u001b[36m1.0996\u001b[0m        1.4177        0.0005  2.5634\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b28a46ff80a49e4843a6402acfc7a96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     14        \u001b[36m1.0857\u001b[0m        1.4183        0.0002  2.7192\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dde6c8abd0ec4d55932ed2e7687b2191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     15        \u001b[36m1.0747\u001b[0m        1.4179        0.0001  2.7360\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bde097038d51488dbd986b7bb14eb08f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     16        1.0918        \u001b[32m1.3997\u001b[0m     +  0.0050  2.8452\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "263abf00676d4388b53afebccc73730a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from models.pridict import train_pridict\n",
    "\n",
    "train_pridict('pd-pd-hek293t-pe2.csv', lr=0.005, batch_size=2048, epochs=500, patience=10, num_runs=5, adjustment='log', num_features=24)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
